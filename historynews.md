# üí• History News

## 2023

‚òÑÔ∏è **EgoAlpha releases the TrustGPT focuses on reasoning. Trust the GPT with the strongest reasoning abilities for authentic and reliable answers. You can click [here](https://trustgpt.co) or visit the [Playgrounds](./Playground.md) directly to experience it„ÄÇ**


**[2023.7.31]**
- Paper: [Foundational Models Defining a New Era in Vision: A Survey and Outlook](https://arxiv.org/abs/2307.13721)

**[2023.7.30]**
- Paper: [Challenges and Applications of Large Language Models](https://arxiv.org/abs/2307.10169)

**[2023.7.29]**
- Paper: [A Watermark for Large Language Models](https://openreview.net/forum?id=aX8ig9X2a7 )

**[2023.7.28]**
- Paper:[Med-Flamingo: a Multimodal Medical Few-shot Learner](https://arxiv.org/pdf/2307.15189v1.pdf)

**[2023.7.27]**
- Paper: [using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)

**[2023.7.26]**
- Paper:[3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)

**[2023.7.25]**
- Paper:[ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning](https://arxiv.org/abs/2307.09474)

**[2023.7.24]**
- Paper:[Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)

**[2023.7.23]**
- [ChatGPT based on Android, pre-register location](https://play.google.com/store/apps/details?id=com.openai.chatgpt)

- Paper:[OBJECT 3DIT: Language-guided 3D-aware Image Editing](https://arxiv.org/abs/2307.11073)
    
**[2023.7.22]**
- Paper:[Brain2Music: Reconstructing Music from Human Brain Activity](https://arxiv.org/abs/2307.11078)

**[2023.7.21]**
- Paper:[A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)
    
**[2023.7.20]**
- New Architecture: RetNetwork, beyond Transformer üëâ[Paper](https://arxiv.org/abs/2307.08621 )üëà

**[2023.7.19]**
- [Meta releases LLAMA 2, open source and commercially available.](https://ai.meta.com/llama/)
    
**[2023.7.18]**
- Paper:[Learning to Retrieve In-Context Examples for Large Language Models](https://arxiv.org/abs/2307.07164)
    
**[2023.7.17]**
- Paper:[HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://arxiv.org/abs/2307.06949)
    
**[2023.7.16]**
- Emu model is open source, a versatile expert in 'multimodal to multimodal': [Model](https://github.com/baaivision/Emu) / [Demo](https://emu.ssi.plus/)
    
**[2023.7.15]**
- Paper:[Self-consistency for open-ended generations](https://arxiv.org/abs/2307.06857)
    
**[2023.7.14]**
- Paper:[Patch n‚Äô Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/pdf/2307.06304.pdf)
    
**[2023.7.13]**
- Paper:[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)

**[2023.7.12]**
- Claude2üëâ **\[Paper\]**[Model Card and Evaluations for Claude Models](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf) / **[Website]**(https://claude.ai/)

**[2023.7.11]**
- Paper: [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/voxposer.pdf)
- Paper: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)

**[2023.7.10]**
- Paper:[Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)
    
**[2023.7.9]**
- Paper: [Schema-learning and rebinding as mechanisms of in-context learning and emergence](https://arxiv.org/abs/2307.01201)

**[2023.7.8]**
- Paper:[SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs](https://arxiv.org/abs/2306.17842)

**[2023.7.7]**
- [GPT-4 API general availability](https://openai.com/blog/gpt-4-api-general-availability)
- Paper:[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)
    
**[2023.7.6]**
- Paper:[LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)

**[2023.7.5]**
- [MetaGPT: Multi-Role Meta-Programming Framework](https://github.com/geekan/MetaGPT)
- Paper: [Conformer LLMs -- Convolution Augmented Large Language Models](https://arxiv.org/abs/2307.00461)

**[2023.7.4]**
- Paper:[Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic](http://arxiv.org/abs/2306.15195)
    
**[2023.7.3]**
- [Largest-scale Chinese multitask instruction set, introducing a thousand Chinese datasets.](https://huggingface.co/datasets/BAAI/COIG-PC)
    
**[2023.7.2]**
- Paper: [Towards Measuring the Representation of Subjective Global Opinions in Language Models](https://arxiv.org/abs/2306.16388)
    
**[2023.7.1]**
- Paper:[Masked Vision-language Transformer in Fashion](https://link.springer.com/article/10.1007/s11633-022-1394-4)
    
**[2023.6.30]**
- Paper: [Inferring the Goals of Communicating Agents from Actions and Instructions](https://arxiv.org/abs/2306.16207)

**[2023.6.29]**
- Paper: [AudioPaLM: A Large Language Model That Can Speak and Listen](https://arxiv.org/pdf/2306.12925.pdf)

**[2023.6.28]**
- Paper:[Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs//2306.14824)

**[2023.6.27]**
- Paper: [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549v1.pdf)
    
**[2023.6.26]**
- Paper: [PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance](https://arxiv.org/abs/2306.05443)

**[2023.6.25]**
- Paper: [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561)

**[2023.6.24]**
- Paper: [PromptIR: Prompting for All-in-One Blind Image Restoration](https://www.researchgate.net/publication/371786106_PromptIR_Prompting_for_All-in-One_Blind_Image_Restoration)

**[2023.6.23]**
- Paper: [OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue](https://arxiv.org/format/2306.12174)
    
**[2023.6.22]**
- [Stanford has released an automatic evaluation system for LLM called AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)
- [Ocean-1: the world's first contact center foundation model.](https://cresta.com/blog/introducing-ocean-1-worlds-first-contact-center-foundation-model/)

**[2023.6.21]**
- [GPT-Engineering, Who generates an entire codebase based on a prompt.](https://github.com/AntonOsika/gpt-engineer)

**[2023.6.20]**
- Paper: [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://scontent-fra3-1.xx.fbcdn.net/v/t39.8562-6/354636794_599417672291955_3799385851435258804_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=PW3or0UVoKoAX-2_D0q&_nc_ht=scontent-fra3-1.xx&oh=00_AfB2SW8Rp55YKG0AEJcpC9tUECbXdl_m83yk9cxX7jie1A&oe=64967631)
    
**[2023.6.19]**
- Technical Report: [AIGC industry overview article from SEALAND SECURITIES](https://pdf.dfcfw.com/pdf/H3_AP202303201584404280_1.pdf?1679327615000.pdf)

**[2023.6.18]**
- Paper: [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory](https://arxiv.org/abs/2306.03901)
    
**[2023.6.17]**
- Paper: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://www.researchgate.net/publication/371540959_Macaw-LLM_Multi-Modal_Language_Modeling_with_Image_Audio_Video_and_Text_Integration)

**[2023.6.16]**
- Financial [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) model open source, benchmarked against BloombergGPT, training parameters can be reduced from 6.17 billion to 3.67 million, can predict stock prices. ([Paper](https://arxiv.org/abs/2306.06031)/[Code](https://github.com/AI4Finance-Foundation/FinGPT))

**[2023.6.15]**
- Paper:[MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://arxiv.org/abs/2306.01311 )
    
**[2023.6.14]**
- Paper:[XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models](https://arxiv.org/abs/2306.07971)
    
**[2023.6.13]**
- Paper:[Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
 
**[2023.6.12]**
- Paper: [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425)
    
**[2023.6.11]**
- Paper: [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/abs/2306.05179)

**[2023.6.10]**
- [Aquila, language model series, including the Aquila Basic Model (7B and 33B), AquilaChat dialogue model, and AquilaCode text-to-code generation model.](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)
    
**[2023.6.9]**
- Paper: [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)

**[2023.6.8]**
- Paper: [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/pdf/2305.05176.pdf)
    
**[2023.6.7]**
- Paper: [XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech](https://arxiv.org/abs/2305.19709)
    
**[2023.6.6]**
- Paper: [XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters](https://arxiv.org/abs/2305.12002)
- Paper: [UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild](https://arxiv.org/abs/2305.11147)

**[2023.6.5]**
- Paper: [Controllable Text-to-Image Generation with GPT-4](https://arxiv.org/abs/2305.18583)

**[2023.6.4]**
- PandaGPT: One model unifies six modalities([Page](https://panda-gpt.github.io/)/[Paper](https://arxiv.org/abs/2305.16355))

**[2023.6.3]**
- Paper: [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)

**[2023.6.2]**
- Paper: [SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/abs/2305.17390)

**[2023.6.1]**
- Paper: [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216)

**[2023.5.31]**
- [Intel announces the Aurora genAI, which is generative AI model with 1 trillion parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/)
    
- HuotuoGPT, towards Taming Language Model to Be a Doctor([Github](https://github.com/FreedomIntelligence/HuatuoGPT)/[Demo](https://www.huatuogpt.cn)/[Paper](https://arxiv.org/pdf/2305.15075.pdf))
- Paper: [Large Language Models Meet NL2Code: A Survey](https://arxiv.org/abs/2212.09420)

**[2023.5.30]** 
- Paper: [Large Language Models as Tool Makers](https://arxiv.org/pdf/2305.17126.pdf)
    
- Paper: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100)

- Paper: [OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities](https://arxiv.org/pdf/2305.16334.pdf)

**[2023.5.29]**

- [Falcon: based on the 1 trillion token open-source large model, surpassing 65 billion LLaMA, commercially available.](https://huggingface.co/tiiuae)

- [ToolBench: the open platform for large language models used for training, service, and evaluation of tool learning](https://github.com/Navezjt/ToolBench)
- Paper: [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/abs/2305.15021)

**[2023.5.28]**
- Paper: [ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation](https://arxiv.org/abs/2305.16213)
- Paper:[RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text](https://arxiv.org/abs/2305.13304)

**[2023.5.27]**
- Paper: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)

- Paper: [ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs](https://arxiv.org/pdf/2305.15964v1.pdf)

- Paper: [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720)

**[2023.5.26]**
- Paper: [Iterative Forward Tuning Boosts In-context Learning in Language Models](https://arxiv.org/pdf/2305.13016.pdf)
    
**[2023.5.25]**
- Paper: [Diversity-Aware Meta Visual Prompting](https://arxiv.org/abs/2303.08138)
    
**[2023.5.24]**
- Paper:[VideoLLM: Modeling Video Sequence with Large Language Models](https://arxiv.org/pdf/2305.13292.pdf)

**[2023.5.23]**
- Paper:[Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)
- Paper: [PointGPT: Auto-regressively GenerativePre-training from Point Clouds](https://arxiv.org/pdf/2305.11487.pdf)

**[2023.5.22]**
- Paper: [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://arxiv.org/abs/2305.10266)
    
**[2023.5.21]**
- Paper:[Language Models Meet World Models: Embodied Experiences Enhance Language Models](https://arxiv.org/abs/2305.10626)

**[2023.5.20]**
- Paper:[AttentionViz: A Global View of Transformer Attention](https://arxiv.org/pdf/2305.03210.pdf)
    
**[2023.5.19]**
- [OpenAI introducing the ChatGPT APP for IOS](https://apps.apple.com/app/openai-chatgpt/id6448311069)
- Paper: [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)
    
**[2023.5.18]**
- Paper: [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)
    
**[2023.5.17]**
- Paper: [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)
    
**[2023.5.16]**
- Paper:[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185.pdf)

**[2023.5.15]** 
- Paper: [ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4](https://arxiv.org/pdf/2305.07490.pdf)

**[2023.5.14]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301v1.pdf)

**[2023.5.13]**
- Paper: [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)

**[2023.5.12]**
- Paper: [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)
    
**[2023.5.11]**
- Google has released PaLM 2, which improves multiple abilities and offers four versions for selection. ([Paper](https://event-cdn.baai.ac.cn/file/file-browser/KDtjMkep6E6n5XjRNJjknjewCF7Pcebx.pdf)/[Page](https://makersuite.google.com/waitlist ))
- [The open-source healthcare large language model NHS-LLM and OpenGPT.](https://github.com/CogStack/opengpt)

- Paper: [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)

**[2023.5.10]** 
- DetGPT: Detect What You Need via Reasoning ([Code](https://github.com/OptimalScale/DetGPT)/[Demo](https://detgpt.github.io/))

- Meta releases a large-scale model called ImageBind that can traverse six senses, and it is now open-source. ([Paper](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)/[Code](https://github.com/facebookresearch/ImageBind))

- [HuoTuo: Open Source Chinese Medical Large Model of Harbin Institute of Technology](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)

- Paper: [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/pdf/2305.04160.pdf)

**[2023.5.9]**

- Paper: [Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278) „Äê[Code](https://github.com/VPGTrans/VPGTrans)„Äë

**[2023.5.8]**
- PandaLM: the first large model for automated evaluation.([Code](https://github.com/WeOpenML/PandaLM))
    
- Paper:[AutoML-GPT: Automatic Machine Learning with GPT](https://arxiv.org/abs/2305.02499)

**[2023.5.7]**
- Paper: [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf)
    
**[2023.5.6]**
- [IFLYTEK officially releases the LLM named SparkDesk](https://xinghuo.xfyun.cn/)

- OpenAI release the language-to-3D model: Shape.E ([Paper](https://arxiv.org/pdf/2305.02463.pdf)/[Project Page](https://github.com/openai/shap-e))

- Paper: [Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks](https://arxiv.org/abs/2304.14732)
    
**[2023.5.5]** 
- Paper: [Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner](https://arxiv.org/pdf/2305.01711.pdf)

**[2023.5.4]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301.pdf)

**[2023.5.3]**
- Paper:[Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)


**[2023.5.2]** 
- Customize your own LLMs, stop prompt-tunning: [Lamini](https://lamini.ai/)

- Paper: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/pdf/2305.01625.pdf)

**[2023.5.1]**
- Paper: [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/pdf/2304.14178.pdf)

**[2023.4.30]** 
- Paper: [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712)

**[2023.4.29]**
- Paper: [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/pdf/2304.11477.pdf)

**[2023.4.28]**

- Paper: [Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System](https://arxiv.org/pdf/2304.13343.pdf)

**[2023.4.27]** 
- AudioGPT: [[Project Page](https://github.com/AIGC-Audio/AudioGPT)/[Paper](https://arxiv.org/pdf/2304.12995.pdf)]

- Paper:[Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/pdf/2304.13007.pdf)

**[2023.4.26]** 
- Paper: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2304.12244.pdf)

**[2023.4.25]** 
- [Google releases the Security AI workbench](https://cloud.google.com/blog/products/identity-security/rsa-google-cloud-security-ai-workbench-generative-ai)

- Paper: [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models](https://arxiv.org/pdf/2304.11657.pdf)

**[2023.4.24]** 
- Paper: [Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback](https://arxiv.org/pdf/2304.10750.pdf)

**[2023.4.23]** 
- Paper: [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/pdf/2304.11158.pdf)

**[2023.4.22]** Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[Paper](https://arxiv.org/abs/2304.09842)/[Project](https://chameleon-llm.github.io)]

**[2023.4.21]**
- Paper:

    [Progressive-Hint Prompting Improves Reasoning in Large Language Models](https://arxiv.org/pdf/2304.09797.pdf)

    [Pretrained Language Models as Visual Planners for Human Assistance](https://arxiv.org/pdf/2304.09179.pdf)

**[2023.4.20]**

- [Google DeepMind: Bringing together two world-class AI teams](https://blog.google/technology/ai/april-ai-update/)

- Paper: [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)

**[2023.4.19]** 
    
- Paper: [Towards Robust Prompts on Vision-Language Models](https://arxiv.org/pdf/2304.08479.pdf)

**[2023.4.18]**
-   [HuaTuo:Tunning LLaMA Model with chinese medical instructions](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)
-   MiniGPT-4 [[Project Page](https://minigpt-4.github.io/)/[Paper](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf)]

- Paper: 
        [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text](https://arxiv.org/abs/2304.06939 )

**[2023.4.17]** 
- The open source democratizes large language models,**OpenAssistant**, supports 35 languages, and can use RLHF data for free[[Project Page](https://open-assistant.io/chat)/[Code](ttps://github.com/LAION-AI/Open-Assistant)/[Paper](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view )]

- Paper: [Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition](https://arxiv.org/pdf/2304.04704.pdf)

**[2023.4.16]** 
- Paper: [AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models](https://arxiv.org/pdf/2304.06364.pdf)

**[2023.4.15]** 
- Paper: [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society](https://arxiv.org/abs/2303.17760)

- [Visual Med-Alpaca: Bridging Modalities in Biomedical Language Models](https://cambridgeltl.github.io/visual-med-alpaca/)

**[2023.4.14]**

- [Amazon announcing new tools for building with Generative AI](https://aws.amazon.com/cn/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/)

- Paper: [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://arxiv.org/pdf/2304.05613.pdf)

**[2023.4.13]** Three Amazing Works:

- AutoGPT: An Autonomous GPT-4 Experiment üëâ[Code](https://github.com/torantulino/auto-gpt)üëà

- [Databricks releases Dolly 2.0, the first open, instruction-following LLM for commercial use](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)

- [Microsoft released the DeepSpeed Chat: Own your ChatGPT](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)

**[2023.4.12]** [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/pdf/2304.04370.pdf)

**[2023.4.11]** [Why think step-by-step? Reasoning emerges from the locality of experience](https://arxiv.org/pdf/2304.03843.pdf)

**[2023.4.10]** [TagGPT: Large Language Models are Zero-shot Multimodal Taggers](https://arxiv.org/pdf/2304.03022.pdf)

**[2023.4.9]** A new AI model from Meta AI: Segment Anything Model (SAM) ([Paper](https://arxiv.org/pdf/2304.02643.pdf)/[Code](https://github.com/facebookresearch/segment-anything))

**[2023.4.8]** EleutherAI&Yale et al. proposed a large-scale language model analysis suite that spans training and extension: Pythia ([Paper](https://arxiv.org/pdf/2304.01373.pdf)/[Code](https://github.com/EleutherAI/pythia))

**[2023.4.7]** [Stanford releases the 7 billion parameter open-source model Vicuna-7B, which is compact, efficient, but powerful in functionality](https://vicuna.lmsys.org/)

**[2023.4.6]** [Effective Theory of Transformers at Initialization](https://arxiv.org/pdf/2304.02034.pdf)

**[2023.4.5]** [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf)

**[2023.4.4]** [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?](https://arxiv.org/pdf/2303.18240.pdf)

**[2023.4.3]** [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/pdf/2303.17651.pdf)

**[2023.4.1]** [A survey of Large Language Models](https://arxiv.org/abs/2303.18223)

**[2023.3.31]** [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)

**[2023.3.30]** [GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/pdf/2303.16634.pdf)

**[2023.3.29]** [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf.)

**[2023.3.28]** [ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks](https://arxiv.org/pdf/2303.15056.pdf)

**[2023.3.27]** [Scaling Expert Language Models with Unsupervised Domain Discovery](https://arxiv.org/pdf/2303.14177.pdf)
 
**[2023.3.26]** [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)

**[2023.3.23]** [OpenAI announces 'Plug-ins' for ChatGPT that enable it to perform actions beyond text.](https://platform.openai.com/docs/plugins/introduction)

**[2023.3.22]** [GitHub launches Copilot X, aiming at the future of AI-powered software development.](https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/) 

**[2023.3.21]** [Google Bard is now available in the US and UK, w/ more countries to come.](https://bard.google.com) 

**[2023.3.20]** OpenAI‚Äôs new paper looks at the economical impact of LLMs+Labor Market.[GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) 

**[2023.3.17]** [Microsoft 365 Copilot released. Word, Excel, PowerPoint, Outlook powered by LLMs.](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)

**[2023.3.16]**  Baidu announcing the LLM named ["ÊñáÂøÉ‰∏ÄË®Ä"(ERNIE3.0 + PLATO)](https://yiyan.baidu.com/welcome) 

**[2023.3.15]** Two Breaking News:
    -  Announcing [GPT4](https://openai.com/product/gpt-4) by OpenAI from Microsoft. **[Paperüîó](https://cdn.openai.com/papers/gpt-4.pdf)**
    -  Announcing [PaLM](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html) API by Google. 

**[2023.3.13]** [LLaMA has been fine-tuned by Stanford](https://github.com/tatsu-lab/stanford_alpaca)

**[2023.3.10]** [Announcing OpenChatKit by Together](https://huggingface.co/spaces/togethercomputer/OpenChatKit)

**[2023.3.9]**  GPT-4 is coming next week and it will be multimodal,announced by OpenAI.

**[2023.3.8]** [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)

**[2023.3.7]** [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)

**[2023.3.6]** [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)