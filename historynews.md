# üí• History News

## 2023

‚òÑÔ∏è **EgoAlpha releases the TrustGPT focuses on reasoning. Trust the GPT with the strongest reasoning abilities for authentic and reliable answers. You can click [here](https://trustgpt.co) or visit the [Playgrounds](./Playground.md) directly to experience it„ÄÇ**

**[2023.10.29]**
- Paper: [Creative Robot Tool Use with Large Language Models](https://arxiv.org/abs/2310.13065 )

**[2023.10.28]**
- Paper: [Large Graph Models: A Perspective](https://arxiv.org/abs/2308.14522)

**[2023.10.27]**
- Paper: [SuperHF: Supervised Iterative Learning from Human Feedback](https://arxiv.org/abs/2310.16763 )

**[2023.10.26]**
- Paper: [Woodpecker: Hallucination Correction for Multimodal Large Language Models](https://arxiv.org/pdf/2310.16045.pdf)

**[2023.10.25]**
- Paper: [OpenAgents: An Open Platform for Language Agents in the Wild](https://arxiv.org/abs/2310.10634#)

**[2023.10.24]**
- Paper: [Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading](https://arxiv.org/abs/2310.05029)

**[2023.10.23]**
- Paper: [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://arxiv.org/abs/2310.11954)
- Paper: [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798)

**[2023.10.22]**
- Paper: [Uni3D: Exploring Unified 3D Representation at Scale](https://arxiv.org/abs/2310.06773)

**[2023.10.21]**
- Paper: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)

**[2023.10.20]**
- Paper: [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/fmti.pdf)
- Paper: [XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.02989.pdf)

**[2023.10.19]**
- Paper: [The Consensus Game: Language Model Generation via Equilibrium Search](http://arxiv.org/abs/2310.09139)

**[2023.10.18]**
- Paper: [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770)

**[2023.10.17]**
- Paper: [Table-GPT: Table-tuned GPT for Diverse Table Tasks](https://arxiv.org/abs/2310.09263)

**[2023.10.16]**
- Paper: [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)

**[2023.10.15]**
- Paper: [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226)

**[2023.10.14]**
- Paper: [How FaR Are Large Language Models From Agents with Theory-of-Mind?](http://arxiv.org/abs/2310.03051)

**[2023.10.13]**
- Paper: [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)

**[2023.10.12]**
- Paper: [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://arxiv.org/abs/2310.06452 )
- Paper: [Learning Interactive Real-World Simulators](https://arxiv.org/abs/2310.06114 )

**[2023.10.11]**
- Paper: [PB-LLM: Partially Binarized Large Language Models](https://arxiv.org/abs/2310.00034)

**[2023.10.10]**
- Paper: [MVDREAM:MULTI-VIEW DIFFUSION FOR 3D GENERATION](https://arxiv.org/pdf/2308.16512.pdf)

**[2023.10.9]**
- Paper: [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/pdf/2310.02239v2.pdf)

**[2023.10.8]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301)

**[2023.10.7]**
- Survey Paper: [A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future](https://arxiv.org/abs/2309.15402)

**[2023.10.6]**
- Paper: [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207)

**[2023.10.5]**
- Paper: [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530)

**[2023.10.4]**
- Paper: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

**[2023.10.3]**
- Paper: [CHAIN-OF-VERIFICATION REDUCES HALLUCINATION IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2309.11495)

**[2023.10.2]**
- Paper: [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499)

**[2023.10.1]**
- Paper: [Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2309.10313)

**[2023.9.30]**
- Paper: [Cumulative Reasoning with Large Language Models](https://arxiv.org/abs/2308.04371)

**[2023.9.29]**
- Paper: [Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2309.10313)

**[2023.9.28]**
- Survey Paper: [Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/abs/2308.10792)

**[2023.9.27]**
- [Chinese LLaMA-2 tops the list, open source and commercially available! With a budget of one thousand yuan, training for half a day, the effect is comparable to mainstream large models.](https://github.com/hpcaitech/ColossalAI)
- [Lingxin Intelligence releases CharacterGLM: Play AI role-playing, 6B model is now open source.](https://huggingface.co/LingxinAI/CharacterGLM-6b)
- Paper: [Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement](https://arxiv.org/abs/2303.06705)

**[2023.9.26]**
- The biggest bug of the large model! The accuracy of the answers is almost zero, from GPT to Llama, none are spared: Paper:[The Reversal Curse: LLMs trained on ‚ÄúA is B‚Äù fail to learn ‚ÄúB is A‚Äù ](https://arxiv.org/abs/2309.12288)
- Paper: [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495)
- Paper: [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)

**[2023.9.25]**
- Paper: [Cell2Sentence: Teaching Large Language Models the Language of Biology](https://www.biorxiv.org/content/10.1101/2023.09.11.557287v1 )

**[2023.9.24]**
- [Writer model is open source, commercially available, and there are a total of 8 models.](https://huggingface.co/Writer)
- Paper: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

**[2023.9.23]**
- [Defeat GPT-4? 70 billion parameter Xwin-LM climbs up to the top of  Stanford AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)
- Paper: [End-to-End Speech Recognition Contextualization with Large Language Models](https://arxiv.org/abs/2309.10917)

**[2023.9.22]**
- [AgentVerse: A Framework for Multi-LLM Environment Simulation](https://github.com/OpenBMB/AgentVerse)
- [The performance of the 20 billion scale large model is comparable to Llama2-70B! It is completely open source, and everything from the base to the tools is well arranged.](https://github.com/InternLM/InternLM)
- Paper: [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419)

**[2023.9.21]**
- 34B parameter exceeds GPT-4! "Mathematical Universal Large Model" MAmmoTH open source: average accuracy rate increased by 29% ([Paper](https://arxiv.org/pdf/2309.05653.pdf)/[Project Page](https://tiger-ai-lab.github.io/MAmmoTH/))
- Paper: [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions](https://q-transformer.github.io/assets/q-transformer.pdf)

**[2023.9.20]**
- [Optimizing LLMs from a Dataset Perspective](https://lightning.ai/pages/community/tutorial/optimizing-llms-from-a-dataset-perspective/)
- Google DeepMind predicts 71 million genetic mutations, decrypts the human genetic code, and is now published in Science. It has been open-sourced.([Paper](https://www.deepmind.com/blog/alphamissense-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases )/[Science](https://www.science.org/doi/10.1126/science.adg7492 )/[Dataset](https://github.com/deepmind/alphamissense))
- Paper: [Replacing softmax with ReLU in Vision Transformers](https://arxiv.org/pdf/2309.08586.pdf)

**[2023.9.19]**
- Paper: [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519)

**[2023.9.18]**
- Survey Paper: [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864.pdf)

**[2023.9.17]**
- Paper: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)

**[2023.9.16]**
- Paper: [DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2309.03883.pdf)

**[2023.9.15]**
- Microsoft Open Sources EvoDiff: A New Generation of Protein Generative AI : [\[Paper\]](https://doi.org/10.1101/2023.09.11.556673 )
- Paper: [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://arxiv.org/pdf/2309.05173v1.pdf)

**[2023.9.14]**
- Can LLMs Really Reason and Plan? | blog @ CACM | Communications of the ACM„Äê[Paper](https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext)/[Video](https://www.youtube.com/watch?v=BmyB-4S9QuY )„Äë

**[2023.9.13]**
- Chinese multimodal large model VisCPM open API interface! The upgraded version is far more capable than similar models\([Paper](https://arxiv.org/pdf/2308.12038.pdf)/[Github](https://github.com/OpenBMB/VisCPM)\)
- Survey Paper: [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432v2.pdf)

**[2023.9.12]**
- Paper: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)

**[2023.9.11]**
- Paper:[Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning](https://arxiv.org/pdf/2303.09410.pdf)

**[2023.9.10]**
- Survey Paper: [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)

**[2023.9.9]**
- Open-source version of code interpreter tops GitHub hotlist, runs locally, accesses Internet: [\[Github\]](https://github.com/KillianLucas/open-interpreter/#commands)
- Peking University propose the Structured Chain of Thought SCoT: [\[Paper\]](https://arxiv.org/pdf/2305.06599.pdf)
- Paper:[LARGE LANGUAGE MODELS AS OPTIMIZERS](https://arxiv.org/pdf/2309.03409.pdf)
- Survey Paper:[RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model](https://arxiv.org/abs/2309.00810)

**[2023.9.8]**
- Paper:[Physically Grounded Vision-Language Models for Robotic Manipulation](https://arxiv.org/abs/2309.02561)

**[2023.9.7]**
- Baichuan Intelligence Releases Baichuan2 Big Model: Comprehensively Ahead of Llama2, Training Slices Also Open Source: [Github](https://github.com/baichuan-inc/Baichuan2)/[Technical Report](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)

**[2023.9.6]**
- Paper: [TouchStone: Evaluating Vision-Language Models by Language Models](https://arxiv.org/abs/2308.16890)

**[2023.9.5]**
- [70 billion parameters Llama 2 training accelerated 195%! Training/fine-tuning/reasoning full-process program open source, 0 code one-stop solution!](https://github.com/hpcaitech/ColossalAI)
    
- Survey Paper:[Large language models in medicine: the potentials and pitfalls](https://arxiv.org/abs/2309.00087)

**[2023.9.4]**
- Paper:[SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding](https://arxiv.org/abs/2308.10529)

**[2023.9.3]**
- Paper:[PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation](https://arxiv.org/pdf/2308.13746.pdf)

**[2023.9.2]**
- Paper: [SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection](https://arxiv.org/pdf/2308.12863v1.pdf)

**[2023.9.1]**
- 8 LLM products are fully open to the whole community, including IOS and Android APP„Äê[Baidu](https://yiyan.baidu.com), [ÁôæÂ∑ùÊô∫ËÉΩ](www.baichuan-ai.com), [SenseChat]( https://chat.sensetime.com), Êô∫Ë∞±Ê∏ÖË®Ä, ByteDance, INTERN, CAS, MiniMax„Äë
- Paper:[FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer](https://arxiv.org/pdf/2111.13824.pdf)

**[2023.8.31]**
- Paper:[FLatten Transformer: Vision Transformer using Focused Linear Attention](http://arxiv.org/pdf/2308.00442.pdf)

**[2023.8.30]**
- Paper: [SeamlessM4T‚ÄîMassively Multilingual & Multimodal Machine Translation](https://ai.meta.com/research/publications/seamless-m4t/)

**[2023.8.29]**
- Paper:[Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882)

**[2023.8.28]**
- Paper:[ExpeL: LLM Agents Are Experiential Learners](https://arxiv.org/abs/2308.10144)

**[2023.8.27]**
- Paper:[OVO: One-shot Vision Transformer Search with Online distillation](https://arxiv.org/pdf/2212.13766.pdf)

**[2023.8.26]**
- WizardLM: Open-source! \[[demo](http://47.103.63.15:50085/) / [HuggingFace](https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0) / [github](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder)\]
    
**[2023.8.25]**
- Paper:[Safe Reinforcement Learning via Probabilistic Logic](https://arxiv.org/abs/2303.03226)

**[2023.8.24]**
- Paper:[Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882 )
    
- Paper:[Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)

**[2023.8.23]**
- [HuggingFace Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model](https://huggingface.co/blog/idefics)
- Paper:[SeamlessM4T‚ÄîMassively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)

**[2023.8.22]**
- Paper:[RoboAgent:Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking](https://robopen.github.io/media/roboagent.pdf)

**[2023.8.21]**
- Paper:[VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use](https://arxiv.org/abs/2308.06595)
- Paper:[Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering](https://arxiv.org/abs/2308.07411)

**[2023.8.20]**
- Paper:[DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997)

**[2023.8.19]**
- WizardMath: [model checkpoints](https://huggingface.co/WizardLM/WizardMath-70B-V1.0) / [project page](https://github.com/victorsungo/WizardLM/tree/main/WizardMath) / [Paper](https://github.com/nlpxucan/WizardLM)

**[2023.8.18]**
- Paper:[Shepherd: A Critic for Language Model Generation](https://arxiv.org/pdf/2308.04592.pdf)

**[2023.8.17]**
- Paper:[Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval](https://arxiv.org/pdf/2308.07648v1.pdf)

**[2023.8.16]**
- Paper:[The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation](https://arxiv.org/abs/2308.07286 )
    
**[2023.8.15]**
- Paper:[Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf)
    
**[2023.8.14]**
- Paper:[VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](https://arxiv.org/abs/2305.10874)
    
**[2023.8.13]**
- Paper:[SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support' qiuhuachuan](https://huggingface.co/qiuhuachuan/MeChat)

**[2023.8.12]**
- Paper:[Pre-Trained Large Language Models for Industrial Control](http://export.arxiv.org/abs/2308.03028)

**[2023.8.11]**
- Paper:[Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/abs/2308.04623)

**[2023.8.10]**
- Paper:[Foundational Models Defining a New Era in Vision: A Survey and Outlook](https://arxiv.org/abs/2307.13721)

**[2023.8.9]**
- [Stability AI has just announced the release of StableCode, its very first LLM generative AI product for coding](https://stability.ai/blog/stablecode-llm-generative-ai-coding)
- Paper:[Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals](https://arxiv.org/abs/2308.02510)

**[2023.8.8]**
- Paper:[AgentBench: Evaluating LLMs as Agents](https://arxiv.org/pdf/2308.03688.pdf)

**[2023.8.7]**
- Paper:[SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization](https://www.usenix.org/system/files/atc23-zhai.pdf)

**[2023.8.6]**
- Paper: [UniVTG: Towards Unified Video-Language Temporal Grounding](https://arxiv.org/abs/2307.16715)

**[2023.8.5]**
- Paper: [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)

**[2023.8.4]**
- [Chinese LLaMA2 model is open source and commercially usable](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b)
- Paper:[Scientific discovery in the age of artificial intelligence](https://www.nature.com/articles/s41586-023-06221-2)
    
**[2023.8.3]**
- Paper: [Scaling Data Generation in Vision-and-Language Navigation](https://arxiv.org/pdf/2307.15644.pdf)

**[2023.8.2]**
- Paper: [AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://arxiv.org/pdf/2307.16368v1.pdf)

**[2023.8.1]**
- Paper:[Robust Distortion-free Watermarks for Language Models](https://arxiv.org/abs/2307.15593)


**[2023.7.31]**
- Paper: [Foundational Models Defining a New Era in Vision: A Survey and Outlook](https://arxiv.org/abs/2307.13721)

**[2023.7.30]**
- Paper: [Challenges and Applications of Large Language Models](https://arxiv.org/abs/2307.10169)

**[2023.7.29]**
- Paper: [A Watermark for Large Language Models](https://openreview.net/forum?id=aX8ig9X2a7 )

**[2023.7.28]**
- Paper:[Med-Flamingo: a Multimodal Medical Few-shot Learner](https://arxiv.org/pdf/2307.15189v1.pdf)

**[2023.7.27]**
- Paper: [using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)

**[2023.7.26]**
- Paper:[3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)

**[2023.7.25]**
- Paper:[ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning](https://arxiv.org/abs/2307.09474)

**[2023.7.24]**
- Paper:[Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)

**[2023.7.23]**
- [ChatGPT based on Android, pre-register location](https://play.google.com/store/apps/details?id=com.openai.chatgpt)

- Paper:[OBJECT 3DIT: Language-guided 3D-aware Image Editing](https://arxiv.org/abs/2307.11073)
    
**[2023.7.22]**
- Paper:[Brain2Music: Reconstructing Music from Human Brain Activity](https://arxiv.org/abs/2307.11078)

**[2023.7.21]**
- Paper:[A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)
    
**[2023.7.20]**
- New Architecture: RetNetwork, beyond Transformer üëâ[Paper](https://arxiv.org/abs/2307.08621 )üëà

**[2023.7.19]**
- [Meta releases LLAMA 2, open source and commercially available.](https://ai.meta.com/llama/)
    
**[2023.7.18]**
- Paper:[Learning to Retrieve In-Context Examples for Large Language Models](https://arxiv.org/abs/2307.07164)
    
**[2023.7.17]**
- Paper:[HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://arxiv.org/abs/2307.06949)
    
**[2023.7.16]**
- Emu model is open source, a versatile expert in 'multimodal to multimodal': [Model](https://github.com/baaivision/Emu) / [Demo](https://emu.ssi.plus/)
    
**[2023.7.15]**
- Paper:[Self-consistency for open-ended generations](https://arxiv.org/abs/2307.06857)
    
**[2023.7.14]**
- Paper:[Patch n‚Äô Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/pdf/2307.06304.pdf)
    
**[2023.7.13]**
- Paper:[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)

**[2023.7.12]**
- Claude2üëâ **\[Paper\]**[Model Card and Evaluations for Claude Models](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf) / **[Website]**(https://claude.ai/)

**[2023.7.11]**
- Paper: [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/voxposer.pdf)
- Paper: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)

**[2023.7.10]**
- Paper:[Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)
    
**[2023.7.9]**
- Paper: [Schema-learning and rebinding as mechanisms of in-context learning and emergence](https://arxiv.org/abs/2307.01201)

**[2023.7.8]**
- Paper:[SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs](https://arxiv.org/abs/2306.17842)

**[2023.7.7]**
- [GPT-4 API general availability](https://openai.com/blog/gpt-4-api-general-availability)
- Paper:[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)
    
**[2023.7.6]**
- Paper:[LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)

**[2023.7.5]**
- [MetaGPT: Multi-Role Meta-Programming Framework](https://github.com/geekan/MetaGPT)
- Paper: [Conformer LLMs -- Convolution Augmented Large Language Models](https://arxiv.org/abs/2307.00461)

**[2023.7.4]**
- Paper:[Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic](http://arxiv.org/abs/2306.15195)
    
**[2023.7.3]**
- [Largest-scale Chinese multitask instruction set, introducing a thousand Chinese datasets.](https://huggingface.co/datasets/BAAI/COIG-PC)
    
**[2023.7.2]**
- Paper: [Towards Measuring the Representation of Subjective Global Opinions in Language Models](https://arxiv.org/abs/2306.16388)
    
**[2023.7.1]**
- Paper:[Masked Vision-language Transformer in Fashion](https://link.springer.com/article/10.1007/s11633-022-1394-4)
    
**[2023.6.30]**
- Paper: [Inferring the Goals of Communicating Agents from Actions and Instructions](https://arxiv.org/abs/2306.16207)

**[2023.6.29]**
- Paper: [AudioPaLM: A Large Language Model That Can Speak and Listen](https://arxiv.org/pdf/2306.12925.pdf)

**[2023.6.28]**
- Paper:[Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs//2306.14824)

**[2023.6.27]**
- Paper: [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549v1.pdf)
    
**[2023.6.26]**
- Paper: [PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance](https://arxiv.org/abs/2306.05443)

**[2023.6.25]**
- Paper: [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561)

**[2023.6.24]**
- Paper: [PromptIR: Prompting for All-in-One Blind Image Restoration](https://www.researchgate.net/publication/371786106_PromptIR_Prompting_for_All-in-One_Blind_Image_Restoration)

**[2023.6.23]**
- Paper: [OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue](https://arxiv.org/format/2306.12174)
    
**[2023.6.22]**
- [Stanford has released an automatic evaluation system for LLM called AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)
- [Ocean-1: the world's first contact center foundation model.](https://cresta.com/blog/introducing-ocean-1-worlds-first-contact-center-foundation-model/)

**[2023.6.21]**
- [GPT-Engineering, Who generates an entire codebase based on a prompt.](https://github.com/AntonOsika/gpt-engineer)

**[2023.6.20]**
- Paper: [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://scontent-fra3-1.xx.fbcdn.net/v/t39.8562-6/354636794_599417672291955_3799385851435258804_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=PW3or0UVoKoAX-2_D0q&_nc_ht=scontent-fra3-1.xx&oh=00_AfB2SW8Rp55YKG0AEJcpC9tUECbXdl_m83yk9cxX7jie1A&oe=64967631)
    
**[2023.6.19]**
- Technical Report: [AIGC industry overview article from SEALAND SECURITIES](https://pdf.dfcfw.com/pdf/H3_AP202303201584404280_1.pdf?1679327615000.pdf)

**[2023.6.18]**
- Paper: [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory](https://arxiv.org/abs/2306.03901)
    
**[2023.6.17]**
- Paper: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://www.researchgate.net/publication/371540959_Macaw-LLM_Multi-Modal_Language_Modeling_with_Image_Audio_Video_and_Text_Integration)

**[2023.6.16]**
- Financial [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) model open source, benchmarked against BloombergGPT, training parameters can be reduced from 6.17 billion to 3.67 million, can predict stock prices. ([Paper](https://arxiv.org/abs/2306.06031)/[Code](https://github.com/AI4Finance-Foundation/FinGPT))

**[2023.6.15]**
- Paper:[MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://arxiv.org/abs/2306.01311 )
    
**[2023.6.14]**
- Paper:[XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models](https://arxiv.org/abs/2306.07971)
    
**[2023.6.13]**
- Paper:[Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
 
**[2023.6.12]**
- Paper: [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425)
    
**[2023.6.11]**
- Paper: [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/abs/2306.05179)

**[2023.6.10]**
- [Aquila, language model series, including the Aquila Basic Model (7B and 33B), AquilaChat dialogue model, and AquilaCode text-to-code generation model.](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)
    
**[2023.6.9]**
- Paper: [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)

**[2023.6.8]**
- Paper: [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/pdf/2305.05176.pdf)
    
**[2023.6.7]**
- Paper: [XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech](https://arxiv.org/abs/2305.19709)
    
**[2023.6.6]**
- Paper: [XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters](https://arxiv.org/abs/2305.12002)
- Paper: [UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild](https://arxiv.org/abs/2305.11147)

**[2023.6.5]**
- Paper: [Controllable Text-to-Image Generation with GPT-4](https://arxiv.org/abs/2305.18583)

**[2023.6.4]**
- PandaGPT: One model unifies six modalities([Page](https://panda-gpt.github.io/)/[Paper](https://arxiv.org/abs/2305.16355))

**[2023.6.3]**
- Paper: [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)

**[2023.6.2]**
- Paper: [SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/abs/2305.17390)

**[2023.6.1]**
- Paper: [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216)

**[2023.5.31]**
- [Intel announces the Aurora genAI, which is generative AI model with 1 trillion parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/)
    
- HuotuoGPT, towards Taming Language Model to Be a Doctor([Github](https://github.com/FreedomIntelligence/HuatuoGPT)/[Demo](https://www.huatuogpt.cn)/[Paper](https://arxiv.org/pdf/2305.15075.pdf))
- Paper: [Large Language Models Meet NL2Code: A Survey](https://arxiv.org/abs/2212.09420)

**[2023.5.30]** 
- Paper: [Large Language Models as Tool Makers](https://arxiv.org/pdf/2305.17126.pdf)
    
- Paper: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100)

- Paper: [OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities](https://arxiv.org/pdf/2305.16334.pdf)

**[2023.5.29]**

- [Falcon: based on the 1 trillion token open-source large model, surpassing 65 billion LLaMA, commercially available.](https://huggingface.co/tiiuae)

- [ToolBench: the open platform for large language models used for training, service, and evaluation of tool learning](https://github.com/Navezjt/ToolBench)
- Paper: [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/abs/2305.15021)

**[2023.5.28]**
- Paper: [ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation](https://arxiv.org/abs/2305.16213)
- Paper:[RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text](https://arxiv.org/abs/2305.13304)

**[2023.5.27]**
- Paper: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)

- Paper: [ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs](https://arxiv.org/pdf/2305.15964v1.pdf)

- Paper: [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720)

**[2023.5.26]**
- Paper: [Iterative Forward Tuning Boosts In-context Learning in Language Models](https://arxiv.org/pdf/2305.13016.pdf)
    
**[2023.5.25]**
- Paper: [Diversity-Aware Meta Visual Prompting](https://arxiv.org/abs/2303.08138)
    
**[2023.5.24]**
- Paper:[VideoLLM: Modeling Video Sequence with Large Language Models](https://arxiv.org/pdf/2305.13292.pdf)

**[2023.5.23]**
- Paper:[Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)
- Paper: [PointGPT: Auto-regressively GenerativePre-training from Point Clouds](https://arxiv.org/pdf/2305.11487.pdf)

**[2023.5.22]**
- Paper: [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://arxiv.org/abs/2305.10266)
    
**[2023.5.21]**
- Paper:[Language Models Meet World Models: Embodied Experiences Enhance Language Models](https://arxiv.org/abs/2305.10626)

**[2023.5.20]**
- Paper:[AttentionViz: A Global View of Transformer Attention](https://arxiv.org/pdf/2305.03210.pdf)
    
**[2023.5.19]**
- [OpenAI introducing the ChatGPT APP for IOS](https://apps.apple.com/app/openai-chatgpt/id6448311069)
- Paper: [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)
    
**[2023.5.18]**
- Paper: [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)
    
**[2023.5.17]**
- Paper: [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)
    
**[2023.5.16]**
- Paper:[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185.pdf)

**[2023.5.15]** 
- Paper: [ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4](https://arxiv.org/pdf/2305.07490.pdf)

**[2023.5.14]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301v1.pdf)

**[2023.5.13]**
- Paper: [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)

**[2023.5.12]**
- Paper: [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)
    
**[2023.5.11]**
- Google has released PaLM 2, which improves multiple abilities and offers four versions for selection. ([Paper](https://event-cdn.baai.ac.cn/file/file-browser/KDtjMkep6E6n5XjRNJjknjewCF7Pcebx.pdf)/[Page](https://makersuite.google.com/waitlist ))
- [The open-source healthcare large language model NHS-LLM and OpenGPT.](https://github.com/CogStack/opengpt)

- Paper: [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)

**[2023.5.10]** 
- DetGPT: Detect What You Need via Reasoning ([Code](https://github.com/OptimalScale/DetGPT)/[Demo](https://detgpt.github.io/))

- Meta releases a large-scale model called ImageBind that can traverse six senses, and it is now open-source. ([Paper](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)/[Code](https://github.com/facebookresearch/ImageBind))

- [HuoTuo: Open Source Chinese Medical Large Model of Harbin Institute of Technology](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)

- Paper: [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/pdf/2305.04160.pdf)

**[2023.5.9]**

- Paper: [Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278) „Äê[Code](https://github.com/VPGTrans/VPGTrans)„Äë

**[2023.5.8]**
- PandaLM: the first large model for automated evaluation.([Code](https://github.com/WeOpenML/PandaLM))
    
- Paper:[AutoML-GPT: Automatic Machine Learning with GPT](https://arxiv.org/abs/2305.02499)

**[2023.5.7]**
- Paper: [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf)
    
**[2023.5.6]**
- [IFLYTEK officially releases the LLM named SparkDesk](https://xinghuo.xfyun.cn/)

- OpenAI release the language-to-3D model: Shape.E ([Paper](https://arxiv.org/pdf/2305.02463.pdf)/[Project Page](https://github.com/openai/shap-e))

- Paper: [Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks](https://arxiv.org/abs/2304.14732)
    
**[2023.5.5]** 
- Paper: [Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner](https://arxiv.org/pdf/2305.01711.pdf)

**[2023.5.4]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301.pdf)

**[2023.5.3]**
- Paper:[Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)


**[2023.5.2]** 
- Customize your own LLMs, stop prompt-tunning: [Lamini](https://lamini.ai/)

- Paper: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/pdf/2305.01625.pdf)

**[2023.5.1]**
- Paper: [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/pdf/2304.14178.pdf)

**[2023.4.30]** 
- Paper: [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712)

**[2023.4.29]**
- Paper: [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/pdf/2304.11477.pdf)

**[2023.4.28]**

- Paper: [Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System](https://arxiv.org/pdf/2304.13343.pdf)

**[2023.4.27]** 
- AudioGPT: [[Project Page](https://github.com/AIGC-Audio/AudioGPT)/[Paper](https://arxiv.org/pdf/2304.12995.pdf)]

- Paper:[Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/pdf/2304.13007.pdf)

**[2023.4.26]** 
- Paper: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2304.12244.pdf)

**[2023.4.25]** 
- [Google releases the Security AI workbench](https://cloud.google.com/blog/products/identity-security/rsa-google-cloud-security-ai-workbench-generative-ai)

- Paper: [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models](https://arxiv.org/pdf/2304.11657.pdf)

**[2023.4.24]** 
- Paper: [Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback](https://arxiv.org/pdf/2304.10750.pdf)

**[2023.4.23]** 
- Paper: [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/pdf/2304.11158.pdf)

**[2023.4.22]** Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[Paper](https://arxiv.org/abs/2304.09842)/[Project](https://chameleon-llm.github.io)]

**[2023.4.21]**
- Paper:

    [Progressive-Hint Prompting Improves Reasoning in Large Language Models](https://arxiv.org/pdf/2304.09797.pdf)

    [Pretrained Language Models as Visual Planners for Human Assistance](https://arxiv.org/pdf/2304.09179.pdf)

**[2023.4.20]**

- [Google DeepMind: Bringing together two world-class AI teams](https://blog.google/technology/ai/april-ai-update/)

- Paper: [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)

**[2023.4.19]** 
    
- Paper: [Towards Robust Prompts on Vision-Language Models](https://arxiv.org/pdf/2304.08479.pdf)

**[2023.4.18]**
-   [HuaTuo:Tunning LLaMA Model with chinese medical instructions](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)
-   MiniGPT-4 [[Project Page](https://minigpt-4.github.io/)/[Paper](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf)]

- Paper: 
        [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text](https://arxiv.org/abs/2304.06939 )

**[2023.4.17]** 
- The open source democratizes large language models,**OpenAssistant**, supports 35 languages, and can use RLHF data for free[[Project Page](https://open-assistant.io/chat)/[Code](ttps://github.com/LAION-AI/Open-Assistant)/[Paper](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view )]

- Paper: [Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition](https://arxiv.org/pdf/2304.04704.pdf)

**[2023.4.16]** 
- Paper: [AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models](https://arxiv.org/pdf/2304.06364.pdf)

**[2023.4.15]** 
- Paper: [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society](https://arxiv.org/abs/2303.17760)

- [Visual Med-Alpaca: Bridging Modalities in Biomedical Language Models](https://cambridgeltl.github.io/visual-med-alpaca/)

**[2023.4.14]**

- [Amazon announcing new tools for building with Generative AI](https://aws.amazon.com/cn/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/)

- Paper: [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://arxiv.org/pdf/2304.05613.pdf)

**[2023.4.13]** Three Amazing Works:

- AutoGPT: An Autonomous GPT-4 Experiment üëâ[Code](https://github.com/torantulino/auto-gpt)üëà

- [Databricks releases Dolly 2.0, the first open, instruction-following LLM for commercial use](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)

- [Microsoft released the DeepSpeed Chat: Own your ChatGPT](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)

**[2023.4.12]** [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/pdf/2304.04370.pdf)

**[2023.4.11]** [Why think step-by-step? Reasoning emerges from the locality of experience](https://arxiv.org/pdf/2304.03843.pdf)

**[2023.4.10]** [TagGPT: Large Language Models are Zero-shot Multimodal Taggers](https://arxiv.org/pdf/2304.03022.pdf)

**[2023.4.9]** A new AI model from Meta AI: Segment Anything Model (SAM) ([Paper](https://arxiv.org/pdf/2304.02643.pdf)/[Code](https://github.com/facebookresearch/segment-anything))

**[2023.4.8]** EleutherAI&Yale et al. proposed a large-scale language model analysis suite that spans training and extension: Pythia ([Paper](https://arxiv.org/pdf/2304.01373.pdf)/[Code](https://github.com/EleutherAI/pythia))

**[2023.4.7]** [Stanford releases the 7 billion parameter open-source model Vicuna-7B, which is compact, efficient, but powerful in functionality](https://vicuna.lmsys.org/)

**[2023.4.6]** [Effective Theory of Transformers at Initialization](https://arxiv.org/pdf/2304.02034.pdf)

**[2023.4.5]** [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf)

**[2023.4.4]** [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?](https://arxiv.org/pdf/2303.18240.pdf)

**[2023.4.3]** [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/pdf/2303.17651.pdf)

**[2023.4.1]** [A survey of Large Language Models](https://arxiv.org/abs/2303.18223)

**[2023.3.31]** [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)

**[2023.3.30]** [GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/pdf/2303.16634.pdf)

**[2023.3.29]** [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf.)

**[2023.3.28]** [ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks](https://arxiv.org/pdf/2303.15056.pdf)

**[2023.3.27]** [Scaling Expert Language Models with Unsupervised Domain Discovery](https://arxiv.org/pdf/2303.14177.pdf)
 
**[2023.3.26]** [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)

**[2023.3.23]** [OpenAI announces 'Plug-ins' for ChatGPT that enable it to perform actions beyond text.](https://platform.openai.com/docs/plugins/introduction)

**[2023.3.22]** [GitHub launches Copilot X, aiming at the future of AI-powered software development.](https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/) 

**[2023.3.21]** [Google Bard is now available in the US and UK, w/ more countries to come.](https://bard.google.com) 

**[2023.3.20]** OpenAI‚Äôs new paper looks at the economical impact of LLMs+Labor Market.[GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) 

**[2023.3.17]** [Microsoft 365 Copilot released. Word, Excel, PowerPoint, Outlook powered by LLMs.](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)

**[2023.3.16]**  Baidu announcing the LLM named ["ÊñáÂøÉ‰∏ÄË®Ä"(ERNIE3.0 + PLATO)](https://yiyan.baidu.com/welcome) 

**[2023.3.15]** Two Breaking News:
    -  Announcing [GPT4](https://openai.com/product/gpt-4) by OpenAI from Microsoft. **[Paperüîó](https://cdn.openai.com/papers/gpt-4.pdf)**
    -  Announcing [PaLM](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html) API by Google. 

**[2023.3.13]** [LLaMA has been fine-tuned by Stanford](https://github.com/tatsu-lab/stanford_alpaca)

**[2023.3.10]** [Announcing OpenChatKit by Together](https://huggingface.co/spaces/togethercomputer/OpenChatKit)

**[2023.3.9]**  GPT-4 is coming next week and it will be multimodal,announced by OpenAI.

**[2023.3.8]** [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)

**[2023.3.7]** [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)

**[2023.3.6]** [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)