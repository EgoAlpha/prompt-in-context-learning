# üí• History News

## 2023

‚òÑÔ∏è **EgoAlpha releases the TrustGPT focuses on reasoning. Trust the GPT with the strongest reasoning abilities for authentic and reliable answers. You can click [here](https://trustgpt.co) or visit the [Playgrounds](./Playground.md) directly to experience it„ÄÇ**

**[2024.2.27]**
- [Mistral has released Mistral Large, with an MMLU rating second only to GPT-4, 32K contexts, no Chinese support, and API calls via La Plateforme and Azure.](https://chat.mistral.ai%20/)
- Paper: [Instruct-Imagen: Image Generation with Multi-modal Instruction](https://arxiv.org/abs/2401.01952)

**[2024.2.26]**
- Paper: [MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property](https://arxiv.org/pdf/2402.16389v1.pdf)

**[2024.2.25]**
- Paper: [Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations](https://arxiv.org/pdf/2308.16505v3.pdf)

**[2024.2.24]**
- Paper: [ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing](https://arxiv.org/pdf/2402.16445.pdf)

**[2024.2.23]**
- Paper: [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)

**[2024.2.22]**
- Paper: [Video ReCap: Recursive Captioning of Hour-Long Videos](https://arxiv.org/abs/2402.13250)

**[2024.2.21]**
- Paper: [TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249)

**[2024.2.20]**
- Paper: [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive](https://arxiv.org/abs/2402.13228)

**[2024.2.19]**
- Paper: [How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts](https://arxiv.org/abs/2402.13220)

**[2024.2.18]**
- Paper: [Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation](https://arxiv.org/abs/2402.10210)

**[2024.2.17]**
- Paper: [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200)

**[2024.2.16]**
- üî•üî•üî•[SORA: Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)

**[2024.2.15]**
- Paper: [PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs](https://arxiv.org/abs/2402.07872)
- Paper:[VisualWebArena: EVALUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS](https://arxiv.org/pdf/2401.13649.pdf)

**[2024.2.14]**
- Paper: [Large Language Models for Autonomous Driving: Real-World Experiments](https://arxiv.org/abs/2312.09397)

**[2024.2.13]**
- Paper: [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627)

**[2024.2.12]**
- üî•üî•üî•[The year of AI video generation exploded!](https://a16z.com/why-2023-was-ai-videos-breakout-year-and-what-to-expect-in-2024/)

**[2024.2.11]**
- Paper: [Large Language Model Meets Graph Neural Network in Knowledge Distillation](https://arxiv.org/abs/2402.05894)

**[2024.2.10]**
- Paper: [FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs](https://arxiv.org/abs/2402.05904)

**[2024.2.9]**
- Paper: [Efficient Stagewise Pretraining via Progressive Subnetworks](https://arxiv.org/abs/2402.05913)

**[2024.2.8]**
- Paper: [On the Convergence of Zeroth-Order Federated Tuning in Large Language Models](https://arxiv.org/abs/2402.05926)

**[2024.2.7]**
- Paper: [SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://arxiv.org/abs/2402.05935)

**[2024.2.6]**
- Paper: [How Well Can LLMs Negotiate? NEGOTIATIONARENA Platform and Analysis](https://arxiv.org/abs/2402.05863)

**[2024.2.5]**
- Paper: [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868)

**[2024.2.4]**
- Paper: [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861)

**[2024.2.3]**
- Paper: [Can Large Language Models Understand Context?](https://arxiv.org/abs/2402.00858)

**[2024.2.2]**
- Paper: [Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.00787)

**[2024.2.1]**
- Paper: [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://simg.baai.ac.cn/paperfile/591f8fb9-d377-4926-8dd8-7effaf0232ad.pdf)

**[2024.1.31]**
- Paper: [Binding Touch to Everything: Learning Unified Multimodal Tactile Representations](https://simg.baai.ac.cn/paperfile/2847c7b0-e2e5-4169-96e8-4a233fb82e5c.pdf)

**[2024.1.30]**
- Paper: [Scaling Sparse Fine-Tuning to Large Language Models](https://simg.baai.ac.cn/paperfile/7990a7f2-2968-455e-ba0d-e5ce6c692191.pdf)

**[2024.1.29]**
- Paper: [InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models](https://simg.baai.ac.cn/paperfile/898eb205-ce3d-44e0-9104-91f9a6a8b71f.pdf)

**[2024.1.28]**
- Paper: [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities](https://simg.baai.ac.cn/paperfile/b9aad3a5-4943-463c-8983-0b5bfe38fcc6.pdf)

**[2024.1.27]**
- Survey Paper: [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601)

**[2024.1.26]**
- Paper: [ChipNeMo: Domain-Adapted LLMs for Chip Design](https://arxiv.org/abs/2311.00176)

**[2024.1.25]**
- Paper: [LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488)

**[2024.1.24]**
- Paper: [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/abs/2401.08577)

**[2024.1.23]**
- [Yi Visual Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.](https://huggingface.co/01-ai)

**[2024.1.22]**
- Paper: [Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection](https://arxiv.org/abs/2401.03737)

**[2024.1.21]**
- Paper: [Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation](https://arxiv.org/pdf/2401.10186v1.pdf)

**[2024.1.20]**
- Paper: [UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer](https://arxiv.org/abs/2401.06426)

**[2024.1.19]**
- [Stability AI releases Stable Code 3B, a new code model that rivals the 7 billion Code Llama and can run without a GPU!](https://stability.ai/news/stable-code-2024-llm-code-completion-release)

**[2024.1.18]**
- Paper: [ChatQA: Building GPT-4 Level Conversational QA Models](https://arxiv.org/pdf/2401.10225v1.pdf)

**[2024.1.17]**
- Paper: [Ask the experts: sourcing high-quality datasets for nutritional counselling through Human-AI collaboration](https://simg.baai.ac.cn/paperfile/3b7e4820-692a-424c-bafc-8498d28330cc.pdf)

**[2024.1.16]**
- Paper: [Machine Transla-on with Large Language Models: Prompt Engineering for Persian, English, and Russian Direc-ons](https://simg.baai.ac.cn/paperfile/5939e79e-20c9-46e1-93d5-5b29535d292f.pdf)

**[2024.1.15]**
- Paper: [TOFU: A Task of Fictitious Unlearning for LLMs](https://simg.baai.ac.cn/paperfile/ccd571d3-7b4b-4355-975b-6ca81d882f5b.pdf)

**[2024.1.14]**
- Technical Report: [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/DeepSeekMoE.pdf)

**[2024.1.13]**
- [GPT Store is now online!](https://openai.com/blog/introducing-the-gpt-store)
- Paper: [MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation](https://arxiv.org/abs/2401.04468)
  

**[2024.1.12]**
- Survey Paper: [Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives](https://arxiv.org/abs/2312.11970)

**[2024.1.11]**
- Paper: [Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects](https://arxiv.org/abs/2312.05278)

**[2024.1.10]**
- Paper: [Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models](https://arxiv.org/abs/2401.03105v1)

**[2024.1.9]**
- Paper: [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://arxiv.org/abs/2401.04398v1)

**[2024.1.8]**
- Paper: [WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation](https://arxiv.org/abs/2312.14187)

**[2024.1.7]**
- Paper: [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415v1)

**[2024.1.6]**
- Paper: [Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition](https://arxiv.org/abs/2401.02417v1)

**[2024.1.5]**
- Paper: [Learning to Prompt with Text Only Supervision for Vision-Language Models](https://arxiv.org/abs/2401.02418v1)

**[2024.1.4]**
- Paper: [LLM Augmented LLMs: Expanding Capabilities through Composition](https://arxiv.org/abs/2401.02412v1)

**[2024.1.3]**
- Paper:[CogAgent: A Visual Language Model for GUI Agents](https://arxiv.org/abs/2312.08914)
- Paper: [TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones](https://arxiv.org/abs/2312.16862)

**[2024.1.2]**
- Paper:[MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886)

**[2024.1.1]**
- Paper:[Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math](https://arxiv.org/pdf/2312.17120.pdf)

**[2023.12.31]**
- [Hugging Face Year in Review: 2023, The Year of Open Source LLMs](https://huggingface.co/blog/2023-in-llms)

**[2023.12.30]**
- Paper: [Unsupervised embedding of trajectories captures the latent structure of scientific migration](https://www.pnas.org/doi/10.1073/pnas.2305414120)

**[2023.12.29]**
- KwaiAgents is a series of Agent-related works open-sourced by the KwaiKEG from Kuaishou Technology „Äê[Paper](https://arxiv.org/abs/2312.04889)/[Github](https://github.com/KwaiKEG/KwaiAgents?tab=readme-ov-file)„Äë

**[2023.12.28]**
- Paper: [EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI](https://arxiv.org/pdf/2312.16170v1.pdf)
- Paper: [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/pdf/2312.16171v1.pdf)

**[2023.12.27]**
- Paper: [The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction](https://arxiv.org/pdf/2312.13558v1.pdf)

**[2023.12.26]**
- Paper: [LORAMOE: REVOLUTIONIZING MIXTURE OF EXPERTS FOR MAINTAINING WORLD KNOWLEDGE IN LANGUAGE MODEL ALIGNMENT](https://simg.baai.ac.cn/paperfile/96f0cfd7-79c7-4110-88e5-4ea80a7fbc8d.pdf)

**[2023.12.25]**
- Survey Paper: [Retrieval-Augmented Generation for Large Language Models: A Survey](https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf)


**[2023.12.24]**
- Paper: [Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/pdf/2312.13401v1.pdf)

**[2023.12.23]**
- Paper: [AppAgent: Multimodal Agents as Smartphone Users](https://arxiv.org/pdf/2312.13771v1.pdf)

**[2023.12.22]**
- Paper: [A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS](https://simg.baai.ac.cn/paperfile/dba4a459-56f2-43e5-9217-db6e7047a57c.pdf)

**[2023.12.21]**
- Paper: [Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents](https://arxiv.org/abs/2311.11797v1)

**[2023.12.20]**
- Paper: [G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model](https://arxiv.org/abs/2312.11370v1)

**[2023.12.19]**
- Paper: [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550v1)

**[2023.12.18]**
- Paper: [Control Risk for Potential Misuse of Artificial Intelligence in Science](https://arxiv.org/abs/2312.06632)

**[2023.12.17]**
- Paper: [Mathematical discoveries from program search with large language models](https://www.nature.com/articles/s41586-023-06924-6)

**[2023.12.16]**
- üî•üî•üî•Paper: [Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied LMM-based Agent on Drones](https://arxiv.org/abs/2311.15033)

**[2023.12.15]**
- Paper: [Photorealistic Video Generation with Diffusion Models](https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf)

**[2023.12.14]**
- Paper: [LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios](https://arxiv.org/abs/2310.08348)
- Paper: [From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"](https://arxiv.org/abs/2312.06571)
- Paper: [WonderJourney: Going from Anywhere to Everywhere](https://arxiv.org/abs/2312.03884)

**[2023.12.13]**
- Paper: [How to Convert Any Text Into a Graph of Concepts](https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)

**[2023.12.12]**
- Paper: [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)

**[2023.12.11]**
- Thesis from CMU (Carnegie Mellon University), Juncheng Billy Li: [Towards Robust Large-scale Audio/Visual Learning](https://pan.baidu.com/s/1dMXmHdAxrpwhuicITwDkpA?pwd=52ia)

**[2023.12.10]**
- Survey Paper: [Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning](https://arxiv.org/abs/2303.10475)
- üî•üî•üî•[The first open source MoE Structure LLM is released!](https://x.com/MistralAI/status/1733150512395038967?s=20)

**[2023.12.9]**
- Paper: [Gemini: A Family of Highly Capable Multimodal Models](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)

**[2023.12.8]**
- Paper: [An Embodied Generalist Agent in 3D World](https://arxiv.org/abs/2311.12871 )

**[2023.12.7]**
- üî•üî•üî•[Gemini:The largest and most capable Google LLM is coming! Gemini Ultra, Gemini Pro, and Gemini Nano!](https://blog.google/technology/ai/google-gemini-ai/#scalable-efficient)

**[2023.12.6]**
- Paper: [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/pdf/2311.13878)

**[2023.12.5]**
- Paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

**[2023.12.4]**
- Paper: [Sequential Modeling Enables Scalable Learning for Large Vision Models](https://arxiv.org/abs/2312.00785)

**[2023.12.3]**
- Paper: [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/pdf/2311.03348.pdf)

**[2023.12.2]**
- [The PyTorch team has accelerated large model inference by a factor of 10, and with less than 1,000 lines of pure native PyTorch code!](https://pytorch.org/blog/accelerating-generative-ai-2/?)

**[2023.12.1]**
- Peking University's newest multimodal LLM open source: trained on mixed datasets and directly used for image-video tasks without modification: „Äê[arXiv](https://arxiv.org/pdf/2311.08046.pdf)/[Demo](https://huggingface.co/spaces/Chat-UniVi/Chat-UniVi)/[GitHub](https://github.com/PKU-YuanGroup/Chat-UniVi)/[HuggingFace](https://huggingface.co/Chat-UniVi)„Äë

**[2023.11.30]**
- Paper: [Learning skillful medium-range global weather forecasting](https://www.science.org/doi/10.1126/science.adi2336)

**[2023.11.29]**
- üî•üî•üî•[Text to Video, PIKA1.0 officially released!](https://pika.art/blog)
- Paper: [MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers](https://arxiv.org/abs/2311.15475)

**[2023.11.28]**
- [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://github.com/hao-ai-lab/LookaheadDecoding)

**[2023.11.27]**
- üî•üî•üî• Paper: [White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?](https://arxiv.org/abs/2311.13110)

**[2023.11.26]**
- [Use vision to make Prompt! Xiangyang Shen demonstrates IDEA Research Institute's new model, no training or fine-tuning, out-of-the-box!](https://trex-counting.github.io/)

**[2023.11.25]**
- Paper:[MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning](https://arxiv.org/abs/2311.10537)

**[2023.11.24]**
- Paper: [Social Motion Prediction with Cognitive Hierarchies](https://arxiv.org/pdf/2311.04726.pdf)

**[2023.11.23]**
- Paper: [3D-GPT: PROCEDURAL 3D MODELING WITH LARGE LANGUAGE MODELS.](https://arxiv.org/abs/2310.12945)

**[2023.11.22]**
- Paper: [Holographic codes from hyper invariant tensor networks](https://www.nature.com/articles/s41467-023-42743-z)

**[2023.11.21]**
- Paper: [CAMEL: Communicative Agents for ‚ÄúMind‚Äù Exploration of Large Language Model Society](https://ghli.org/camel.pdf)

**[2023.11.20]**
- Paper: [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631)

**[2023.11.19]**
- [OpenAI Late Night Change, Sam Altman Kicked Out, Former CTO Temporary Interim CEO](https://openai.com/blog/openai-announces-leadership-transition)
- Paper: [IN-CONTEXT LEARNING WITH ITERATIVE DEMON- STRATION SELECTION](https://arxiv.org/pdf/2310.09881.pdf)

**[2023.11.18]**
- Paper: [EMU VIDEO: Factorizing Text-to-Video Generation by Explicit Image Conditioning](https://emu-video.metademolab.com/assets/emu_video.pdf)

**[2023.11.17]**
- [DeepMind's big model on Science: 1-minute prediction of 10 days of weather data, 90% of the indicators beyond the strongest human model](https://www.science.org/doi/10.1126/science.adi2336)
- Paper: [Towards Verifiable Text Generation with Symbolic References](https://arxiv.org/abs/2311.09188 )


**[2023.11.16]**
- [Microsoft's late-night amplification: GPT-4, DALL-E 3, GPTs for free, self-research big model dedicated AI chip](https://www.theverge.com/2023/11/15/23960345/microsoft-cpu-gpu-ai-chips-azure-maia-cobalt-specifications-cloud-infrastructure)
- [DevOps finally has an exclusive big model, Ant and BYU jointly released](https://github.com/codefuse-ai/CodeFuse-DevOps-Model/tree/main)

**[2023.11.15]**
- üî•üî•üî•Paper: [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/abs/2311.08046)
- üî•üî•üî•Paper: [SpectralGPT: Spectral Foundation Model](https://arxiv.org/pdf/2311.07113v1.pdf)
- Paper: [CHATMAP : LARGE LANGUAGE MODEL INTERACTION WITH CARTOGRAPHIC DATA](https://arxiv.org/pdf/2310.01429.pdf)
- Paper: [EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images](https://arxiv.org/abs/2311.06400)
  

**[2023.11.14]**
- Paper: [Can LLMs Follow Simple Rules?](https://arxiv.org/abs/2311.04235 )

**[2023.11.13]**
- Paper: [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)

**[2023.11.12]**
- Paper: [Ziya2: Data-centric Learning is All LLMs Need](https://arxiv.org/abs/2311.03301)

**[2023.11.11]**
- Paper: [PILL:Plug Into LLM with Adapter Expert and Attention Gat](https://arxiv.org/pdf/2311.02126v1.pdf)

**[2023.11.10]**
- Paper: [Levels of AGI: Operationalizing Progress on the Path to AGI](https://arxiv.org/abs/2311.02462)
- Paper: [mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration](https://arxiv.org/abs/2311.04257)

**[2023.11.9]**
- Paper: [Pre-training LLMs using human-like development data corpus](https://arxiv.org/abs/2311.04666 )

**[2023.11.8]**
- Paper: [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/abs/2311.01449)

**[2023.11.7]**
- Paper: [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)

**[2023.11.6]**
- üî•üî•üî• 01.Ai first open source large models, the Yi series of large models: [Yi-34B](https://huggingface.co/01-ai/Yi-34B) and [Yi-6B](https://huggingface.co/01-ai/Yi-6B).
- üî•üî•üî• Elon Musk's xAI products in two consecutive releases: [PromptIDE](https://x.ai/prompt-ide/) & [Grok](https://grok.x.ai)

**[2023.11.5]**
- Paper: [GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond](https://arxiv.org/abs/2309.16583)

**[2023.11.4]**
- Paper: [RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://arxiv.org/abs/2311.01455)

**[2023.11.3]**
- Paper: [In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern](https://arxiv.org/abs/2310.13220)

**[2023.11.2]**
- [DeepMind Exposes Next-Generation AlphaFold, Prediction Accuracy Skyrockets by Nearly 10 Percent! The AlphaFold moment for DNA and RNA is here!](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf)

**[2023.11.1]**
- [NVIDIA releases ChipNeMo, a large model to design semiconductors and accelerate AI design chips](https://blogs.nvidia.com/blog/2023/10/30/llm-semiconductors-chip-nemo/ )
- Paper: [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791 )

**[2023.10.31]**
- Paper: [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680)

**[2023.10.30]**
- Paper: [GraphGPT: Graph Instruction Tuning for Large Language Models](https://arxiv.org/abs/2310.13023)

**[2023.10.29]**
- Paper: [Creative Robot Tool Use with Large Language Models](https://arxiv.org/abs/2310.13065 )

**[2023.10.28]**
- Paper: [Large Graph Models: A Perspective](https://arxiv.org/abs/2308.14522)

**[2023.10.27]**
- Paper: [SuperHF: Supervised Iterative Learning from Human Feedback](https://arxiv.org/abs/2310.16763 )

**[2023.10.26]**
- Paper: [Woodpecker: Hallucination Correction for Multimodal Large Language Models](https://arxiv.org/pdf/2310.16045.pdf)

**[2023.10.25]**
- Paper: [OpenAgents: An Open Platform for Language Agents in the Wild](https://arxiv.org/abs/2310.10634#)

**[2023.10.24]**
- Paper: [Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading](https://arxiv.org/abs/2310.05029)

**[2023.10.23]**
- Paper: [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://arxiv.org/abs/2310.11954)
- Paper: [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798)

**[2023.10.22]**
- Paper: [Uni3D: Exploring Unified 3D Representation at Scale](https://arxiv.org/abs/2310.06773)

**[2023.10.21]**
- Paper: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)

**[2023.10.20]**
- Paper: [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/fmti.pdf)
- Paper: [XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.02989.pdf)

**[2023.10.19]**
- Paper: [The Consensus Game: Language Model Generation via Equilibrium Search](http://arxiv.org/abs/2310.09139)

**[2023.10.18]**
- Paper: [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770)

**[2023.10.17]**
- Paper: [Table-GPT: Table-tuned GPT for Diverse Table Tasks](https://arxiv.org/abs/2310.09263)

**[2023.10.16]**
- Paper: [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)

**[2023.10.15]**
- Paper: [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226)

**[2023.10.14]**
- Paper: [How FaR Are Large Language Models From Agents with Theory-of-Mind?](http://arxiv.org/abs/2310.03051)

**[2023.10.13]**
- Paper: [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)

**[2023.10.12]**
- Paper: [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://arxiv.org/abs/2310.06452 )
- Paper: [Learning Interactive Real-World Simulators](https://arxiv.org/abs/2310.06114 )

**[2023.10.11]**
- Paper: [PB-LLM: Partially Binarized Large Language Models](https://arxiv.org/abs/2310.00034)

**[2023.10.10]**
- Paper: [MVDREAM:MULTI-VIEW DIFFUSION FOR 3D GENERATION](https://arxiv.org/pdf/2308.16512.pdf)

**[2023.10.9]**
- Paper: [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/pdf/2310.02239v2.pdf)

**[2023.10.8]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301)

**[2023.10.7]**
- Survey Paper: [A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future](https://arxiv.org/abs/2309.15402)

**[2023.10.6]**
- Paper: [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207)

**[2023.10.5]**
- Paper: [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530)

**[2023.10.4]**
- Paper: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

**[2023.10.3]**
- Paper: [CHAIN-OF-VERIFICATION REDUCES HALLUCINATION IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2309.11495)

**[2023.10.2]**
- Paper: [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499)

**[2023.10.1]**
- Paper: [Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2309.10313)

**[2023.9.30]**
- Paper: [Cumulative Reasoning with Large Language Models](https://arxiv.org/abs/2308.04371)

**[2023.9.29]**
- Paper: [Investigating the Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2309.10313)

**[2023.9.28]**
- Survey Paper: [Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/abs/2308.10792)

**[2023.9.27]**
- [Chinese LLaMA-2 tops the list, open source and commercially available! With a budget of one thousand yuan, training for half a day, the effect is comparable to mainstream large models.](https://github.com/hpcaitech/ColossalAI)
- [Lingxin Intelligence releases CharacterGLM: Play AI role-playing, 6B model is now open source.](https://huggingface.co/LingxinAI/CharacterGLM-6b)
- Paper: [Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement](https://arxiv.org/abs/2303.06705)

**[2023.9.26]**
- The biggest bug of the large model! The accuracy of the answers is almost zero, from GPT to Llama, none are spared: Paper:[The Reversal Curse: LLMs trained on ‚ÄúA is B‚Äù fail to learn ‚ÄúB is A‚Äù ](https://arxiv.org/abs/2309.12288)
- Paper: [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495)
- Paper: [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)

**[2023.9.25]**
- Paper: [Cell2Sentence: Teaching Large Language Models the Language of Biology](https://www.biorxiv.org/content/10.1101/2023.09.11.557287v1 )

**[2023.9.24]**
- [Writer model is open source, commercially available, and there are a total of 8 models.](https://huggingface.co/Writer)
- Paper: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

**[2023.9.23]**
- [Defeat GPT-4? 70 billion parameter Xwin-LM climbs up to the top of  Stanford AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)
- Paper: [End-to-End Speech Recognition Contextualization with Large Language Models](https://arxiv.org/abs/2309.10917)

**[2023.9.22]**
- [AgentVerse: A Framework for Multi-LLM Environment Simulation](https://github.com/OpenBMB/AgentVerse)
- [The performance of the 20 billion scale large model is comparable to Llama2-70B! It is completely open source, and everything from the base to the tools is well arranged.](https://github.com/InternLM/InternLM)
- Paper: [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419)

**[2023.9.21]**
- 34B parameter exceeds GPT-4! "Mathematical Universal Large Model" MAmmoTH open source: average accuracy rate increased by 29% ([Paper](https://arxiv.org/pdf/2309.05653.pdf)/[Project Page](https://tiger-ai-lab.github.io/MAmmoTH/))
- Paper: [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions](https://q-transformer.github.io/assets/q-transformer.pdf)

**[2023.9.20]**
- [Optimizing LLMs from a Dataset Perspective](https://lightning.ai/pages/community/tutorial/optimizing-llms-from-a-dataset-perspective/)
- Google DeepMind predicts 71 million genetic mutations, decrypts the human genetic code, and is now published in Science. It has been open-sourced.([Paper](https://www.deepmind.com/blog/alphamissense-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases )/[Science](https://www.science.org/doi/10.1126/science.adg7492 )/[Dataset](https://github.com/deepmind/alphamissense))
- Paper: [Replacing softmax with ReLU in Vision Transformers](https://arxiv.org/pdf/2309.08586.pdf)

**[2023.9.19]**
- Paper: [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519)

**[2023.9.18]**
- Survey Paper: [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864.pdf)

**[2023.9.17]**
- Paper: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)

**[2023.9.16]**
- Paper: [DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2309.03883.pdf)

**[2023.9.15]**
- Microsoft Open Sources EvoDiff: A New Generation of Protein Generative AI : [\[Paper\]](https://doi.org/10.1101/2023.09.11.556673 )
- Paper: [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://arxiv.org/pdf/2309.05173v1.pdf)

**[2023.9.14]**
- Can LLMs Really Reason and Plan? | blog @ CACM | Communications of the ACM„Äê[Paper](https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext)/[Video](https://www.youtube.com/watch?v=BmyB-4S9QuY )„Äë

**[2023.9.13]**
- Chinese multimodal large model VisCPM open API interface! The upgraded version is far more capable than similar models\([Paper](https://arxiv.org/pdf/2308.12038.pdf)/[Github](https://github.com/OpenBMB/VisCPM)\)
- Survey Paper: [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432v2.pdf)

**[2023.9.12]**
- Paper: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)

**[2023.9.11]**
- Paper:[Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning](https://arxiv.org/pdf/2303.09410.pdf)

**[2023.9.10]**
- Survey Paper: [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)

**[2023.9.9]**
- Open-source version of code interpreter tops GitHub hotlist, runs locally, accesses Internet: [\[Github\]](https://github.com/KillianLucas/open-interpreter/#commands)
- Peking University propose the Structured Chain of Thought SCoT: [\[Paper\]](https://arxiv.org/pdf/2305.06599.pdf)
- Paper:[LARGE LANGUAGE MODELS AS OPTIMIZERS](https://arxiv.org/pdf/2309.03409.pdf)
- Survey Paper:[RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model](https://arxiv.org/abs/2309.00810)

**[2023.9.8]**
- Paper:[Physically Grounded Vision-Language Models for Robotic Manipulation](https://arxiv.org/abs/2309.02561)

**[2023.9.7]**
- Baichuan Intelligence Releases Baichuan2 Big Model: Comprehensively Ahead of Llama2, Training Slices Also Open Source: [Github](https://github.com/baichuan-inc/Baichuan2)/[Technical Report](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)

**[2023.9.6]**
- Paper: [TouchStone: Evaluating Vision-Language Models by Language Models](https://arxiv.org/abs/2308.16890)

**[2023.9.5]**
- [70 billion parameters Llama 2 training accelerated 195%! Training/fine-tuning/reasoning full-process program open source, 0 code one-stop solution!](https://github.com/hpcaitech/ColossalAI)
    
- Survey Paper:[Large language models in medicine: the potentials and pitfalls](https://arxiv.org/abs/2309.00087)

**[2023.9.4]**
- Paper:[SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding](https://arxiv.org/abs/2308.10529)

**[2023.9.3]**
- Paper:[PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation](https://arxiv.org/pdf/2308.13746.pdf)

**[2023.9.2]**
- Paper: [SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection](https://arxiv.org/pdf/2308.12863v1.pdf)

**[2023.9.1]**
- 8 LLM products are fully open to the whole community, including IOS and Android APP„Äê[Baidu](https://yiyan.baidu.com), [ÁôæÂ∑ùÊô∫ËÉΩ](www.baichuan-ai.com), [SenseChat]( https://chat.sensetime.com), Êô∫Ë∞±Ê∏ÖË®Ä, ByteDance, INTERN, CAS, MiniMax„Äë
- Paper:[FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer](https://arxiv.org/pdf/2111.13824.pdf)

**[2023.8.31]**
- Paper:[FLatten Transformer: Vision Transformer using Focused Linear Attention](http://arxiv.org/pdf/2308.00442.pdf)

**[2023.8.30]**
- Paper: [SeamlessM4T‚ÄîMassively Multilingual & Multimodal Machine Translation](https://ai.meta.com/research/publications/seamless-m4t/)

**[2023.8.29]**
- Paper:[Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882)

**[2023.8.28]**
- Paper:[ExpeL: LLM Agents Are Experiential Learners](https://arxiv.org/abs/2308.10144)

**[2023.8.27]**
- Paper:[OVO: One-shot Vision Transformer Search with Online distillation](https://arxiv.org/pdf/2212.13766.pdf)

**[2023.8.26]**
- WizardLM: Open-source! \[[demo](http://47.103.63.15:50085/) / [HuggingFace](https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0) / [github](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder)\]
    
**[2023.8.25]**
- Paper:[Safe Reinforcement Learning via Probabilistic Logic](https://arxiv.org/abs/2303.03226)

**[2023.8.24]**
- Paper:[Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882 )
    
- Paper:[Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)

**[2023.8.23]**
- [HuggingFace Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model](https://huggingface.co/blog/idefics)
- Paper:[SeamlessM4T‚ÄîMassively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)

**[2023.8.22]**
- Paper:[RoboAgent:Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking](https://robopen.github.io/media/roboagent.pdf)

**[2023.8.21]**
- Paper:[VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use](https://arxiv.org/abs/2308.06595)
- Paper:[Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering](https://arxiv.org/abs/2308.07411)

**[2023.8.20]**
- Paper:[DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997)

**[2023.8.19]**
- WizardMath: [model checkpoints](https://huggingface.co/WizardLM/WizardMath-70B-V1.0) / [project page](https://github.com/victorsungo/WizardLM/tree/main/WizardMath) / [Paper](https://github.com/nlpxucan/WizardLM)

**[2023.8.18]**
- Paper:[Shepherd: A Critic for Language Model Generation](https://arxiv.org/pdf/2308.04592.pdf)

**[2023.8.17]**
- Paper:[Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval](https://arxiv.org/pdf/2308.07648v1.pdf)

**[2023.8.16]**
- Paper:[The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation](https://arxiv.org/abs/2308.07286 )
    
**[2023.8.15]**
- Paper:[Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf)
    
**[2023.8.14]**
- Paper:[VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](https://arxiv.org/abs/2305.10874)
    
**[2023.8.13]**
- Paper:[SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support' qiuhuachuan](https://huggingface.co/qiuhuachuan/MeChat)

**[2023.8.12]**
- Paper:[Pre-Trained Large Language Models for Industrial Control](http://export.arxiv.org/abs/2308.03028)

**[2023.8.11]**
- Paper:[Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/abs/2308.04623)

**[2023.8.10]**
- Paper:[Foundational Models Defining a New Era in Vision: A Survey and Outlook](https://arxiv.org/abs/2307.13721)

**[2023.8.9]**
- [Stability AI has just announced the release of StableCode, its very first LLM generative AI product for coding](https://stability.ai/blog/stablecode-llm-generative-ai-coding)
- Paper:[Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals](https://arxiv.org/abs/2308.02510)

**[2023.8.8]**
- Paper:[AgentBench: Evaluating LLMs as Agents](https://arxiv.org/pdf/2308.03688.pdf)

**[2023.8.7]**
- Paper:[SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization](https://www.usenix.org/system/files/atc23-zhai.pdf)

**[2023.8.6]**
- Paper: [UniVTG: Towards Unified Video-Language Temporal Grounding](https://arxiv.org/abs/2307.16715)

**[2023.8.5]**
- Paper: [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)

**[2023.8.4]**
- [Chinese LLaMA2 model is open source and commercially usable](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b)
- Paper:[Scientific discovery in the age of artificial intelligence](https://www.nature.com/articles/s41586-023-06221-2)
    
**[2023.8.3]**
- Paper: [Scaling Data Generation in Vision-and-Language Navigation](https://arxiv.org/pdf/2307.15644.pdf)

**[2023.8.2]**
- Paper: [AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://arxiv.org/pdf/2307.16368v1.pdf)

**[2023.8.1]**
- Paper:[Robust Distortion-free Watermarks for Language Models](https://arxiv.org/abs/2307.15593)


**[2023.7.31]**
- Paper: [Foundational Models Defining a New Era in Vision: A Survey and Outlook](https://arxiv.org/abs/2307.13721)

**[2023.7.30]**
- Paper: [Challenges and Applications of Large Language Models](https://arxiv.org/abs/2307.10169)

**[2023.7.29]**
- Paper: [A Watermark for Large Language Models](https://openreview.net/forum?id=aX8ig9X2a7 )

**[2023.7.28]**
- Paper:[Med-Flamingo: a Multimodal Medical Few-shot Learner](https://arxiv.org/pdf/2307.15189v1.pdf)

**[2023.7.27]**
- Paper: [using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)

**[2023.7.26]**
- Paper:[3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)

**[2023.7.25]**
- Paper:[ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning](https://arxiv.org/abs/2307.09474)

**[2023.7.24]**
- Paper:[Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)

**[2023.7.23]**
- [ChatGPT based on Android, pre-register location](https://play.google.com/store/apps/details?id=com.openai.chatgpt)

- Paper:[OBJECT 3DIT: Language-guided 3D-aware Image Editing](https://arxiv.org/abs/2307.11073)
    
**[2023.7.22]**
- Paper:[Brain2Music: Reconstructing Music from Human Brain Activity](https://arxiv.org/abs/2307.11078)

**[2023.7.21]**
- Paper:[A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)
    
**[2023.7.20]**
- New Architecture: RetNetwork, beyond Transformer üëâ[Paper](https://arxiv.org/abs/2307.08621 )üëà

**[2023.7.19]**
- [Meta releases LLAMA 2, open source and commercially available.](https://ai.meta.com/llama/)
    
**[2023.7.18]**
- Paper:[Learning to Retrieve In-Context Examples for Large Language Models](https://arxiv.org/abs/2307.07164)
    
**[2023.7.17]**
- Paper:[HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://arxiv.org/abs/2307.06949)
    
**[2023.7.16]**
- Emu model is open source, a versatile expert in 'multimodal to multimodal': [Model](https://github.com/baaivision/Emu) / [Demo](https://emu.ssi.plus/)
    
**[2023.7.15]**
- Paper:[Self-consistency for open-ended generations](https://arxiv.org/abs/2307.06857)
    
**[2023.7.14]**
- Paper:[Patch n‚Äô Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/pdf/2307.06304.pdf)
    
**[2023.7.13]**
- Paper:[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)

**[2023.7.12]**
- Claude2üëâ **\[Paper\]**[Model Card and Evaluations for Claude Models](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf) / **[Website]**(https://claude.ai/)

**[2023.7.11]**
- Paper: [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/voxposer.pdf)
- Paper: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)

**[2023.7.10]**
- Paper:[Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)
    
**[2023.7.9]**
- Paper: [Schema-learning and rebinding as mechanisms of in-context learning and emergence](https://arxiv.org/abs/2307.01201)

**[2023.7.8]**
- Paper:[SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs](https://arxiv.org/abs/2306.17842)

**[2023.7.7]**
- [GPT-4 API general availability](https://openai.com/blog/gpt-4-api-general-availability)
- Paper:[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)
    
**[2023.7.6]**
- Paper:[LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)

**[2023.7.5]**
- [MetaGPT: Multi-Role Meta-Programming Framework](https://github.com/geekan/MetaGPT)
- Paper: [Conformer LLMs -- Convolution Augmented Large Language Models](https://arxiv.org/abs/2307.00461)

**[2023.7.4]**
- Paper:[Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic](http://arxiv.org/abs/2306.15195)
    
**[2023.7.3]**
- [Largest-scale Chinese multitask instruction set, introducing a thousand Chinese datasets.](https://huggingface.co/datasets/BAAI/COIG-PC)
    
**[2023.7.2]**
- Paper: [Towards Measuring the Representation of Subjective Global Opinions in Language Models](https://arxiv.org/abs/2306.16388)
    
**[2023.7.1]**
- Paper:[Masked Vision-language Transformer in Fashion](https://link.springer.com/article/10.1007/s11633-022-1394-4)
    
**[2023.6.30]**
- Paper: [Inferring the Goals of Communicating Agents from Actions and Instructions](https://arxiv.org/abs/2306.16207)

**[2023.6.29]**
- Paper: [AudioPaLM: A Large Language Model That Can Speak and Listen](https://arxiv.org/pdf/2306.12925.pdf)

**[2023.6.28]**
- Paper:[Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs//2306.14824)

**[2023.6.27]**
- Paper: [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549v1.pdf)
    
**[2023.6.26]**
- Paper: [PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance](https://arxiv.org/abs/2306.05443)

**[2023.6.25]**
- Paper: [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://arxiv.org/abs/2306.02561)

**[2023.6.24]**
- Paper: [PromptIR: Prompting for All-in-One Blind Image Restoration](https://www.researchgate.net/publication/371786106_PromptIR_Prompting_for_All-in-One_Blind_Image_Restoration)

**[2023.6.23]**
- Paper: [OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue](https://arxiv.org/format/2306.12174)
    
**[2023.6.22]**
- [Stanford has released an automatic evaluation system for LLM called AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)
- [Ocean-1: the world's first contact center foundation model.](https://cresta.com/blog/introducing-ocean-1-worlds-first-contact-center-foundation-model/)

**[2023.6.21]**
- [GPT-Engineering, Who generates an entire codebase based on a prompt.](https://github.com/AntonOsika/gpt-engineer)

**[2023.6.20]**
- Paper: [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://scontent-fra3-1.xx.fbcdn.net/v/t39.8562-6/354636794_599417672291955_3799385851435258804_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=PW3or0UVoKoAX-2_D0q&_nc_ht=scontent-fra3-1.xx&oh=00_AfB2SW8Rp55YKG0AEJcpC9tUECbXdl_m83yk9cxX7jie1A&oe=64967631)
    
**[2023.6.19]**
- Technical Report: [AIGC industry overview article from SEALAND SECURITIES](https://pdf.dfcfw.com/pdf/H3_AP202303201584404280_1.pdf?1679327615000.pdf)

**[2023.6.18]**
- Paper: [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory](https://arxiv.org/abs/2306.03901)
    
**[2023.6.17]**
- Paper: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://www.researchgate.net/publication/371540959_Macaw-LLM_Multi-Modal_Language_Modeling_with_Image_Audio_Video_and_Text_Integration)

**[2023.6.16]**
- Financial [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) model open source, benchmarked against BloombergGPT, training parameters can be reduced from 6.17 billion to 3.67 million, can predict stock prices. ([Paper](https://arxiv.org/abs/2306.06031)/[Code](https://github.com/AI4Finance-Foundation/FinGPT))

**[2023.6.15]**
- Paper:[MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://arxiv.org/abs/2306.01311 )
    
**[2023.6.14]**
- Paper:[XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models](https://arxiv.org/abs/2306.07971)
    
**[2023.6.13]**
- Paper:[Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
 
**[2023.6.12]**
- Paper: [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425)
    
**[2023.6.11]**
- Paper: [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/abs/2306.05179)

**[2023.6.10]**
- [Aquila, language model series, including the Aquila Basic Model (7B and 33B), AquilaChat dialogue model, and AquilaCode text-to-code generation model.](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)
    
**[2023.6.9]**
- Paper: [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)

**[2023.6.8]**
- Paper: [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/pdf/2305.05176.pdf)
    
**[2023.6.7]**
- Paper: [XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech](https://arxiv.org/abs/2305.19709)
    
**[2023.6.6]**
- Paper: [XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters](https://arxiv.org/abs/2305.12002)
- Paper: [UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild](https://arxiv.org/abs/2305.11147)

**[2023.6.5]**
- Paper: [Controllable Text-to-Image Generation with GPT-4](https://arxiv.org/abs/2305.18583)

**[2023.6.4]**
- PandaGPT: One model unifies six modalities([Page](https://panda-gpt.github.io/)/[Paper](https://arxiv.org/abs/2305.16355))

**[2023.6.3]**
- Paper: [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)

**[2023.6.2]**
- Paper: [SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/abs/2305.17390)

**[2023.6.1]**
- Paper: [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216)

**[2023.5.31]**
- [Intel announces the Aurora genAI, which is generative AI model with 1 trillion parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/)
    
- HuotuoGPT, towards Taming Language Model to Be a Doctor([Github](https://github.com/FreedomIntelligence/HuatuoGPT)/[Demo](https://www.huatuogpt.cn)/[Paper](https://arxiv.org/pdf/2305.15075.pdf))
- Paper: [Large Language Models Meet NL2Code: A Survey](https://arxiv.org/abs/2212.09420)

**[2023.5.30]** 
- Paper: [Large Language Models as Tool Makers](https://arxiv.org/pdf/2305.17126.pdf)
    
- Paper: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100)

- Paper: [OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities](https://arxiv.org/pdf/2305.16334.pdf)

**[2023.5.29]**

- [Falcon: based on the 1 trillion token open-source large model, surpassing 65 billion LLaMA, commercially available.](https://huggingface.co/tiiuae)

- [ToolBench: the open platform for large language models used for training, service, and evaluation of tool learning](https://github.com/Navezjt/ToolBench)
- Paper: [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/abs/2305.15021)

**[2023.5.28]**
- Paper: [ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation](https://arxiv.org/abs/2305.16213)
- Paper:[RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text](https://arxiv.org/abs/2305.13304)

**[2023.5.27]**
- Paper: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)

- Paper: [ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs](https://arxiv.org/pdf/2305.15964v1.pdf)

- Paper: [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720)

**[2023.5.26]**
- Paper: [Iterative Forward Tuning Boosts In-context Learning in Language Models](https://arxiv.org/pdf/2305.13016.pdf)
    
**[2023.5.25]**
- Paper: [Diversity-Aware Meta Visual Prompting](https://arxiv.org/abs/2303.08138)
    
**[2023.5.24]**
- Paper:[VideoLLM: Modeling Video Sequence with Large Language Models](https://arxiv.org/pdf/2305.13292.pdf)

**[2023.5.23]**
- Paper:[Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)
- Paper: [PointGPT: Auto-regressively GenerativePre-training from Point Clouds](https://arxiv.org/pdf/2305.11487.pdf)

**[2023.5.22]**
- Paper: [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://arxiv.org/abs/2305.10266)
    
**[2023.5.21]**
- Paper:[Language Models Meet World Models: Embodied Experiences Enhance Language Models](https://arxiv.org/abs/2305.10626)

**[2023.5.20]**
- Paper:[AttentionViz: A Global View of Transformer Attention](https://arxiv.org/pdf/2305.03210.pdf)
    
**[2023.5.19]**
- [OpenAI introducing the ChatGPT APP for IOS](https://apps.apple.com/app/openai-chatgpt/id6448311069)
- Paper: [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)
    
**[2023.5.18]**
- Paper: [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)
    
**[2023.5.17]**
- Paper: [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)
    
**[2023.5.16]**
- Paper:[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185.pdf)

**[2023.5.15]** 
- Paper: [ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4](https://arxiv.org/pdf/2305.07490.pdf)

**[2023.5.14]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301v1.pdf)

**[2023.5.13]**
- Paper: [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)

**[2023.5.12]**
- Paper: [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)
    
**[2023.5.11]**
- Google has released PaLM 2, which improves multiple abilities and offers four versions for selection. ([Paper](https://event-cdn.baai.ac.cn/file/file-browser/KDtjMkep6E6n5XjRNJjknjewCF7Pcebx.pdf)/[Page](https://makersuite.google.com/waitlist ))
- [The open-source healthcare large language model NHS-LLM and OpenGPT.](https://github.com/CogStack/opengpt)

- Paper: [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)

**[2023.5.10]** 
- DetGPT: Detect What You Need via Reasoning ([Code](https://github.com/OptimalScale/DetGPT)/[Demo](https://detgpt.github.io/))

- Meta releases a large-scale model called ImageBind that can traverse six senses, and it is now open-source. ([Paper](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)/[Code](https://github.com/facebookresearch/ImageBind))

- [HuoTuo: Open Source Chinese Medical Large Model of Harbin Institute of Technology](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)

- Paper: [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/pdf/2305.04160.pdf)

**[2023.5.9]**

- Paper: [Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278) „Äê[Code](https://github.com/VPGTrans/VPGTrans)„Äë

**[2023.5.8]**
- PandaLM: the first large model for automated evaluation.([Code](https://github.com/WeOpenML/PandaLM))
    
- Paper:[AutoML-GPT: Automatic Machine Learning with GPT](https://arxiv.org/abs/2305.02499)

**[2023.5.7]**
- Paper: [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf)
    
**[2023.5.6]**
- [IFLYTEK officially releases the LLM named SparkDesk](https://xinghuo.xfyun.cn/)

- OpenAI release the language-to-3D model: Shape.E ([Paper](https://arxiv.org/pdf/2305.02463.pdf)/[Project Page](https://github.com/openai/shap-e))

- Paper: [Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks](https://arxiv.org/abs/2304.14732)
    
**[2023.5.5]** 
- Paper: [Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner](https://arxiv.org/pdf/2305.01711.pdf)

**[2023.5.4]**
- Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301.pdf)

**[2023.5.3]**
- Paper:[Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)


**[2023.5.2]** 
- Customize your own LLMs, stop prompt-tunning: [Lamini](https://lamini.ai/)

- Paper: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/pdf/2305.01625.pdf)

**[2023.5.1]**
- Paper: [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/pdf/2304.14178.pdf)

**[2023.4.30]** 
- Paper: [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712)

**[2023.4.29]**
- Paper: [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/pdf/2304.11477.pdf)

**[2023.4.28]**

- Paper: [Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System](https://arxiv.org/pdf/2304.13343.pdf)

**[2023.4.27]** 
- AudioGPT: [[Project Page](https://github.com/AIGC-Audio/AudioGPT)/[Paper](https://arxiv.org/pdf/2304.12995.pdf)]

- Paper:[Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/pdf/2304.13007.pdf)

**[2023.4.26]** 
- Paper: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2304.12244.pdf)

**[2023.4.25]** 
- [Google releases the Security AI workbench](https://cloud.google.com/blog/products/identity-security/rsa-google-cloud-security-ai-workbench-generative-ai)

- Paper: [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models](https://arxiv.org/pdf/2304.11657.pdf)

**[2023.4.24]** 
- Paper: [Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback](https://arxiv.org/pdf/2304.10750.pdf)

**[2023.4.23]** 
- Paper: [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/pdf/2304.11158.pdf)

**[2023.4.22]** Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[Paper](https://arxiv.org/abs/2304.09842)/[Project](https://chameleon-llm.github.io)]

**[2023.4.21]**
- Paper:

    [Progressive-Hint Prompting Improves Reasoning in Large Language Models](https://arxiv.org/pdf/2304.09797.pdf)

    [Pretrained Language Models as Visual Planners for Human Assistance](https://arxiv.org/pdf/2304.09179.pdf)

**[2023.4.20]**

- [Google DeepMind: Bringing together two world-class AI teams](https://blog.google/technology/ai/april-ai-update/)

- Paper: [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)

**[2023.4.19]** 
    
- Paper: [Towards Robust Prompts on Vision-Language Models](https://arxiv.org/pdf/2304.08479.pdf)

**[2023.4.18]**
-   [HuaTuo:Tunning LLaMA Model with chinese medical instructions](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)
-   MiniGPT-4 [[Project Page](https://minigpt-4.github.io/)/[Paper](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf)]

- Paper: 
        [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text](https://arxiv.org/abs/2304.06939 )

**[2023.4.17]** 
- The open source democratizes large language models,**OpenAssistant**, supports 35 languages, and can use RLHF data for free[[Project Page](https://open-assistant.io/chat)/[Code](ttps://github.com/LAION-AI/Open-Assistant)/[Paper](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view )]

- Paper: [Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition](https://arxiv.org/pdf/2304.04704.pdf)

**[2023.4.16]** 
- Paper: [AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models](https://arxiv.org/pdf/2304.06364.pdf)

**[2023.4.15]** 
- Paper: [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society](https://arxiv.org/abs/2303.17760)

- [Visual Med-Alpaca: Bridging Modalities in Biomedical Language Models](https://cambridgeltl.github.io/visual-med-alpaca/)

**[2023.4.14]**

- [Amazon announcing new tools for building with Generative AI](https://aws.amazon.com/cn/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/)

- Paper: [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://arxiv.org/pdf/2304.05613.pdf)

**[2023.4.13]** Three Amazing Works:

- AutoGPT: An Autonomous GPT-4 Experiment üëâ[Code](https://github.com/torantulino/auto-gpt)üëà

- [Databricks releases Dolly 2.0, the first open, instruction-following LLM for commercial use](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)

- [Microsoft released the DeepSpeed Chat: Own your ChatGPT](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)

**[2023.4.12]** [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/pdf/2304.04370.pdf)

**[2023.4.11]** [Why think step-by-step? Reasoning emerges from the locality of experience](https://arxiv.org/pdf/2304.03843.pdf)

**[2023.4.10]** [TagGPT: Large Language Models are Zero-shot Multimodal Taggers](https://arxiv.org/pdf/2304.03022.pdf)

**[2023.4.9]** A new AI model from Meta AI: Segment Anything Model (SAM) ([Paper](https://arxiv.org/pdf/2304.02643.pdf)/[Code](https://github.com/facebookresearch/segment-anything))

**[2023.4.8]** EleutherAI&Yale et al. proposed a large-scale language model analysis suite that spans training and extension: Pythia ([Paper](https://arxiv.org/pdf/2304.01373.pdf)/[Code](https://github.com/EleutherAI/pythia))

**[2023.4.7]** [Stanford releases the 7 billion parameter open-source model Vicuna-7B, which is compact, efficient, but powerful in functionality](https://vicuna.lmsys.org/)

**[2023.4.6]** [Effective Theory of Transformers at Initialization](https://arxiv.org/pdf/2304.02034.pdf)

**[2023.4.5]** [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf)

**[2023.4.4]** [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?](https://arxiv.org/pdf/2303.18240.pdf)

**[2023.4.3]** [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/pdf/2303.17651.pdf)

**[2023.4.1]** [A survey of Large Language Models](https://arxiv.org/abs/2303.18223)

**[2023.3.31]** [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)

**[2023.3.30]** [GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/pdf/2303.16634.pdf)

**[2023.3.29]** [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf.)

**[2023.3.28]** [ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks](https://arxiv.org/pdf/2303.15056.pdf)

**[2023.3.27]** [Scaling Expert Language Models with Unsupervised Domain Discovery](https://arxiv.org/pdf/2303.14177.pdf)
 
**[2023.3.26]** [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)

**[2023.3.23]** [OpenAI announces 'Plug-ins' for ChatGPT that enable it to perform actions beyond text.](https://platform.openai.com/docs/plugins/introduction)

**[2023.3.22]** [GitHub launches Copilot X, aiming at the future of AI-powered software development.](https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/) 

**[2023.3.21]** [Google Bard is now available in the US and UK, w/ more countries to come.](https://bard.google.com) 

**[2023.3.20]** OpenAI‚Äôs new paper looks at the economical impact of LLMs+Labor Market.[GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) 

**[2023.3.17]** [Microsoft 365 Copilot released. Word, Excel, PowerPoint, Outlook powered by LLMs.](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)

**[2023.3.16]**  Baidu announcing the LLM named ["ÊñáÂøÉ‰∏ÄË®Ä"(ERNIE3.0 + PLATO)](https://yiyan.baidu.com/welcome) 

**[2023.3.15]** Two Breaking News:
    -  Announcing [GPT4](https://openai.com/product/gpt-4) by OpenAI from Microsoft. **[Paperüîó](https://cdn.openai.com/papers/gpt-4.pdf)**
    -  Announcing [PaLM](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html) API by Google. 

**[2023.3.13]** [LLaMA has been fine-tuned by Stanford](https://github.com/tatsu-lab/stanford_alpaca)

**[2023.3.10]** [Announcing OpenChatKit by Together](https://huggingface.co/spaces/togethercomputer/OpenChatKit)

**[2023.3.9]**  GPT-4 is coming next week and it will be multimodal,announced by OpenAI.

**[2023.3.8]** [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)

**[2023.3.7]** [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)

**[2023.3.6]** [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)