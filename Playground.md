# ðŸ”¥ Playground
> **Prompt Engineering aims to carefully curate input prompts that can extract the best possible results from Large language models(LLMs).** 

### ðŸŒ€ <font size=6>A</font>s a prominent example of LLMs, ChatGPT has received widespread attention and skyrocketed in popularity. Nonetheless, in recent years, a significant number of LLMs have emerged, typically several tens of gigabytes in size and trained on massive amounts of textual data. Therefore, there are several alternatives available that we can use to practice prompt techniques using these models.

<img width="200%" src="./figures/hr.gif" />

## ðŸ“œ Table of Contents
- [TrustGPT From EgoAlpha](#trustgpt)
- [Directly Usage of LLMs](#directly-usage-of-llms)
- [Providing the Pre-train Weights](#providing-the-pre-train-weights)
- [Without Opensource Till Now](#without-opensource-till-now)
- [LLMs in Coding](#llms-in-coding)
- [Dataset in LLMs area](#dataset-in-llms-area)
- [LLMs from China](#llms-from-china)

<img width="200%" src="./figures/hr.gif" />

<div align="center">
<img src="./figures/TrustGPT.png" width="600px">
</div>

## TrustGPT
ðŸŒŸ [TrustGPT](https://trustgpt.co/trust/index.html) can also serve as a playground for everyone's convenience to learn and practice advanced prompt techniques. You can also commit your issues from TrustGPT to this repo page. Thanks a lot.

We will gradually release the following features: 
1. Prompt example 
2. Question answering over your own document
3. Autonomous agent 
4. Access to various LLMs 

As resources are limited, we suggest using this playground for learning and practicing prompt techniques rather than for work. This will help more people access prompt engineering.

<img width="200%" src="./figures/hr.gif" />

## Directly Usage of LLMs
### ðŸ¤© *These models in the table below are directly accessible via links, The page contains the usage guide and API interface of the model for the convenience of all developers and researchers to explore and experience. The Checkpoints can also obtained by corresponding links.*

|   Model  | Type | Lab | Playgrounds| Params(B) | Blog/Paper/Github | Checkpoints |Announced Time|
| :----: | :----: | :----:  | :----: | :----:|:----: |:----:|:----:|
| Gemma | Decoder| Google | [ðŸ”—](https://www.kaggle.com/models/google/gemma) | 2,7 |[Github]()|[Gemma-2B](https://huggingface.co/google/gemma-2b)/[Gemma-7B](https://huggingface.co/google/gemma-7b)|Feb-24|
| Yi series | Decoder| 01.Ai | [ðŸ”—](https://huggingface.co/01-ai/Yi-34B) | 6,34 |[Github](https://github.com/01-ai/Yi)|[Yi-34B](https://huggingface.co/01-ai/Yi-34B)/[Yi-6B](https://huggingface.co/01-ai/Yi-6B)|Nov-23|
| InternLM | Decoder| Shanghai Artificial Intelligence Laboratory | [ðŸ”—](https://huggingface.co/internlm/internlm-20b) | 20 |[Github](https://github.com/InternLM/InternLM)|[InternLM-20B](https://huggingface.co/internlm/internlm-chat-20b)|Aug-23|
| Mistral 7B | Decoder| | [ðŸ”—](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 7 |[Paper](https://arxiv.org/pdf/2310.06825.pdf)/[Blog](https://mistral.ai/news/announcing-mistral-7b/)|[Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)|Oct-23|
| Llama-2 | Decoder| Meta | [ðŸ”—](https://ai.meta.com/llama/#inside-the-model) | 7,13,70 |[Github](https://github.com/facebookresearch/llama)/[Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/ )/[Blog](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)|[Llama-7B, Llama-13B, Llama-70B](https://ai.meta.com/llama/#inside-the-model)|Jul-23|
| TigerBot | Decoder| - | [ðŸ”—](https://huggingface.co/TigerResearch/tigerbot-70b-base) | 70 |[Github](https://github.com/TigerResearch/TigerBot)|[TigerBot-70B](https://huggingface.co/TigerResearch/tigerbot-70b-base)|Jun-23|
| Falcon | Decoder| TII | [ðŸ”—](https://huggingface.co/tiiuae) | 1,7,40 |[Blog](https://falconllm.tii.ae/)|[Falcon-40B-instruct](https://huggingface.co/tiiuae/falcon-40b), [Falcon-7B-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct),[Falcon-RW-1B](https://huggingface.co/tiiuae/falcon-rw-1b),[Falcon-RW-7B](https://huggingface.co/tiiuae/falcon-rw-7b)|May-23|
| GPT-J-6B | Decoder| EleutherAI | [ðŸ”—](https://github.com/EleutherAI/pythia) | 6 |[Blog](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)|[GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b), [GPT4All-J](https://github.com/nomic-ai/gpt4all#raw-model)|May-23|
| DLite | Decoder| EleutherAI | [ðŸ”—](https://github.com/EleutherAI/pythia) | 0.124-1.5 |[Blog](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e)|[dlite-v2-1_5b](https://huggingface.co/aisquared/dlite-v2-1_5b)|May-23|
| OpenLLaMA | Decoder| H2O.AI | [ðŸ”—](https://huggingface.co/h2oai) | 3,7 |[Github](https://github.com/openlm-research/open_llama)|[OpenLLaMA-7b-preview-300bt](https://huggingface.co/openlm-research/open_llama_7b_preview_300bt)|May-23|
| RedPajama-INCITE | Decoder| Together | [ðŸ”—](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1/blob/157bf3174feebb67f37e131ea68f84dee007c687/config.json#L13) | 3-7 |[Blog](https://www.together.xyz/blog/redpajama-models-v1)|[RedPajama-INCITE](https://huggingface.co/togethercomputer)|May-23|
| MPT-7B  | Decoder| mosaic | [ðŸ”—](https://huggingface.co/mosaicml/mpt-7b#how-is-this-model-different) | 7 |[Blog](https://www.mosaicml.com/blog/mpt-7b)|[MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct)|May-23|
| h2oGPT | Decoder| EleutherAI | [ðŸ”—](https://huggingface.co/h2oai) | 12-20 |[Blog](https://h2o.ai/blog/building-the-worlds-best-open-source-large-language-model-h2o-ais-journey/)|[h2oGPT](https://github.com/h2oai/h2ogpt)|May-23|
| Dolly | Decoder| EleutherAI | [ðŸ”—](https://github.com/EleutherAI/pythia) | 3,7,12 |[Blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)/[Github](https://github.com/databrickslabs/dolly#dolly)|[dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)|Apr-23|
| Pythia | Decoder| EleutherAI | [ðŸ”—](https://github.com/EleutherAI/pythia) | 0.07-12 |[Paper](https://arxiv.org/abs/2304.01373)/[Github](https://github.com/EleutherAI/pythia)|[pythia 70M - 12B](https://github.com/EleutherAI/pythia)|Apr-23|
| FastChat-T5 | Decoder| EleutherAI | [ðŸ”—](https://github.com/EleutherAI/pythia) | 3 |[Blog](https://twitter.com/lmsysorg/status/1652037026705985537?s=20)|[fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)|Apr-23|
| StableLM-Alpha | Decoder| EleutherAI | [ðŸ”—](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)| 3-65 |[Github](https://github.com/Stability-AI/StableLM#stablelm-alpha)|[StableLM-Alpha](https://github.com/Stability-AI/StableLM#stablelm-alpha)|Apr-23|
| oasst-sft-6-llama-30b | Decoder| HuggingFace | [ðŸ”—](https://huggingface.co/chat/) | 30 |[Github](https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor)|-|Apr-23|
| Cerebras-GPT | Decoder| HuggingFace | [ðŸ”—](https://huggingface.co/cerebras/) | 0.111-13 |[Paper](https://arxiv.org/abs/2304.03208)|[Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)|Mar-23|
| OpenAssistant(Pythia family) | Decoder| LAION AI  | [ðŸ”—](https://open-assistant.io/chat) | 12 |[Paper](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view )/[Github](https://github.com/LAION-AI/Open-Assistant)|[OA-Pythia-12B-SFT-8](https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps), [OA-Pythia-12B-SFT-4](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5), [OA-Pythia-12B-SFT-1](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b)|Apr-23|
| GPT-4 | Decoder| OpenAI | [ðŸ”—](https://openai.com/waitlist/gpt-4-api) | 20 |[Paper](https://cdn.openai.com/papers/gpt-4.pdf)|-|Mar-23|
| OpenChatKit | Decoder| Together | [ðŸ”—](https://huggingface.co/spaces/togethercomputer/OpenChatKit) | 20 |[Github](https://github.com/togethercomputer/OpenChatKit)|-|Mar-23|
| Alpaca | Decoder| Stanford | [ðŸ”—](https://alpaca-ai-custom1.ngrok.io/) | 7 |[Github](https://github.com/tatsu-lab/stanford_alpaca)|-|Mar-23|
| ChatGPT | Decoder| OpenAI | [ðŸ”—](https://platform.openai.com/playground) | 175 |[Paper](https://arxiv.org/abs/2005.14165)|-|Nov-22|
| GPT-JT | Decoder | Together | [ðŸ”—](https://huggingface.co/spaces/togethercomputer/GPT-JT) | 6 |[Github](https://huggingface.co/togethercomputer/GPT-JT-6B-v1)|-|Nov-22|
| Flan-T5 | Encoder-Decoder | Google Research | [ðŸ”—](https://huggingface.co/google/flan-t5-xxl?text=Please+answer+the+following+question+What+is+the+boiling+point+of+Nitrogen%3F) | 11|[Paper](https://arxiv.org/abs/2210.11416)/[Github](https://huggingface.co/google/flan-t5-xxl/tree/main)|[Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)|Oct-22|
| Flan-UL2 | Encoder-Decoder | Google Research | [ðŸ”—](https://huggingface.co/google/flan-ul2#citation) | 20|[Paper](https://arxiv.org/pdf/2205.05131v1.pdf)/[Github](https://github.com/google-research/google-research/tree/master/ul2)|[Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints)|Oct-22|
| CodeGeeX | Decoder | Tsinghua | [ðŸ”—](https://huggingface.co/spaces/THUDM/CodeGeeX) | 13 | [Github](https://github.com/THUDM/CodeGeeX)|[CodeGeeX register path](https://models.aminer.cn/codegeex/download/request)|Sep-22|
| GLM-130B | Encoder-Decoder | Tsinghua & Zhipu | [ðŸ”—](https://huggingface.co/spaces/THUDM/GLM-130B) | 130 |[Paper](https://arxiv.org/abs/2210.02414)/[Github](https://huggingface.co/spaces/THUDM/GLM-130B)|-|Aug-22|
| BLOOM(tr11-176B-ml) | Decoder | BigScience | [ðŸ”—](https://huggingface.co/spaces/huggingface/bloom_demo) | 176|[Github](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml)|[BLOOM](https://huggingface.co/bigscience/bloom)|Jul-22|
| PaLM | Decoder | Google Research | [ðŸ”—](https://cloud.google.com/vertex-ai) | 540|[Paper](https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html)|-|Apr-22|
| GPT-NeoX-20B | Decoder | EleutherAI | [ðŸ”—](https://huggingface.co/EleutherAI/gpt-neox-20b) | 20|[Paper](https://arxiv.org/abs/2304.04165)|[GPT-NEOX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)|Apr-22|
| CodeT5 | Encoder-Decoder | Salesforce Research Asia | [ðŸ”—](https://huggingface.co/Salesforce/codet5-base) | small:0.06,base:0.22|[Paper](https://arxiv.org/abs/2109.00859)|-|Mar-22|
| ERNIE3.0 | Encoder-Decoder | Baidu | [ðŸ”—](https://huggingface.co/swtx/ernie-3.0-base-chinese) | 10|[Paper](https://arxiv.org/abs/2112.12731)|-|Dec-21|
| CodeX | Decoder | OpenAI | [ðŸ”—](https://platform.openai.com/playground) | 12|[Paper](https://arxiv.org/abs/2107.03374)|-|Aug-21|
| RWKV | Decoder | OpenAI | [ðŸ”—](https://platform.openai.com/playground) | 0.1-14|[Github](https://github.com/BlinkDL/RWKV-LM)|[RWKV, ChatRWKV](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v)|Aug-21|
| GPT-3 | Decoder | OpenAI | [ðŸ”—](https://huggingface.co/models?sort=downloads&search=gpt3) | 175|[Paper](https://arxiv.org/abs/2005.14165)|-|May-20|
| T5 | Encoder-Decoder | Google | [ðŸ”—](https://huggingface.co/t5-base) | 11|[Paper](https://arxiv.org/abs/1910.10683)|[T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)|Oct-19|
| RoBERTa | Encoder | MetaAI | [ðŸ”—](https://huggingface.co/xlm-roberta-base) | 0.355|[Paper](https://arxiv.org/abs/1907.11692)|[roberta-series](https://huggingface.co/models?sort=downloads&search=RoBERTa)|Jul-19|
| GPT-2 | Decoder| OpenAI | [ðŸ”—](https://huggingface.co/openai-gpt) | 1.5|[Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|[GPT_2 Series](https://huggingface.co/models?sort=downloads&search=GPT2)|Feb-19|
| BERT | Encoder | Google | [ðŸ”—](https://huggingface.co/bert-base-uncased) | 0.3|[Paper](https://arxiv.org/abs/1810.04805)|[BERT Series](https://huggingface.co/models?sort=downloads&search=BERT)|Oct-18|
| GPT-1 | Decoder| OpenAI | [ðŸ”—](https://huggingface.co/models) | 0.117|[Paper](https://gwern.net/doc/www/s3-us-west-2.amazonaws.com/d73fdc5ffa8627bce44dcda2fc012da638ffb158.pdf)|[GPT_1_seriers](https://huggingface.co/models?sort=downloads&search=GPT1)|Jun-18|

<img width="200%" src="./figures/hr.gif" />

## Providing the Pre-train weights
### ðŸ¤¨ *The models in the table below all provide pre-trained weights on which developers can fine-tune (without changing the original backbone architecture), and people can visually see the work of a good team of researchers by using the pre-trained weights of the models directly for a good Demo.*

| Model  | Type | Lab | Github| Params(B) |Paper/Code|Announced Time|
| :----: | :----: | :----:  | :----: | :----: |:----:|:----:|
| Gorilla-OpenFunctions series | Decoder| Gorilla LLM | [ðŸ”—](https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2) | - |[Paper](https://arxiv.org/abs/2305.15334)/[Github](https://gorilla.cs.berkeley.edu/leaderboard)|-|May-23|
| LLaMA-65B| Decoder | MetaAI | [ðŸ”—](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) | 65|[Paper](https://scontent-hkg4-1.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=4srK2r5szdYAX8tuGSV&_nc_ht=scontent-hkg4-1.xx&oh=00_AfAUdcLc_-aVJHTm60I_1mIOLIEcecJ1N9s8-G4drVrd3Q&oe=6409B2E2)/[Code](https://github.com/facebookresearch/llama)|Feb-23|
| OPT-IML | Decoder| MetaAI | [ðŸ”—](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML) | 175|[Paper](https://arxiv.org/pdf/2212.12017.pdf)/-|Dec-22|
| ERNIE-Code | Encoder-Decoder | Baidu | [ðŸ”—](https://github.com/thunlp/ERNIE) | 0.56|[Paper](https://arxiv.org/abs/2212.06742#baidu)/-|Dec-22|
| Galactica | Decoder| MetaAI | [ðŸ”—](https://galactica.org/) | 120|[Paper](https://galactica.org/static/paper.pdf)/-|Nov-22|
| mT0 | Encoder-Decoder | BigScience | [ðŸ”—](https://github.com/bigscience-workshop/xmtf) | 13|[Paper](https://arxiv.org/abs/2211.01786)/-|Nov-22|
| BLOOMZ | Decoder | BigScience | [ðŸ”—](https://github.com/bigscience-workshop/xmtf)| 176|[Paper](https://arxiv.org/abs/2211.01786)/-|Nov-22|
| Atlas | Encoder-Decoder | MetaAI | [ðŸ”—](https://github.com/facebookresearch/atlas) | 11|[Paper](https://arxiv.org/abs/2208.03299)/-|Aug-22|
| OPT-175B | Decoder  | MetaAI | [ðŸ”—](https://huggingface.co/docs/transformers/model_doc/opt)| 175|[Paper](https://arxiv.org/abs/2205.01068)/-|May-22|
| RETRO | Encoder-Decoder | DeepMind | [ðŸ”—](https://github.com/lucidrains/RETRO-pytorch) | 7.5|[Paper](https://arxiv.org/abs/2112.04426)/-|Dec-21|
| FLAN | Encoder-Decoder | Google | [ðŸ”—](https://github.com/google-research/FLAN) | 137|[Paper](https://arxiv.org/abs/2109.01652)/-|Sep-21|

<img width="200%" src="./figures/hr.gif" />

## Without Opensource Till Now
### ðŸ˜£ *The following table show that the related models and codes are not open-source till now.*

| Model  | Type | Lab | Report| Params(B) |Paper/Code|Announced Time|
| :----: | :----: | :----:| :----: | :----: |:----:|:----:|
| Med-PaLM | Encoder | Google & DeepMind | [ðŸ”—](https://gpt3demo.com/apps/med-palm) | 540|[Paper](https://arxiv.org/abs/2212.13138)/-|Dec-22|
| GLaM | Encoder | Google Inc | [ðŸ”—](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) | 1200|[Paper](https://arxiv.org/abs/2112.06905)/-|Dec-22|
| RL-CAI | Encoder| Anthropic | [ðŸ”—](https://lifearchitect.ai/anthropic/) | 52|[Paper](https://arxiv.org/abs/2212.08073)/-|Dec-22|
| Sparrow | Decoder | DeepMind | [ðŸ”—](https://www.deepmind.com/blog/building-safer-dialogue-agents) | 70|[Paper](https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf)/-|Sep-22|
| PaLI | Encoder-Decoder | Google | [ðŸ”—](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html) | 17|[Paper](https://arxiv.org/abs/2209.06794)/-|Sep-22|
| Gato(Cat) | Encoder-Decoder | DeepMind | [ðŸ”—](https://www.deepmind.com/blog/a-generalist-agent) | 1|[Paper](https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf)/-|May-22|
| Chinchilla | Encoder | DeepMind | [ðŸ”—](https://deepmind.github.io/dramatron/details.html) | 70|[Paper](https://arxiv.org/abs/2203.15556)/-|Mar-22|
| Gopher | Encoder  | DeepMind | [ðŸ”—](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval) | 280|[Paper](https://arxiv.org/abs/2112.11446)/-|Dec-21|
| LaMDA | Decoder | GoogleAI |[ðŸ”—](https://www.youtube.com/watch?v=aUSSfo5nCdM)| 137|[Paper](https://arxiv.org/abs/2201.08239)/-|Jun-21|

<img width="200%" src="./figures/hr.gif" />

## LLMs in Coding
### ðŸŽ­ *The following table shows the LLMs for Coding.* 

| Model |  Checkpoints | Paper/Blog | Params (B)|Announced Time |
| :---: | :---: | :---: | :---: | :---: |
| StarCoder |  [starcoder](https://huggingface.co/bigcode/starcoder) | [Blog](https://huggingface.co/blog/starcoder) | 15 | May-23 |
| StarChat Alpha |  [starchat-alpha](https://huggingface.co/HuggingFaceH4/starchat-alpha) | [Blog](https://huggingface.co/blog/starchat-alpha) | 16 | May-23 |
| Replit Code |  [replit-code-v1-3b](https://huggingface.co/replit/replit-code-v1-3b) | [Blog](https://www.latent.space/p/reza-shabani#details) | 2.7 | May-23 |
| CodeT5+ |  [CodeT5+](https://github.com/salesforce/CodeT5/tree/main/CodeT5+)     | [Paper](https://arxiv.org/abs/2305.07922) | 0.22 - 16 | May-23 |
| CodeGen2 |  [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [Paper](https://arxiv.org/abs/2305.02309) | 1 - 16 | Apr-23 |
| SantaCoder |  [santacoder](https://huggingface.co/bigcode/santacoder) |[Paper](https://arxiv.org/abs/2301.03988) | 1.1| Jan-23 |

<img width="200%" src="./figures/hr.gif" />

## Dataset in LLMs Area
### ðŸ“ˆ *The following table shows the Dataset of the LLM area, with instruction-tunning and alignment-tuning.*

| Dataset |   Paper/Blog | Dataset | Samples (K) | Announced Time |Type|
| :---: | :---: | :---: | :---: | :---: | :---:|
| MPT-7B-Instruct |  [Blog](https://www.mosaicml.com/blog/mpt-7b) | [dolly_hhrlhf](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) | 59 | May-23 |instruction-tuning|
| databricks-dolly-15k |  [Blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) |  [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) | 15 |  Apr-23 |instruction-tuning|
| OpenAssistant Conversations Dataset |  [Blog](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view) | [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1) | 161 |Apr-23 |alignment-tuning|
| OIG (Open Instruction Generalist)   |  [Blog](https://laion.ai/blog/oig-dataset/) | [OIG](https://huggingface.co/datasets/laion/OIG) | 44,000 | Mar-23 |instruction-tuning|

<img width="200%" src="./figures/hr.gif" />

## LLMs from China
### ðŸ‡¨ðŸ‡³ *The following table shows the LLMs from China, including the research lab, firms, and some universities.*

>Noteï¼š The part of contents of the list are from [here](https://github.com/wgwang/LLMs-In-China), and we have made appropriate modifications and supplements, hereby noted.

|Source|Model & Link|Description|
|:-:|:-:|:-:|
|å¤æ—¦å¤§å­¦|[MOSS](https://github.com/OpenLMLab/MOSS)|[Playground](https://moss.fastnlp.top/)|
|è´å£³|[BELLE](https://github.com/LianjiaTech/BELLE)|åŸºäºŽBLOOMZæˆ–[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ç³»åˆ—çš„å¤šä¸ªæ¨¡åž‹|
|å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦|[æœ¬è‰](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)|åŒ»å­¦ï¼›åŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ï¼›å¦æœ‰åŸºäºŽ ChatGLM çš„[Med-ChatGLM](https://github.com/SCIR-HI/Med-ChatGLM)|
|äº‘çŸ¥å£°|[å±±æµ·](https://shanhai.unisound.com/) |é€šç”¨å¤§æ¨¡åž‹|
|ç™¾åº¦| [æ–‡å¿ƒä¸€è¨€](https://yiyan.baidu.com)|[ç”³è¯·è´¦å·](https://yiyan.baidu.com)|
|ç§‘å¤§è®¯é£ž|[æ˜Ÿç«](https://xinghuo.xfyun.cn)|[ç”³è¯·è´¦å·](https://xinghuo.xfyun.cn/desk) |
|æ¸…åŽå¤§å­¦|[ChatGLM](https://chatglm.cn/),[NowcastNet](https://www.nature.com/articles/s41586-023-06184-4)| [å¼€æº6B](https://github.com/THUDM/ChatGLM-6B)ï¼Œ[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [æ™ºè°±AI](http://open.bigmodel.ai/),[æ°”è±¡,ä¸´è¿‘é¢„æŠ¥å¤§æ¨¡åž‹](https://mp.weixin.qq.com/s/MwJWjCLNqJM3lZ33RwK4Bg)|
|åŽä¸º|[ç›˜å¤](https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha),[ç›˜å¤æ°”è±¡](https://www.nature.com/articles/s41586-023-06185-3),[ç›˜å¤-Î£](https://arxiv.org/pdf/2303.10845.pdf)|åŽä¸º+é¹åŸŽ,[åŽä¸ºäº‘ç›˜å¤](https://www.huaweicloud.com/product/pangu.html)|
|è¾¾è§‚æ•°æ®|[æ›¹æ¤](http://www.datagrand.com/products/aigc/)|[è¯•ç”¨éœ€è´¦å·](https://aigc.datagrand.com/) |
|é˜¿é‡Œäº‘|[é€šä¹‰åƒé—®](https://tongyi.aliyun.com/)|[è¯•ç”¨éœ€è´¦å·]( https://tongyi.aliyun.com)|
|æµ™æ±Ÿå¤§å­¦|[å¯çœŸ](https://github.com/CMKRG/QiZhenGPT),[PromptProtein](https://github.com/HICAI-ZJU/PromptProtein)|åŒ»å­¦å¤§æ¨¡åž‹æä¾›åŸºäºŽLLaMA-7Bã€CaMA-13Bå’ŒChatGLM-6B ä¸‰ä¸ªç‰ˆæœ¬,ç”¨äºŽPromptProteinçš„[æ¨¡åž‹](https://github.com/HICAI-ZJU/OpenProtein)|
|ç™¾å·æ™ºèƒ½|[baichuan-7B](https://github.com/baichuan-inc/baichuan-7B),[Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B)|æ¨¡åž‹ä¸‹è½½ï¼š[Baichuan-13B-Base](https://huggingface.co/baichuan-inc/Baichuan-13B-Base),[Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat),[Baichuan-7B](https://huggingface.co/baichuan-inc/Baichuan-7B),å¼€æºå¯å•†ç”¨|
|ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤|[ä¹¦ç”ŸÂ·æµ¦è¯­](https://internlm.org/), [OpenMEDLabæµ¦åŒ»](https://github.com/openmedlab) |[æŠ€æœ¯æŠ¥å‘Š](https://github.com/InternLM/InternLM-techreport),[å¼€æºçš„InternLM-7B](https://github.com/InternLM/InternLM),[HuggingFaceä¸‹è½½æ¨¡åž‹æƒé‡](https://huggingface.co/internlm/internlm-7b)|
|OpenBMB|[CPM](https://live.openbmb.org/),[CPM-Bee](https://github.com/OpenBMB/CPM-Bee)|[é¢å£æ™ºèƒ½](https://modelbest.cn/),[CPM-Bee-10B](https://huggingface.co/openbmb/cpm-bee-10b)|
|æ¸¯ä¸­æ–‡æ·±åœ³|[åŽä½—](https://github.com/FreedomIntelligence/HuatuoGPT)ï¼Œ[å‡¤å‡°](https://github.com/FreedomIntelligence/LLMZoo)|é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰å’Œæ·±åœ³å¸‚å¤§æ•°æ®ç ”ç©¶é™¢ï¼ŒåŒ»å­¦,[Demo](https://www.huatuogpt.cn/),åŽä½—å’Œå‡¤å‡°éƒ½åŸºäºŽBLOOMZ|
|ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€| [ç´«ä¸œÂ·å¤ªåˆ](https://gitee.com/zidongtaichu/multi-modal-models)|ç´«ä¸œå¤ªåˆ2.0å·ç§°100Bå‚æ•°ï¼Œå…¨æ¨¡æ€|
|è™Žåšç§‘æŠ€|[TigerBot](https://github.com/TigerResearch/TigerBot)|åŸºäºŽ[BLOOM](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ)|
|ä¸œåŒ—å¤§å­¦|[TechGPT](https://github.com/neukg/TechGPT),[PICA](https://github.com/NEU-DataMining/PICA)|TechGPT->BELLE->[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ï¼Œå›¾è°±æž„å»ºå’Œé˜…è¯»ç†è§£é—®ç­”;PICA->ChatGLM2-6Bæƒ…æ„Ÿå¤§æ¨¡åž‹|
|ä¸Šæµ·äº¤é€šå¤§å­¦|[K2](https://github.com/davendw49/k2),[ç™½çŽ‰å…°](https://mp.weixin.qq.com/s/3eON8L4b7-d-1URwgdR6Bg)|[Demo](https://k2.acemap.info/)ï¼ŒGeoLLaMAï¼ŒåŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ï¼Œ[HuggingFace](https://huggingface.co/daven3/k2_it_adapter) |
|IDEAç ”ç©¶é™¢|[å°ç¥žæ¦œMindBot](https://fengshenbang-lm.com/) |[å§œå­ç‰™](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1)ç³»åˆ—æ¨¡åž‹ |
|æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢|[æ‚Ÿé“Â·å¤©é¹°](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila),[æ‚Ÿé“Â·EMU](https://github.com/baaivision/Emu)|æ‚Ÿé“3.0,è§†ç•Œè§†è§‰ï¼ŒAQUILAå¤©é¹°åº§ï¼Œ[Aquila-7B](https://model.baai.ac.cn/model-detail/100098),[AquilaChat-7B](https://model.baai.ac.cn/model-detail/100101),[AquilaCode-7B-NV](https://model.baai.ac.cn/model-detail/100102),[AquilaCode-7B-TS](https://model.baai.ac.cn/model-detail/100099),[HuggingFace](https://huggingface.co/BAAI),[EMU](https://huggingface.co/BAAI/Emu)åŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)|
|åº¦å°æ»¡|[è½©è¾•](https://huggingface.co/xyz-nlp/XuanYuan2.0) |åŸºäºŽ[BLOOM](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ)|
|23|360| [æ™ºè„‘](https://ai.360.cn/),[ä¸€è§](https://github.com/360CVGroup/SEEChat)||
|è‰¾å†™ç§‘æŠ€|[Anima](https://github.com/lyogavin/Anima)|åŸºäºŽGuanaco->åŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ï¼Œä½¿ç”¨QLoRA|
|è¥¿æ¹–å¿ƒè¾°|[è¥¿æ¹–](https://xinchenai.com/)|é€šç”¨å¤§æ¨¡åž‹|
|æ™“å¤šç§‘æŠ€+å›½å®¶è¶…ç®—æˆéƒ½ä¸­å¿ƒ|[æ™“æ¨¡åž‹XPT](https://www.xiaoduoai.com/blog/12433.html)|è¯•ç”¨éœ€è¦è´¦å·ï¼Œ[ä½ç½®](https://www.xiaoduoai.com/x_model)|
|ç¨€å®‡ç§‘æŠ€|[MiniMax](https://api.minimax.chat/)|GLOWè™šæ‹Ÿç¤¾äº¤|
|åŒ—äº¬è¯­è¨€å¤§å­¦ |[æ¡ƒæŽ](https://github.com/blcuicall/taoli) |åŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA),åŒ—è¯­+æ¸…åŽ+ä¸œåŒ—ã€åŒ—äº¬äº¤å¤§|
|å•†æ±¤ç§‘æŠ€ | [SenseNovaæ—¥æ—¥æ–°](https://techday.sensetime.com/list)|å•†æ±¤ç§‘æŠ€ç‰ˆChatGPT |
|å›½å®¶è¶…çº§è®¡ç®—å¤©æ´¥ä¸­å¿ƒ|[å¤©æ²³å¤©å…ƒ](https://mp.weixin.qq.com/s/A9jnnL3-LjcDLsDD2PCa6g)| ç›®å‰å®˜ç½‘æŸ¥è¯¢ä¸åˆ°|
|æ˜ŸçŽ¯ç§‘æŠ€|[æ— æ¶¯ã€æ±‚ç´¢](https://mp.weixin.qq.com/s/6rYmk58OypU_Wwu0L7-nTw)|æ— æ¶¯â€”â€”é‡‘èžï¼›æ±‚ç´¢â€”â€”å¤§æ•°æ®åˆ†æž|
|æ…§è¨€ç§‘æŠ€+å¤©æ´¥å¤§å­¦|[æµ·æ²³Â·è°›å¬](https://mp.weixin.qq.com/s/FCnXXmT0jRfk4tTRIAK9FA)| -|
|æ’ç”Ÿç”µå­|LightGPT|- |
|ç”µä¿¡æ™ºç§‘|[æ˜Ÿæ²³](https://mp.weixin.qq.com/s/ntd0z5CJOY6peou4bOVJqA)|é€šç”¨è§†è§‰ï¼Œä¸­å›½ç”µä¿¡|
|å·¦æ‰‹åŒ»ç”Ÿ|[å·¦åŒ»GPT](https://mp.weixin.qq.com/s/Tv9nIG_9K-Lf5AKatjichA)|åŒ»ç–—ï¼Œ[è¯•ç”¨éœ€Key](https://gpt.zuoshouyisheng.com/)|
|æ™ºæ…§çœ¼|[ç ­çŸ³](https://mp.weixin.qq.com/s/lid0nUBwXEdoUhnw_guteA)|åŒ»ç–—é¢†åŸŸ|
|å¥½æœªæ¥|[MathGPT](https://mp.weixin.qq.com/s/evLrZAFKa9mCplcqZnpZMw)|å­¦è€Œæ€|
|æ•°æ…§æ—¶ç©º|[é•¿åŸŽ](https://mp.weixin.qq.com/s/KYB-noOt7gB0l5hh-rwfkQ)|è‡ªç„¶èµ„æºï¼Œé¥æ„Ÿ|
|ç†æƒ³ç§‘æŠ€|å¤§é“Dao|è¿ç»´å¤§æ¨¡åž‹|
|ç¡…åŸºæ™ºèƒ½|[ç‚Žå¸](https://mp.weixin.qq.com/s/XNu3UrSKm4jy1ayJJ6-HMg)|æ—…æ¸¸è¡Œä¸šå¤§æ¨¡åž‹|
| ä¸­å·¥äº’è”|[æ™ºå·¥](https://mp.weixin.qq.com/s/ANsZeqj4V_NeVCquwX-aSQ)|ä¸Žå¤æ—¦NLPå®žéªŒå®¤è”åˆï¼Œå·¥ä¸šé¢†åŸŸ|
|åˆ›ä¸šé»‘é©¬|[å¤©å¯](https://mp.weixin.qq.com/s/lYqCe9skc0MOSzmmTiGAug) | åˆ›ä¸šé»‘é©¬ä¸Ž360åˆä½œ,ç§‘åˆ›æœåŠ¡è¡Œä¸š|
|è¿½ä¸€ç§‘æŠ€|[åšæ–‡Bowen](https://mp.weixin.qq.com/s/cYVh6K6edmColgMEOaGFKg) | -|
|ä¸Šæµ·ç§‘æŠ€å¤§å­¦|[DoctorGLM](https://github.com/xionghonglin/DoctorGLM)|åŒ»å­¦å¤§æ¨¡åž‹ï¼Œ[è®ºæ–‡](https://arxiv.org/pdf/2304.01097.pdf)|
|åŽä¸œå¸ˆèŒƒå¤§å­¦ |[EmoGPT](https://mp.weixin.qq.com/s/xP-qm5YUj8fZD9YQ7t08NQ),[EduChat](https://github.com/icalk-nlp/EduChat)|EmoGPTæ˜¯ä¸Šæµ·å¸‚å¿ƒç†å¥åº·ä¸Žå±æœºå¹²é¢„é‡ç‚¹å®žéªŒå®¤ä¸Žé•œè±¡ç§‘æŠ€å…¬å¸åˆä½œå®Œæˆ, æ•™å­¦æ•™è‚²å¤§æ¨¡åž‹EduChatåŸºäºŽBELLEï¼ˆBELLEåŸºäºŽLLaMAï¼‰|
|æ˜†ä»‘ä¸‡ç»´ | [å¤©å·¥](https://github.com/SkyWorkAIGC)|ä¸Žå¥‡ç‚¹æ™ºæºè”åˆç ”å‘||
|æ™ºåª’å¼€æºç ”ç©¶é™¢| [æ™ºåª’](https://github.com/IMOSR/Media-LLaMA)|åŸºäºŽLLaMAï¼Œé¢å‘è‡ªåª’ä½“|
|åŒ»ç–—ç®—ç½‘|Uni-talk|ä¸Šæµ·è”é€š+åŽå±±åŒ»é™¢+ä¸Šæµ·è¶…ç®—ä¸­å¿ƒ+åŽä¸º|
|èš‚èšé›†å›¢|è´žä»ª|æ®ä¼ è¯­è¨€å’Œå¤šæ¨¡æ€ä¸¤ä¸ª|
|é¦™æ¸¯ç§‘æŠ€å¤§å­¦|[ç½—å®¾Robin](https://huggingface.co/OptimalScale)|åŸºäºŽ[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA),[æ¸¯ç§‘å¤§å¼€æºLMFlow](https://github.com/OptimalScale/LMFlow)|
|è…¾è®¯|æ··å…ƒ|-|
|æ‹“å°”æ€|[æ‹“å¤©](https://mp.weixin.qq.com/s/beQardxjpner6vvk_LTOJA)| ä¸­æ–‡é€šç”¨å¤§æ¨¡åž‹|
|ä¹è¨€ç§‘æŠ€|ä¹è¨€ | TRSGPT|
|æ¸…åšæ™ºèƒ½ | [å…ˆé—®](https://mp.weixin.qq.com/s/Et-nVHjxDP3W-PFWmDo3YQ)|åŸºäºŽç»“æž„åŒ–æ•°æ®|
|æ™ºå­å¼•æ“Ž | [å…ƒä¹˜è±¡](https://chatimg.aixiaoqingxu.com/)|æ‰‹æœºå·å¿«é€Ÿç™»å½•ï¼Œä½¿ç”¨æ–¹ä¾¿|
|æ‹“ä¸–ç§‘æŠ€ |[æ‹“ä¸–](https://tskjgroup.com/article_news?id=822)|[æ•°ä¸‡äº¿å‚æ•°é‡ï¼Œé€šç”¨é¢†åŸŸ](https://tskjgroup.com/article_news?id=900)|
|å¾ªçŽ¯æ™ºèƒ½| [ç›˜å¤](https://www.rcrai.com/product/ai/pangu)|å¾ªçŽ¯æ™ºèƒ½,æ¸…åŽå¤§å­¦,åŽä¸º |
|å°è±¡ç¬”è®°|[å¤§è±¡GPT](https://mp.weixin.qq.com/s/O3nRSKBM29bCWKuRRi_PBg)| AGIæ™ºèƒ½åŒ–äº§å“|
|ç¬¬å››èŒƒå¼|[å¼è¯´](https://www.4paradigm.com/product/SageGPT.html)|ä»¥ç”Ÿæˆå¼AIé‡æž„ä¼ä¸šè½¯ä»¶ï¼ˆAI-Generated Softwareï¼‰ï¼Œæå‡ä¼ä¸šè½¯ä»¶çš„ä½“éªŒå’Œå¼€å‘æ•ˆçŽ‡ã€‚|
|å­—èŠ‚è·³åŠ¨ |Grace|å†…éƒ¨ä»£å·|
|å‡ºé—¨é—®é—®|[åºåˆ—çŒ´å­](https://write.mobvoi.com/)| AIå†™ä½œåŠ©ç†å¤§æ¨¡åž‹|
|æ•°è¯´æ•…äº‹|[SocialGPT](https://mp.weixin.qq.com/s/Tt3dcwefIvdlyB_IwaFYRQ)| èšç„¦ç¤¾äº¤å¯¹è¯å¤§æ¨¡åž‹|
|äº‘ä»Žç§‘æŠ€|[ä»Žå®¹](https://www.cloudwalk.com/news/show/id/178)|é€šç”¨å¤§æ¨¡åž‹|
|æµªæ½®ä¿¡æ¯|[æº](https://air.inspur.com/) |è®ºæ–‡æ”¯æ’‘â€”â€”[æº](https://github.com/Shawn-Inspur/Yuan-1.0)|
|ä¸­å›½å†œä¸šé“¶è¡Œ|[å°æ•°ChatABC](https://mp.weixin.qq.com/s/CXyZRIqhwrcGAKxzUC-qgg)|é‡‘èžè¡Œä¸šå¤§æ¨¡åž‹ |
|éº’éºŸåˆç›› |[å¤©ç‡•AiLMe](https://www.apusai.com/)| éœ€è¦è´¦å·ç™»å½•ï¼Œ[ç™»å½•ä½ç½®](https://ailme.apusai.com/#/login)|
|å°æ™ºäº‘|[ç¦å°”æ‘©æ–¯FFM](https://tws.twcc.ai/afs/)|åŽç¡•å­å…¬å¸|
|åŒ»è”ç§‘æŠ€|[medGPT](https://www.medlinker.com/news/198)| å›½å†…é¦–æ¬¾AIåŒ»ç”Ÿ|
|ç†æƒ³æ±½è½¦|MindGPT| -|
|æ·±æ€è€ƒäººå·¥æ™ºèƒ½|[Dongni](https://www.dongni.ai/)| ç™»å½•éœ€è¦è´¦å·|
|é•¿è™¹|é•¿è™¹è¶…è„‘ |-|
|å­©å­çŽ‹|KidsGPT|- |
|ä¸­ç§‘é—»æ­Œ|[é›…æ„](https://mp.weixin.qq.com/s/IGYV3t3JRlq4quvNJmZ4vA)|åª’ä½“ã€é‡‘èžã€å®£ä¼ ç­‰é¢†åŸŸçš„å¤§æ¨¡åž‹åº”ç”¨|
|ä¸­å›½è”é€š |é¸¿æ¹–|-|
|æ€å¿…é©°|[DFM-2](https://mp.weixin.qq.com/s/FxLw5UfJpYS1tCPDMkhvXA)|é€šç”¨å¤§æ¨¡åž‹|
|ä¸­ç§‘åˆ›è¾¾|é­”æ–¹Rubik| -|
|ç”µç§‘å¤ªæž |[å°å¯](https://mp.weixin.qq.com/s/8ci7g7R9j3pxkQC4UOLh2A)|å…šæ”¿ä¼è¡Œä¸šåº”ç”¨ |
|ä¸­å›½ç§»åŠ¨|ä¹å¤©|-|
|ä¸­å›½ç”µä¿¡|TeleChat|-|
|å®¹è”äº‘|èµ¤å…”|å®¢æœï¼Œè¥é”€|
|äº‘å¤©åŠ±é£ž|å¤©ä¹¦|-|
|ç»´æ™ºç§‘æŠ€|CityGPT|åŸŽå¸‚å¤§æ¨¡åž‹|
|æ¾œèˆŸç§‘æŠ€| [å­Ÿå­](https://www.langboat.com/portal/mengzi-model) |è‡ªç ”å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹|
|äº¬ä¸œ|[è¨€çŠ€](https://www.jdcloud.com/cn/news/detail/1235)|é¢å‘ä¸åŒè¿‡äº§ä¸šå¤§æ¨¡åž‹|
|æ™ºè‡»æ™ºèƒ½|[åŽè—](https://mp.weixin.qq.com/s/MZO2tvun05WnJkSe0seJnw)|å°iæœºå™¨äºº|
|æ–°åŽä¸‰H3C|ç™¾ä¸šçµçŠ€|-|
|é¹åŸŽå®žéªŒå®¤|é¹åŸŽÂ·è„‘æµ·|Peng Cheng Mind|
|å®‡è§†ç§‘æŠ€|[æ¢§æ¡](https://mp.weixin.qq.com/s/H8FsrEyJsIijy0Cowyu3GQ)|AIoTè¡Œä¸š |
|ç½‘æ˜“æœ‰é“|å­æ›° |- |
|ç¾ŽäºšæŸç§‘|[å¤©æ“Ž](https://mp.weixin.qq.com/s/D3ki3G4Q7QZPAVJ8iwTvDg)|å…¬å…±å®‰å…¨|
|èµ›çµåŠ›ç§‘æŠ€ |è¾¾å°”æ–‡|èµ›çµåŠ›,æ¸…åŽç ä¸‰è§’ç ”ç©¶é™¢,èµ›ä¸šç”Ÿç‰©,å¤§æ¹¾åŒºç§‘æŠ€åˆ›æ–°æœåŠ¡ä¸­å¿ƒ|
|ä½³éƒ½ç§‘æŠ€ |ä½³éƒ½çŸ¥è¡Œ |äº¤é€šé¢†åŸŸ|
|çŸ¥ä¹Ž|çŸ¥æµ·å›¾|çŸ¥ä¹Žå’Œé¢å£ç§‘æŠ€åˆä½œ|
|å®žåœ¨æ™ºèƒ½ |å¡”æ–¯| TARS|
|ç½‘æ˜“ä¼ç¾²|çŽ‰è¨€|- |
|åŒ—äº¬å¤§å­¦ä¿¡æ¯å·¥ç¨‹å­¦é™¢|[ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw)|[ChatLaw-13B](https://huggingface.co/JessyTsu1/ChatLaw-13B)åŸºäºŽZiya-LLaMA-13B-v1->LLaMA,[ChatLaw-33B](https://huggingface.co/JessyTsu1/ChatLaw-33B)åŸºäºŽAnima33B->Guanaco->[LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)|
|åŽå—ç†å·¥å¤§å­¦|[æ‰é¹Š](https://github.com/scutcyr/BianQue),[çµå¿ƒSoulChat](https://github.com/scutcyr/SoulChat)|åŒ»ç–—å¤§æ¨¡åž‹|
|ä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€|[ç™¾è†](https://github.com/ictnlp/BayLing)|åŸºäºŽ [LLaMA](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)ï¼Œæƒé‡Diffä¸‹è½½[7B](https://huggingface.co/ICTNLP/bayling-7b-diff)å’Œ[13B](https://huggingface.co/ICTNLP/bayling-13b-diff),[demo](http://nlp.ict.ac.cn/bayling/demo/) |
|æ²ªæ¸äººå·¥æ™ºèƒ½ç ”ç©¶é™¢|[å…†è¨€](https://mp.weixin.qq.com/s/RtcdWGrfIW5unyvxVplCSg)|ä¹Ÿç§°ï¼šä¸Šæµ·äº¤é€šå¤§å­¦é‡åº†äººå·¥æ™ºèƒ½ç ”ç©¶é™¢|
|ä¼æŸ¥æŸ¥|çŸ¥å½¼é˜¿å°”æ³•|-|
|è¶…å¯¹ç§°æŠ€æœ¯å…¬å¸| [ä¹¾å…ƒ](https://bbt.ssymmetry.com/)|BBT-1-1Bé‡‘èžæ¨¡åž‹ï¼ŒBBT-2-12B-TFé‡‘èžæ¨¡åž‹ï¼ŒBBT-2-12B-TCä»£ç æ¨¡åž‹ï¼ŒBBT-2-12B-Imageæ–‡ç”Ÿå›¾æ¨¡åž‹ï¼ŒBBT-2-12B-Scienceç§‘å­¦è®ºæ–‡æ¨¡åž‹ï¼ŒBBT-2.5-13B-Textä¸­è‹±åŒè¯­åŸºç¡€æ¨¡åž‹|
|æ¸…ç¿æ™ºèƒ½|[ArynGPT](https://mp.weixin.qq.com/s/FFRfzwoXBM2dGs9O7F-Z5A) |è‹±è¯­æ™ºèƒ½å¯¹è¯å£è¯­è€å¸ˆ|
|å¾®ç›Ÿ|[WAI](https://wai.weimob.com/)|-|
|èœœåº¦|[æ–‡ä¿®](https://mp.weixin.qq.com/s/aHSw9Kxib3Zj84qDGn3Dyg)|æ™ºèƒ½æ ¡å¯¹|
|ä¸­å›½ç”µå­äº‘|[æ˜Ÿæ™º](https://mp.weixin.qq.com/s/qNoTD4BY2DX5ziSe1ZPSLw)|æ”¿åŠ¡å¤§æ¨¡åž‹|
|è¥¿åŒ—å·¥ä¸šå¤§å­¦+åŽä¸º |[ç§¦å²­Â·ç¿±ç¿”](https://www.nwpu.edu.cn/info/1198/65828.htm) |æµä½“åŠ›å­¦å¤§æ¨¡åž‹,æ¹æµ+æµåœº |
|å¥‡ç‚¹æ™ºæº| [Singularity OpenAPI](https://openapi.singularity-ai.com/)|[ç‘¶å…‰å’Œå¤©æž¢](https://openapi.singularity-ai.com/index.html#/documentIndex)|
|è”æ±‡ç§‘æŠ€ |æ¬§å§†|[OmModelæ¬§å§†å¤šæ¨¡æ€ï¼ˆè§†è§‰è¯­è¨€ï¼‰å¤§æ¨¡åž‹](https://om.linker.cc/)|
|é˜…æ–‡é›†å›¢|ç½‘æ–‡å¤§æ¨¡åž‹|[å›½å†…é¦–ä¸ªç½‘æ–‡è¡Œä¸šå¤§æ¨¡åž‹](https://app.gmdaily.cn/as/opened/n/ba6a7ddbbef54233b2fec2cb43e9d3c7)|
|åŒ—äº¬äº¤é€šå¤§å­¦|[TransGPT](https://github.com/DUOMO/TransGPT)|[å›½å†…é¦–ä¸ªç»¼åˆäº¤é€šé¢†åŸŸçš„å¤§æ¨¡åž‹](https://github.com/DUOMO/TransGPT)|

---



> # Please keep adding relevant information, we greatly appreciate your contributions.
