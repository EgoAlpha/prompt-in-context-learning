# üìÑ Foundation Models

## Paper List

<div style="line-height:0.2em;">


[**TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts**](https://arxiv.org/abs/2407.03203) Ôºà**2024.07.03**Ôºâ

<font color="gray">Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Pedestrian 3D Shape Understanding for Person Re-Identification via Multi-View Learning**](https://doi.org/10.1109/TCSVT.2024.3358850) Ôºà**2024.07.01**Ôºâ

<font color="gray">Zaiyang Yu, Lusi Li, Jinlong Xie, Changshuo Wang, Weijun Li, etc .  - „ÄêIEEE transactions on circuits and systems for video technology (Print)„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs**](https://arxiv.org/abs/2406.20086) Ôºà**2024.06.28**Ôºâ

<font color="gray">Sheridan Feucht, David Atkinson, Byron C. Wallace, David Bau </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding**](https://arxiv.org/abs/2406.19389) Ôºà**2024.06.27**Ôºâ

<font color="gray">Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, etc </font>

![](https://img.shields.io/badge/Citations-2-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-934-blue)](https://github.com/lxtgh/omg-seg)

---

[**Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?**](https://arxiv.org/abs/2406.19354) Ôºà**2024.06.27**Ôºâ

<font color="gray">Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Efficient World Models with Context-Aware Tokenization**](https://arxiv.org/abs/2406.19320) Ôºà**2024.06.27**Ôºâ

<font color="gray">Vincent Micheli, Eloi Alonso, Franccois Fleuret </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)  [![](https://img.shields.io/badge/Github%20Stars-49-blue)](https://github.com/vmicheli/delta-iris)

---

[**The Remarkable Robustness of LLMs: Stages of Inference?**](https://arxiv.org/abs/2406.19384) Ôºà**2024.06.27**Ôºâ

<font color="gray">Vedang Lad, Wes Gurnee, Max Tegmark </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)

---

[**ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models**](https://arxiv.org/abs/2406.18125) Ôºà**2024.06.26**Ôºâ

<font color="gray">Ahmed Heakl, Youssef Mohamed, Noran Mohamed, Ali Sharkaway, A. Zaky </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/noran-mohamed/Resume-Classification-Dataset)

---

[**AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation**](https://arxiv.org/abs/2406.12805) Ôºà**2024.06.18**Ôºâ

<font color="gray">Xinyu Hou, Xiaoming Li, Chen Change Loy </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/itsmag11/aitti)

---

[**Unveiling Encoder-Free Vision-Language Models**](https://arxiv.org/abs/2406.11832) Ôºà**2024.06.17**Ôºâ

<font color="gray">Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-75-blue)](https://github.com/baaivision/eve)

---

[**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**](https://arxiv.org/abs/2406.11811) Ôºà**2024.06.17**Ôºâ

<font color="gray">Jo√£o Monteiro, Pierre-Andre Noel, √âtienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**LLaNA: Large Language and NeRF Assistant**](https://arxiv.org/abs/2406.11840) Ôºà**2024.06.17**Ôºâ

<font color="gray">Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**VideoLLM-online: Online Video Large Language Model for Streaming Video**](https://arxiv.org/abs/2406.11816) Ôºà**2024.06.17**Ôºâ

<font color="gray">Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models**](https://doi.org/10.1145/3662158.3662806) Ôºà**2024.06.17**Ôºâ

<font color="gray">Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, etc .  - „ÄêACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing„Äë</font>

![](https://img.shields.io/badge/Citations-6-green)

---

[**VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**](https://arxiv.org/abs/2406.10228) Ôºà**2024.06.14**Ôºâ

<font color="gray">Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models**](https://arxiv.org/abs/2406.10224) Ôºà**2024.06.14**Ôºâ

<font color="gray">Julian Straub, Daniel DeTone, Tianwei Shen, Nan Yang, Chris Sweeney, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions**](https://arxiv.org/abs/2406.08842) Ôºà**2024.06.13**Ôºâ

<font color="gray">Xu Zhang, Xunjian Yin, Xiaojun Wan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Advancing High Resolution Vision-Language Models in Biomedicine**](https://arxiv.org/abs/2406.09454) Ôºà**2024.06.12**Ôºâ

<font color="gray">Zekai Chen, Arda Pekis, Kevin Brown </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-19-blue)](https://github.com/standardmodelbio/llama3-med)

---

[**Enhancing End-to-End Autonomous Driving with Latent World Model**](https://arxiv.org/abs/2406.08481) Ôºà**2024.06.12**Ôºâ

<font color="gray">Yingyan Li, Lue Fan, Jiawei He, Yu-Quan Wang, Yuntao Chen, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-60-blue)](https://github.com/bravegroup/law)

---

[**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**](https://arxiv.org/abs/2406.08464) Ôºà**2024.06.12**Ôºâ

<font color="gray">Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, R. Poovendran, etc </font>

![](https://img.shields.io/badge/Citations-2-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)  [![](https://img.shields.io/badge/Github%20Stars-159-blue)](https://github.com/magpie-align/magpie)

---

[**Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams**](https://arxiv.org/abs/2406.08085) Ôºà**2024.06.12**Ôºâ

<font color="gray">Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)  [![](https://img.shields.io/badge/Github%20Stars-21-blue)](https://github.com/IVGSZ/Flash-VStream)

---

[**Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning**](https://arxiv.org/abs/2406.07543) Ôºà**2024.06.11**Ôºâ

<font color="gray">Chenyu Yang, Xizhou Zhu, Jinguo Zhu, Weijie Su, Junjie Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-47-blue)](https://github.com/opengvlab/lcl)

---

[**3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination**](https://arxiv.org/abs/2406.05132) Ôºà**2024.06.07**Ôºâ

<font color="gray">Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/sled-group/3D-GRAND)

---

[**Improving Alignment and Robustness with Circuit Breakers**](https://arxiv.org/abs/2406.04313) Ôºà**2024.06.06**Ôºâ

<font color="gray">Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, etc </font>

![](https://img.shields.io/badge/Citations-2-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-69-blue)](https://github.com/blackswan-ai/circuit-breakers)

---

[**Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models**](https://arxiv.org/abs/2406.04274) Ôºà**2024.06.06**Ôºâ

<font color="gray">Xiang Ji, Sanjeev Kulkarni, Mengdi Wang, Tengyang Xie </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Verbalized Machine Learning: Revisiting Machine Learning with Language Models**](https://arxiv.org/abs/2406.04344) Ôºà**2024.06.06**Ôºâ

<font color="gray">Tim Z. Xiao, Robert Bamler, Bernhard Scholkopf, Weiyang Liu </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training**](https://arxiv.org/abs/2406.03488) Ôºà**2024.06.05**Ôºâ

<font color="gray">Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/maydomine/seq1f1b)

---

[**SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining**](https://arxiv.org/abs/2406.02214) Ôºà**2024.06.04**Ôºâ

<font color="gray">Andi Han, Jiaxiang Li, Wei Huang, Mingyi Hong, Akiko Takeda, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-13-blue)](https://github.com/andyjm3/SLTrain)

---

[**Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**](https://doi.org/10.48550/arXiv.2405.21060) Ôºà**2024.05.31**Ôºâ

<font color="gray">Tri Dao, Albert Gu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-16-green)  [![](https://img.shields.io/badge/Github%20Stars-11.5k-blue)](https://github.com/state-spaces/mamba)

---

[**Spectrum-Aware Parameter Efficient Fine-Tuning for Diffusion Models**](https://doi.org/10.48550/arXiv.2405.21050) Ôºà**2024.05.31**Ôºâ

<font color="gray">Xinxi Zhang, Song Wen, Ligong Han, Felix Juefei-Xu, Akash Srivastava, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond**](https://doi.org/10.48550/arXiv.2405.21013) Ôºà**2024.05.31**Ôºâ

<font color="gray">Pengyuan Lyu, Yulin Li, Hao Zhou, Weihong Ma, Xingyu Wan, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment**](https://doi.org/10.48550/arXiv.2405.20830) Ôºà**2024.05.31**Ôºâ

<font color="gray">Yueqin Yin, Zhendong Wang, Yujia Xie, Weizhu Chen, Mingyuan Zhou .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**](https://doi.org/10.48550/arXiv.2405.21028) Ôºà**2024.05.31**Ôºâ

<font color="gray">Elias Stengel-Eskin, Peter Hase, Mohit Bansal .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/esteng/pragmatic_calibration)

---

[**Graph External Attention Enhanced Transformer**](https://doi.org/10.48550/arXiv.2405.21061) Ôºà**2024.05.31**Ôºâ

<font color="gray">Jianqing Liang, Min Chen, Jiye Liang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/icm1018/geaet)

---

[**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**](https://doi.org/10.48550/arXiv.2405.19332) Ôºà**2024.05.29**Ôºâ

<font color="gray">Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-52-blue)](https://github.com/shenao-zhang/selm)

---

[**Visualizing the loss landscape of Self-supervised Vision Transformer**](https://doi.org/10.48550/arXiv.2405.18042) Ôºà**2024.05.28**Ôºâ

<font color="gray">Youngwan Lee, Jeffrey Willette, Jonghee Kim, Sung Ju Hwang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models**](https://doi.org/10.48550/arXiv.2405.15574) Ôºà**2024.05.24**Ôºâ

<font color="gray">Byung-Kwan Lee, Chae Won Kim, Beomchan Park, Yonghyun Ro .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-85-blue)](https://github.com/byungkwanlee/meteor)

---

[**Disease-informed Adaptation of Vision-Language Models**](https://doi.org/10.48550/arXiv.2405.15728) Ôºà**2024.05.24**Ôºâ

<font color="gray">Jiajin Zhang, Ge Wang, M. Kalra, P. Yan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/rpidial/disease-informed-vlm-adaptation)

---

[**VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks**](https://doi.org/10.48550/arXiv.2405.15179) Ôºà**2024.05.24**Ôºâ

<font color="gray">Yang Li, Shaobo Han, Shihao Ji .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/leo-yangli/vb-lora)

---

[**DAGER: Exact Gradient Inversion for Large Language Models**](https://doi.org/10.48550/arXiv.2405.15586) Ôºà**2024.05.24**Ôºâ

<font color="gray">Ivo Petrov, D. I. Dimitrov, Maximilian Baader, Mark Niklas M√ºller, Martin T. Vechev .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Exploring Alignment in Shared Cross-lingual Spaces**](https://doi.org/10.48550/arXiv.2405.14535) Ôºà**2024.05.23**Ôºâ

<font color="gray">Basel Mousi, Nadir Durrani, Fahim Dalvi, Majd Hawasly, Ahmed Abdelali .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Spectral Adapter: Fine-Tuning in Spectral Space**](https://doi.org/10.48550/arXiv.2405.13952) Ôºà**2024.05.22**Ôºâ

<font color="gray">Fangzhao Zhang, Mert Pilanci .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Large Language Models are Biased Reinforcement Learners**](https://doi.org/10.48550/arXiv.2405.11422) Ôºà**2024.05.19**Ôºâ

<font color="gray">William M. Hayes, Nicolas Yax, Stefano Palminteri .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Libra: Building Decoupled Vision System on Large Language Models**](https://doi.org/10.48550/arXiv.2405.10140) Ôºà**2024.05.16**Ôºâ

<font color="gray">Yifan Xu, Xiaoshan Yang, Y. Song, Changsheng Xu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-33-blue)](https://github.com/yifanxu74/libra)

---

[**Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model**](https://doi.org/10.48550/arXiv.2405.10316) Ôºà**2024.05.16**Ôºâ

<font color="gray">Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Improving Transformers with Dynamically Composable Multi-Head Attention**](https://doi.org/10.48550/arXiv.2405.08553) Ôºà**2024.05.14**Ôºâ

<font color="gray">Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-139-blue)](https://github.com/caiyun-ai/dcformer)

---

[**Efficient Vision-Language Pre-training by Cluster Masking**](https://doi.org/10.48550/arXiv.2405.08815) Ôºà**2024.05.14**Ôºâ

<font color="gray">Zihao Wei, Zixuan Pan, Andrew Owens .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-18-blue)](https://github.com/zi-hao-wei/efficient-vision-language-pre-training-by-cluster-masking)

---

[**Linearizing Large Language Models**](https://doi.org/10.48550/arXiv.2405.06640) Ôºà**2024.05.10**Ôºâ

<font color="gray">Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-69-blue)](https://github.com/tri-ml/linear_open_lm)

---

[**LLM-Generated Black-box Explanations Can Be Adversarially Helpful**](https://doi.org/10.48550/arXiv.2405.06800) Ôºà**2024.05.10**Ôºâ

<font color="gray">R. Ajwani, Shashidhar Reddy Javaji, Frank Rudzicz, Zining Zhu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Vision Mamba: A Comprehensive Survey and Taxonomy**](https://doi.org/10.48550/arXiv.2405.04404) Ôºà**2024.05.07**Ôºâ

<font color="gray">Xiao Liu, Chenxu Zhang, Lei Zhang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-53-blue)](https://github.com/lx6c78/vision-mamba-a-comprehensive-survey-and-taxonomy)

---

[**Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks**](https://doi.org/10.48550/arXiv.2405.04403) Ôºà**2024.05.07**Ôºâ

<font color="gray">Georgios Pantazopoulos, Amit Parekh, Malvina Nikandrou, Alessandro Suglia .  - „ÄêSAFETY4CONVAI„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/gpantaz/vl_jailbreak)

---

[**vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention**](https://doi.org/10.48550/arXiv.2405.04437) Ôºà**2024.05.07**Ôºâ

<font color="gray">Ramya Prabhu, Ajay Nayak, Jayashree Mohan, R. Ramjee, Ashish Panwar .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo**](https://doi.org/10.48550/arXiv.2405.02128) Ôºà**2024.05.03**Ôºâ

<font color="gray">Nakul Rampal, Kaiyu Wang, Matthew Burigana, Lingxiang Hou, Juri Al-Johani, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment**](https://doi.org/10.48550/arXiv.2405.01481) Ôºà**2024.05.02**Ôºâ

<font color="gray">Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-422-blue)](https://github.com/nvidia/nemo-aligner)

---

[**Self-Play Preference Optimization for Language Model Alignment**](https://doi.org/10.48550/arXiv.2405.00675) Ôºà**2024.05.01**Ôºâ

<font color="gray">Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-19-green)  [![](https://img.shields.io/badge/Github%20Stars-281-blue)](https://github.com/uclaml/sppo)

---

[**When Quantization Affects Confidence of Large Language Models?**](https://doi.org/10.48550/arXiv.2405.00632) Ôºà**2024.05.01**Ôºâ

<font color="gray">Irina Proskurina, Luc Brun, Guillaume Metzler, Julien Velcin .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/upunaprosk/quantized-lm-confidence)

---

[**RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization**](https://doi.org/10.48550/arXiv.2405.00657) Ôºà**2024.05.01**Ôºâ

<font color="gray">Dongqi Pu, Vera Demberg .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Investigating Automatic Scoring and Feedback using Large Language Models**](https://doi.org/10.48550/arXiv.2405.00602) Ôºà**2024.05.01**Ôºâ

<font color="gray">G. Katuka, Alexander Gain, Yen-Yun Yu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**CultiVerse: Towards Cross-Cultural Understanding for Paintings with Large Language Model**](https://doi.org/10.48550/arXiv.2405.00435) Ôºà**2024.05.01**Ôºâ

<font color="gray">Wei Zhang, Wong Kam-Kwai, Biying Xu, Yiwen Ren, Yuhuai Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Lost in Recursion: Mining Rich Event Semantics in Knowledge Graphs**](https://doi.org/10.1145/3614419.3644001) Ôºà**2024.04.25**Ôºâ

<font color="gray">Florian Pl√∂tzky, Niklas Kiehne, Wolf-Tilo Balke .  - „ÄêWeb Science Conference„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Make Your LLM Fully Utilize the Context**](https://doi.org/10.48550/arXiv.2404.16811) Ôºà**2024.04.25**Ôºâ

<font color="gray">Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-7-green)  [![](https://img.shields.io/badge/Github%20Stars-225-blue)](https://github.com/microsoft/FILM)

---

[**Unifying Asynchronous Logics for Hyperproperties**](https://doi.org/10.48550/arXiv.2404.16778) Ôºà**2024.04.25**Ôºâ

<font color="gray">A. Bombardelli, L. Bozzelli, C'esar S'anchez, Stefano Tonetta .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**A Survey on Visual Mamba**](https://doi.org/10.48550/arXiv.2404.15956) Ôºà**2024.04.24**Ôºâ

<font color="gray">Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, etc .  - „ÄêApplied Sciences„Äë</font>

![](https://img.shields.io/badge/Citations-12-green)  [![](https://img.shields.io/badge/Github%20Stars-343-blue)](https://github.com/ziyangwang007/mamba-unet)

---

[**The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models**](https://doi.org/10.48550/arXiv.2404.16019) Ôºà**2024.04.24**Ôºâ

<font color="gray">Hannah Rose Kirk, Alexander Whitefield, Paul Rottger, Andrew M. Bean, Katerina Margatina, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-30-blue)](https://github.com/hannahkirk/prism-alignment)

---

[**Re-Thinking Inverse Graphics With Large Language Models**](https://doi.org/10.48550/arXiv.2404.15228) Ôºà**2024.04.23**Ôºâ

<font color="gray">Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, Michael J. Black .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models**](https://doi.org/10.48550/arXiv.2404.15236) Ôºà**2024.04.23**Ôºâ

<font color="gray">Aidan Z. H. Yang, Sophia Kolak, Vincent J. Hellendoorn, Ruben Martins, Claire Le Goues .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Does Instruction Tuning Make LLMs More Consistent?**](https://doi.org/10.48550/arXiv.2404.15206) Ôºà**2024.04.23**Ôºâ

<font color="gray">Constanza Fierro, Jiaang Li, Anders Sogaard .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models**](https://doi.org/10.48550/arXiv.2404.14755) Ôºà**2024.04.23**Ôºâ

<font color="gray">Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zuyong Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs**](https://doi.org/10.1145/3626772.3657712) Ôºà**2024.04.19**Ôºâ

<font color="gray">Clemencia Siro, Mohammad Aliannejadi, M. D. Rijke .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs**](https://doi.org/10.48550/arXiv.2404.13033) Ôºà**2024.04.19**Ôºâ

<font color="gray">Biyang Guo, He Wang, Wenyilin Xiao, Hong Chen, Zhuxin Lee, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-940-blue)](https://github.com/beyondguo/llm-tuning)

---

[**When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes**](https://doi.org/10.48550/arXiv.2404.12365) Ôºà**2024.04.18**Ôºâ

<font color="gray">Asaf Yehudai, Elron Bandel .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-156-blue)](https://github.com/ibm/fastfit)

---

[**Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models**](https://doi.org/10.48550/arXiv.2404.12139) Ôºà**2024.04.18**Ôºâ

<font color="gray">Shouwei Ruan, Yinpeng Dong, Hanqing Liu, Yao Huang, Hang Su, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning**](https://doi.org/10.48550/arXiv.2404.12353) Ôºà**2024.04.18**Ôºâ

<font color="gray">Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Moving Object Segmentation: All You Need Is SAM (and Flow)**](https://doi.org/10.48550/arXiv.2404.12389) Ôºà**2024.04.18**Ôºâ

<font color="gray">Junyu Xie, Charig Yang, Weidi Xie, Andrew Zisserman .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-219-blue)](https://github.com/Jyxarthur/flowsam)

---

[**Quantifying Multilingual Performance of Large Language Models Across Languages**](https://doi.org/10.48550/arXiv.2404.11553) Ôºà**2024.04.17**Ôºâ

<font color="gray">Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ninghao Liu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Self-Supervised Visual Preference Alignment**](https://doi.org/10.48550/arXiv.2404.10501) Ôºà**2024.04.16**Ôºâ

<font color="gray">Ke Zhu, Liang Zhao, Zheng Ge, Xiangyu Zhang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-21-blue)](https://github.com/Kevinz-code/SeVa)

---

[**RecurrentGemma: Moving Past Transformers for Efficient Open Language Models**](https://arxiv.org/abs/2404.07839) Ôºà**2024.04.11**Ôºâ

<font color="gray">Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George Muraru, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)  [![](https://img.shields.io/badge/Github%20Stars-565-blue)](https://github.com/google-deepmind/recurrentgemma)

---

[**Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding**](https://doi.org/10.48550/arXiv.2404.07989) Ôºà**2024.04.11**Ôºâ

<font color="gray">Yiwen Tang, Jiaming Liu, Dong Wang, Zhigang Wang, Shanghang Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-66-blue)](https://github.com/ivan-tang-3d/any2point)

---

[**OpenBias: Open-set Bias Detection in Text-to-Image Generative Models**](https://arxiv.org/abs/2404.07990) Ôºà**2024.04.11**Ôºâ

<font color="gray">Moreno D'Inc√†, E. Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Halu-NLP at SemEval-2024 Task 6: MetaCheckGPT - A Multi-task Hallucination Detection using LLM uncertainty and meta-models**](https://arxiv.org/abs/2404.06948) Ôºà**2024.04.10**Ôºâ

<font color="gray">Rahul Mehta, Andrew Hoblitzell, Jack O‚Äôkeefe, Hyeju Jang, Vasudeva Varma .  - „ÄêInternational Workshop on Semantic Evaluation„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Scaling Up Video Summarization Pretraining with Large Language Models**](https://arxiv.org/abs/2404.03398) Ôºà**2024.04.04**Ôºâ

<font color="gray">Dawit Mureja Argaw, Seunghyun Yoon, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)

---

[**Linear Attention Sequence Parallelism**](https://doi.org/10.48550/arXiv.2404.02882) Ôºà**2024.04.03**Ôºâ

<font color="gray">Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-58-blue)](https://github.com/opennlplab/lasp)

---

[**Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners**](https://arxiv.org/abs/2404.02117) Ôºà**2024.04.02**Ôºâ

<font color="gray">Keon-Hee Park, Kyungwoo Song, Gyeong-Moon Park </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)  [![](https://img.shields.io/badge/Github%20Stars-16-blue)](https://github.com/khu-agi/privilege)

---

[**Fault detection of complicated processes based on an enhanced transformer network with graph attention mechanism**](https://doi.org/10.1016/j.psep.2024.04.012) Ôºà**2024.04.01**Ôºâ

<font color="gray">Yuping Cao, Xiaoguang Tang, Xiaogang Deng, Ping Wang .  - „ÄêChemical engineering research & design„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**WavLLM: Towards Robust and Adaptive Speech Large Language Model**](https://doi.org/10.48550/arXiv.2404.00656) Ôºà**2024.03.31**Ôºâ

<font color="gray">Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Extensive Self-Contrast Enables Feedback-Free Language Model Alignment**](https://doi.org/10.48550/arXiv.2404.00604) Ôºà**2024.03.31**Ôºâ

<font color="gray">Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-25.7k-blue)](https://github.com/hiyouga/llama-factory)

---

[**MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning**](https://doi.org/10.48550/arXiv.2403.20320) Ôºà**2024.03.29**Ôºâ

<font color="gray">Ahmed A. Agiza, Marina Neseem, S. Reda .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ReALM: Reference Resolution As Language Modeling**](https://doi.org/10.48550/arXiv.2403.20329) Ôºà**2024.03.29**Ôºâ

<font color="gray">Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**RSMamba: Remote Sensing Image Classification with State Space Model**](https://doi.org/10.48550/arXiv.2403.19654) Ôºà**2024.03.28**Ôºâ

<font color="gray">Keyan Chen, Bo-Ying Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-202-blue)](https://github.com/KyanChen/RSMamba)

---

[**DreamLIP: Language-Image Pre-training with Long Captions**](https://arxiv.org/abs/2403.17007) Ôºà**2024.03.25**Ôºâ

<font color="gray">Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-47-blue)](https://github.com/zyf0619sjtu/DreamLIP)

---

[**Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model**](https://arxiv.org/abs/2403.13244) Ôºà**2024.03.20**Ôºâ

<font color="gray">Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric**](https://doi.org/10.48550/arXiv.2403.07839) Ôºà**2024.03.12**Ôºâ

<font color="gray">Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**VideoMamba: State Space Model for Efficient Video Understanding**](https://arxiv.org/abs/2403.06977) Ôºà**2024.03.11**Ôºâ

<font color="gray">Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-48-red)  [![](https://img.shields.io/badge/Github%20Stars-679-blue)](https://github.com/opengvlab/videomamba)

---

[**Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models**](https://arxiv.org/abs/2403.03900) Ôºà**2024.03.06**Ôºâ

<font color="gray">Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, James Caverlee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-71-blue)](https://github.com/chengkai-liu/mamba4rec)

---

[**Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training**](https://doi.org/10.48550/arXiv.2403.00249) Ôºà**2024.03.01**Ôºâ

<font color="gray">Haowei Liu, Yaya Shi, Haiyang Xu, Chunfen Yuan, Qinghao Ye, etc .  - „ÄêInternational Conference on Language Resources and Evaluation„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models**](https://arxiv.org/abs/2402.19427) Ôºà**2024.02.29**Ôºâ

<font color="gray">Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, etc </font>

![](https://img.shields.io/badge/Citations-4-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-74-red)  [![](https://img.shields.io/badge/Github%20Stars-565-blue)](https://github.com/google-deepmind/recurrentgemma)

---

[**LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs**](https://arxiv.org/abs/2402.18443) Ôºà**2024.02.28**Ôºâ

<font color="gray">Md Hafizur Rahman, Prabuddha Chakraborty </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models**](https://arxiv.org/abs/2403.08822) Ôºà**2024.02.28**Ôºâ

<font color="gray">Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning**](https://doi.org/10.48550/arXiv.2402.16829) Ôºà**2024.02.26**Ôºâ

<font color="gray">Aivin V. Solatorio .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-27-blue)](https://github.com/avsolatorio/gistembed)

---

[**Set the Clock: Temporal Alignment of Pretrained Language Models**](https://doi.org/10.48550/arXiv.2402.16797) Ôºà**2024.02.26**Ôºâ

<font color="gray">Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hanna Hajishirzi, Noah A. Smith .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-21-blue)](https://github.com/yizhongw/llm-temporal-alignment)

---

[**Generative Pretrained Hierarchical Transformer for Time Series Forecasting**](https://arxiv.org/abs/2402.16516) Ôºà**2024.02.26**Ôºâ

<font color="gray">Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/icantnamemyself/gpht)

---

[**GROUNDHOG: Grounding Large Language Models to Holistic Segmentation**](https://arxiv.org/abs/2402.16846) Ôºà**2024.02.26**Ôºâ

<font color="gray">Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)

---

[**Genie: Generative Interactive Environments**](https://arxiv.org/abs/2402.15391) Ôºà**2024.02.23**Ôºâ

<font color="gray">Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-103-red)

---

[**Self-evolving Autoencoder Embedded Q-Network**](https://doi.org/10.48550/arXiv.2402.11604) Ôºà**2024.02.18**Ôºâ

<font color="gray">Ieee J. Senthilnath Senior Member, Zhen Bangjian Zhou, Wei Ng, Deeksha Aggarwal, Rajdeep Dutta, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation**](https://doi.org/10.48550/arXiv.2402.10210) Ôºà**2024.02.15**Ôºâ

<font color="gray">Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Efficient Stagewise Pretraining via Progressive Subnetworks**](https://doi.org/10.48550/arXiv.2402.05913) Ôºà**2024.02.08**Ôºâ

<font color="gray">Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi, Sashank J. Reddi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ConvLoRA and AdaBN based Domain Adaptation via Self-Training**](https://doi.org/10.48550/arXiv.2402.04964) Ôºà**2024.02.07**Ôºâ

<font color="gray">Sidra Aleem, J. Dietlmeier, Eric Arazo, Suzanne Little .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LoTR: Low Tensor Rank Weight Adaptation**](https://doi.org/10.48550/arXiv.2402.01376) Ôºà**2024.02.02**Ôºâ

<font color="gray">Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, A. Mikhalev, Ivan Oseledets .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer**](https://doi.org/10.48550/arXiv.2401.06426) Ôºà**2024.01.12**Ôºâ

<font color="gray">Ji Liu, Dehua Tang, Yuanxian Huang, Li Zhang, Xiaocheng Zeng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation**](https://doi.org/10.48550/arXiv.2401.04468) Ôºà**2024.01.09**Ôºâ

<font color="gray">Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition**](https://doi.org/10.48550/arXiv.2401.02417) Ôºà**2024.01.04**Ôºâ

<font color="gray">David M. Chan, Shalini Ghosh, Hitesh Tulsiani, A. Rastrow, Bjorn Hoffmeister .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Instruct-Imagen: Image Generation with Multi-modal Instruction**](https://doi.org/10.48550/arXiv.2401.01952) Ôºà**2024.01.03**Ôºâ

<font color="gray">Hexiang Hu, Kelvin C.K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI**](https://doi.org/10.48550/arXiv.2312.16170) Ôºà**2023.12.26**Ôºâ

<font color="gray">Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-367-blue)](https://github.com/openrobotlab/embodiedscan)

---

[**Time is Encoded in the Weights of Finetuned Language Models**](https://arxiv.org/abs/2312.13401) Ôºà**2023.12.20**Ôºâ

<font color="gray">Kai Nylund, Suchin Gururangan, Noah A. Smith </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-28-red)  [![](https://img.shields.io/badge/Github%20Stars-68-blue)](https://github.com/KaiNylund/lm-weights-encode-time)

---

[**Photorealistic Video Generation with Diffusion Models**](https://arxiv.org/abs/2312.06662) Ôºà**2023.12.11**Ôºâ

<font color="gray">Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-115-red)

---

[**Mamba: Linear-Time Sequence Modeling with Selective State Spaces**](https://arxiv.org/abs/2312.00752) Ôºà**2023.12.01**Ôºâ

<font color="gray">Albert Gu, Tri Dao </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-885-red)  [![](https://img.shields.io/badge/Github%20Stars-11.5k-blue)](https://github.com/state-spaces/mamba)

---

[**Minimizing Factual Inconsistency and Hallucination in Large Language Models**](https://doi.org/10.48550/arXiv.2311.13878) Ôºà**2023.11.23**Ôºâ

<font color="gray">I. Muneeswaran, Shreya Saxena, Siva Prasad, M. V. S. Prakash, Advaith Shankar, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?**](https://doi.org/10.48550/arXiv.2311.13110) Ôºà**2023.11.22**Ôºâ

<font color="gray">Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1.1k-blue)](https://github.com/ma-lab-berkeley/crate)

---

[**Learning skillful medium-range global weather forecasting.**](https://doi.org/10.1126/science.adi2336) Ôºà**2023.11.14**Ôºâ

<font color="gray">Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, etc .  - „ÄêScience„Äë</font>

![](https://img.shields.io/badge/Citations-8-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-549-red)

---

[**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](https://doi.org/10.48550/arXiv.2311.08046) Ôºà**2023.11.14**Ôºâ

<font color="gray">Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, Li Yuan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-711-blue)](https://github.com/pku-yuangroup/chat-univi)

---

[**SpectralGPT: Spectral Foundation Model**](https://doi.org/10.48550/arXiv.2311.07113) Ôºà**2023.11.13**Ôºâ

<font color="gray">D. Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Social Motion Prediction with Cognitive Hierarchies**](https://doi.org/10.48550/arXiv.2311.04726) Ôºà**2023.11.08**Ôºâ

<font color="gray">Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Pre-training LLMs using human-like development data corpus**](https://doi.org/10.48550/arXiv.2311.04666) Ôºà**2023.11.08**Ôºâ

<font color="gray">Khushi Bhardwaj, Raj Sanjay Shah, Sashank Varma .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**](https://doi.org/10.48550/arXiv.2311.04257) Ôºà**2023.11.07**Ôºâ

<font color="gray">Qinghao Ye, Haiyang Xu, Jiabo Ye, Mingshi Yan, Anwen Hu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-2.0k-blue)](https://github.com/x-plug/mplug-owl)

---

[**Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation**](https://doi.org/10.48550/arXiv.2311.03348) Ôºà**2023.11.06**Ôºâ

<font color="gray">Rusheb Shah, Quentin Feuillade--Montixi, Soroush Pour, Arush Tagade, Stephen Casper, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Ziya2: Data-centric Learning is All LLMs Need**](https://doi.org/10.48550/arXiv.2311.03301) Ôºà**2023.11.06**Ôºâ

<font color="gray">Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Levels of AGI: Operationalizing Progress on the Path to AGI**](https://doi.org/10.48550/arXiv.2311.02462) Ôºà**2023.11.04**Ôºâ

<font color="gray">Meredith Ringel Morris, Jascha Narain Sohl-Dickstein, Noah Fiedel, T. Warkentin, Allan Dafoe, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**CodeFusion: A Pre-trained Diffusion Model for Code Generation**](https://doi.org/10.48550/arXiv.2310.17680) Ôºà**2023.10.26**Ôºâ

<font color="gray">Mukul Singh, J. Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**3D-GPT: Procedural 3D Modeling with Large Language Models**](https://doi.org/10.48550/arXiv.2310.12945) Ôºà**2023.10.19**Ôºâ

<font color="gray">Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**The Foundation Model Transparency Index**](https://arxiv.org/abs/2310.12941) Ôºà**2023.10.19**Ôºâ

<font color="gray">Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-40-red)  [![](https://img.shields.io/badge/Github%20Stars-61-blue)](https://github.com/stanford-crfm/fmti)

---

[**Language Models Represent Space and Time**](https://arxiv.org/abs/2310.02207) Ôºà**2023.10.03**Ôºâ

<font color="gray">Wes Gurnee, Max Tegmark </font>

![](https://img.shields.io/badge/Citations-3-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-113-red)

---

[**Chatmap : Large Language Model Interaction with Cartographic Data**](https://doi.org/10.48550/arXiv.2310.01429) Ôºà**2023.09.28**Ôºâ

<font color="gray">Eren Unlu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Effective Distillation of Table-based Reasoning Ability from LLMs**](https://doi.org/10.48550/arXiv.2309.13182) Ôºà**2023.09.22**Ôºâ

<font color="gray">Bohao Yang, Chen Tang, Kangning Zhao, Chenghao Xiao, Chenghua Lin .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/bernard-yang/distilltablecot)

---

[**Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions**](https://doi.org/10.48550/arXiv.2309.10150) Ôºà**2023.09.18**Ôºâ

<font color="gray">Yevgen Chebotar, Q. Vuong, A. Irpan, Karol Hausman, F. Xia, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Replacing softmax with ReLU in Vision Transformers**](https://doi.org/10.48550/arXiv.2309.08586) Ôºà**2023.09.15**Ôºâ

<font color="gray">Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, Simon Kornblith .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**ZGaming: Zero-Latency 3D Cloud Gaming by Image Prediction**](https://doi.org/10.1145/3603269.3604819) Ôºà**2023.09.01**Ôºâ

<font color="gray">Jiangkai Wu, Yu Guan, Qi Mao, Yong Cui, Zongming Guo, etc .  - „ÄêProceedings of the ACM SIGCOMM 2023 Conference„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**Explaining Vision and Language through Graphs of Events in Space and Time**](https://doi.org/10.48550/arXiv.2309.08612) Ôºà**2023.08.29**Ôºâ

<font color="gray">Mihai Masala, Nicolae Cudlenco, Traian Rebedea, Marius Leordeanu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation**](https://doi.org/10.48550/arXiv.2308.13746) Ôºà**2023.08.26**Ôºâ

<font color="gray">Ao Chang, Xing Tao, Xin Yang, Yuhao Huang, Xinrui Zhou, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection**](https://doi.org/10.48550/arXiv.2308.12863) Ôºà**2023.08.24**Ôºâ

<font color="gray">Xinyu Zhang, Yan Gong, Zhiwei Li, Xinchen Gao, Dafeng Jin, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding**](https://doi.org/10.48550/arXiv.2308.10529) Ôºà**2023.08.21**Ôºâ

<font color="gray">Tianyu Yu, Chengyue Jiang, Chao Lou, Shen Huang, Xiaobin Wang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-196-blue)](https://github.com/alibaba-nlp/seqgpt)

---

[**Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval**](https://doi.org/10.48550/arXiv.2308.07648) Ôºà**2023.08.15**Ôºâ

<font color="gray">Chaorui Deng, Qi Chen, Pengda Qin, Dave Zhenyu Chen, Qi Wu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-24-blue)](https://github.com/bladewaltz1/promptswitch)

---

[**VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use**](https://doi.org/10.48550/arXiv.2308.06595) Ôºà**2023.08.12**Ôºâ

<font color="gray">Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-45-blue)](https://github.com/mlfoundations/VisIT-Bench)

---

[**Accelerating LLM Inference with Staged Speculative Decoding**](https://doi.org/10.48550/arXiv.2308.04623) Ôºà**2023.08.08**Ôºâ

<font color="gray">Benjamin Spector, Christal Re .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment**](https://doi.org/10.48550/arXiv.2308.04352) Ôºà**2023.08.08**Ôºâ

<font color="gray">Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-7-green)  [![](https://img.shields.io/badge/Github%20Stars-173-blue)](https://github.com/3d-vista/3D-VisTA)

---

[**Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models**](https://doi.org/10.48550/arXiv.2308.03151) Ôºà**2023.08.06**Ôºâ

<font color="gray">Zheng Ma, Mianzhi Pan, Wen-Lan Wu, Ka Leong Cheng, Jianbing Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-7-blue)](https://github.com/aaronma2020/Food500-Cap)

---

[**Pre-Trained Large Language Models for Industrial Control**](https://doi.org/10.48550/arXiv.2308.03028) Ôºà**2023.08.06**Ôºâ

<font color="gray">Lei Song, Chuheng Zhang, Li Zhao, Jiang Bian .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Training Large-scale Foundation Models on Emerging AI Chips**](https://doi.org/10.1145/3580305.3599573) Ôºà**2023.08.04**Ôºâ

<font color="gray">Aashiq Muhamed, Christian Bock, R. Solanki, Youngsuk Park, Yida Wang, etc .  - „ÄêKnowledge Discovery and Data Mining„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**FLatten Transformer: Vision Transformer using Focused Linear Attention**](https://doi.org/10.48550/arXiv.2308.00442) Ôºà**2023.08.01**Ôºâ

<font color="gray">Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, Gao Huang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-358-blue)](https://github.com/leaplabthu/flatten-transformer)

---

[**Med-Flamingo: a Multimodal Medical Few-shot Learner**](https://doi.org/10.48550/arXiv.2307.15189) Ôºà**2023.07.27**Ôºâ

<font color="gray">Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, C. Zakka, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-367-blue)](https://github.com/snap-stanford/med-flamingo)

---

[**Universal and Transferable Adversarial Attacks on Aligned Language Models**](https://doi.org/10.48550/arXiv.2307.15043) Ôºà**2023.07.27**Ôºâ

<font color="gray">Andy Zou, Zifan Wang, J. Z. Kolter, Matt Fredrikson .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-19-green)  [![](https://img.shields.io/badge/Github%20Stars-3.1k-blue)](https://github.com/llm-attacks/llm-attacks)

---

[**CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots**](https://doi.org/10.48550/arXiv.2307.11865) Ôºà**2023.07.21**Ôºâ

<font color="gray">Nikhil Kakodkar, D. Rivkin, Bobak H. Baghi, F. Hogan, Gregory Dudek .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots**](https://arxiv.org/abs/2307.08715) Ôºà**2023.07.16**Ôºâ

<font color="gray">Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, etc </font>

![](https://img.shields.io/badge/Citations-13-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)

---

[**Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation**](https://doi.org/10.48550/arXiv.2307.01542) Ôºà**2023.07.04**Ôºâ

<font color="gray">Jian Guan, Minlie Huang .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/thu-coai/selfcont)

---

[**Kosmos-2: Grounding Multimodal Large Language Models to the World**](https://doi.org/10.48550/arXiv.2306.14824) Ôºà**2023.06.26**Ôºâ

<font color="gray">Zhiliang Peng, Wenhui Wang, Li Dong, Y. Hao, Shaohan Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-19.1k-blue)](https://github.com/microsoft/unilm/tree/master/kosmos-2)

---

[**AudioPaLM: A Large Language Model That Can Speak and Listen**](https://doi.org/10.48550/arXiv.2306.12925) Ôºà**2023.06.22**Ôºâ

<font color="gray">Paul K. Rubenstein, Chulayuth Asawaroengchai, D. Nguyen, Ankur Bapna, Zal√°n Borsos, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Unleashing the AI revolution: exploring the capabilities and challenges of large language models and text‚Äêto‚Äêimage AI programs**](https://doi.org/10.1002/uog.26297) Ôºà**2023.06.17**Ôºâ

<font color="gray">A. Youssef .  - „ÄêUltrasound in Obstetrics and Gynecology„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance**](https://doi.org/10.48550/arXiv.2306.05443) Ôºà**2023.06.08**Ôºâ

<font color="gray">Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-449-blue)](https://github.com/chancefocus/pixiu)

---

[**M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models**](https://doi.org/10.48550/arXiv.2306.05179) Ôºà**2023.06.08**Ôºâ

<font color="gray">Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-87-blue)](https://github.com/damo-nlp-sg/m3exam)

---

[**Simple and Controllable Music Generation**](https://doi.org/10.48550/arXiv.2306.05284) Ôºà**2023.06.08**Ôºâ

<font color="gray">Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-7.8k-blue)](https://github.com/facebookresearch/audiocraft)

---

[**LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion**](https://doi.org/10.48550/arXiv.2306.02561) Ôºà**2023.06.05**Ôºâ

<font color="gray">Dongfu Jiang, Xiang Ren, Bill Yuchen Lin .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-823-blue)](https://github.com/yuchenlin/LLM-Blender)

---

[**DiffRate : Differentiable Compression Rate for Efficient Vision Transformers**](https://doi.org/10.48550/arXiv.2305.17997) Ôºà**2023.05.29**Ôºâ

<font color="gray">Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-37-blue)](https://github.com/opengvlab/diffrate)

---

[**Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**](https://doi.org/10.48550/arXiv.2305.15805) Ôºà**2023.05.25**Ôºâ

<font color="gray">Sotiris Anagnostidis, Dario Pavllo, L. Biggio, Lorenzo Noci, Aur√©lien Lucchi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**On Degrees of Freedom in Defining and Testing Natural Language Understanding**](https://arxiv.org/abs/2305.15130) Ôºà**2023.05.24**Ôºâ

<font color="gray">Saku Sugawara, Shun Tsugita </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Structural Ambiguity and its Disambiguation in Language Model Based Parsers: the Case of Dutch Clause Relativization**](https://arxiv.org/abs/2305.14917) Ôºà**2023.05.24**Ôºâ

<font color="gray">Gijs Wijnholds, Michael Moortgat </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Mitigating Temporal Misalignment by Discarding Outdated Facts**](https://arxiv.org/abs/2305.14824) Ôºà**2023.05.24**Ôºâ

<font color="gray">Michael J.Q. Zhang, Eunsol Choi </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/mikejqzhang/mitigating_misalignment)

---

[**On the Generalization of Diffusion Model**](https://arxiv.org/abs/2305.14712) Ôºà**2023.05.24**Ôºâ

<font color="gray">Mingyang Yi, Jiacheng Sun, Zhenguo Li </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)

---

[**Vision + Language Applications: A Survey**](https://arxiv.org/abs/2305.14598) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yutong Zhou, Nobutaka Shimada </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-32-red)  [![](https://img.shields.io/badge/Github%20Stars-2.0k-blue)](https://github.com/yutong-zhou-cv/awesome-text-to-image)

---

[**Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective**](https://arxiv.org/abs/2305.15408) Ôºà**2023.05.24**Ôºâ

<font color="gray">Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-54-red)

---

[**Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets**](https://arxiv.org/abs/2305.15407) Ôºà**2023.05.24**Ôºâ

<font color="gray">Brandon Smith, Miguel Farinha, Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/oxai/debias-gensynth)

---

[**Unit-based Speech-to-Speech Translation Without Parallel Data**](https://arxiv.org/abs/2305.15405) Ôºà**2023.05.24**Ôºâ

<font color="gray">Anuj Diwan, Anirudh Srinivasan, David F. Harwath, Eunsol Choi </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation**](https://arxiv.org/abs/2305.15403) Ôºà**2023.05.24**Ôºâ

<font color="gray">Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)

---

[**A Neural Space-Time Representation for Text-to-Image Personalization**](https://arxiv.org/abs/2305.15391) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yuval Alaluf, Elad Richardson, Gal Metzer, Daniel Cohen-Or </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-24-red)  [![](https://img.shields.io/badge/Github%20Stars-160-blue)](https://github.com/NeuralTextualInversion/NeTI)

---

[**Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering**](https://arxiv.org/abs/2305.15387) Ôºà**2023.05.24**Ôºâ

<font color="gray">Avi Caciularu, Matthew E. Peters, Jacob Goldberger, Ido Dagan, Arman Cohan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)  [![](https://img.shields.io/badge/Github%20Stars-7-blue)](https://github.com/aviclu/peekacross)

---

[**SAMScore: A Semantic Structural Similarity Metric for Image Translation Evaluation**](https://arxiv.org/abs/2305.15367) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yunxiang Li, Meixu Chen, Wenxuan Yang, Kai Wang, Jun Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)  [![](https://img.shields.io/badge/Github%20Stars-15-blue)](https://github.com/kent0n-li/samscore)

---

[**Context-Aware Transformer Pre-Training for Answer Sentence Selection**](https://arxiv.org/abs/2305.15358) Ôºà**2023.05.24**Ôºâ

<font color="gray">Luca Di Liello, Siddhant Garg, Alessandro Moschitti </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence**](https://arxiv.org/abs/2305.15347) Ôºà**2023.05.24**Ôºâ

<font color="gray">Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-56-red)  [![](https://img.shields.io/badge/Github%20Stars-230-blue)](https://github.com/Junyi42/sd-dino)

---

[**Visual Programming for Text-to-Image Generation and Evaluation**](https://arxiv.org/abs/2305.15328) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jaemin Cho, Abhay Zala, Mohit Bansal </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-28-red)

---

[**Towards Foundation Models for Relational Databases [Vision Paper]**](https://arxiv.org/abs/2305.15321) Ôºà**2023.05.24**Ôºâ

<font color="gray">Liane Vogel, Benjamin Hilprecht, Carsten Binnig </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

---

[**MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation**](https://arxiv.org/abs/2305.15296) Ôºà**2023.05.24**Ôºâ

<font color="gray">Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Bjorn Deiseroth, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-20-red)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/aleph-alpha/multifusion)

---

[**ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers**](https://arxiv.org/abs/2305.15272) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)  [![](https://img.shields.io/badge/Github%20Stars-284-blue)](https://github.com/hustvl/ViTMatte)

---

[**Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model**](https://arxiv.org/abs/2305.15265) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/zirui-ray-liu/wtacrs)

---

[**LMs with a Voice: Spoken Language Modeling beyond Speech Tokens**](https://arxiv.org/abs/2305.15255) Ôºà**2023.05.24**Ôºâ

<font color="gray">Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Robust Classification via a Single Diffusion Model**](https://arxiv.org/abs/2305.15241) Ôºà**2023.05.24**Ôºâ

<font color="gray">Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)  [![](https://img.shields.io/badge/Github%20Stars-45-blue)](https://github.com/huanranchen/AdversarialAttacks)

---

[**Multi-modal Machine Learning for Vehicle Rating Predictions Using Image, Text, and Parametric Data**](https://arxiv.org/abs/2305.15218) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hanqi Su, Binyang Song, Faez Ahmed </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**L-CAD: Language-based Colorization with Any-level Descriptions**](https://arxiv.org/abs/2305.15217) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zheng Chang, Shuchen Weng, Pei Zhang, Yu Li, Si Li, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models**](https://arxiv.org/abs/2305.15194) Ôºà**2023.05.24**Ôºâ

<font color="gray">Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, Namhyuk Ahn </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)  [![](https://img.shields.io/badge/Github%20Stars-41-blue)](https://github.com/sungnyun/diffblender)

---

[**Pre-training Multi-party Dialogue Models with Latent Discourse Inference**](https://arxiv.org/abs/2305.15175) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yiyang Li, Xinting Huang, Wei Bi, Hai Zhao </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/ericlee8/mpd_emvi)

---

[**Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models**](https://arxiv.org/abs/2305.15171) Ôºà**2023.05.24**Ôºâ

<font color="gray">Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, Chi-Keung Tang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)

---

[**Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**](https://arxiv.org/abs/2305.15099) Ôºà**2023.05.24**Ôºâ

<font color="gray">Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)  [![](https://img.shields.io/badge/Github%20Stars-25-blue)](https://github.com/lumia-group/fouriertransformer)

---

[**CSTS: Conditional Semantic Textual Similarity**](https://arxiv.org/abs/2305.15093) Ôºà**2023.05.24**Ôºâ

<font color="gray">Ameet Deshpande, Carlos E. Jimenez, Howard Chen, Vishvak S. Murahari, Victoria Graf, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-22-red)

---

[**STAR: Boosting Low-Resource Event Extraction by Structure-to-Text Data Generation with Large Language Models**](https://arxiv.org/abs/2305.15090) Ôºà**2023.05.24**Ôºâ

<font color="gray">Mingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung, P. Jeffrey Brantingham, Nanyun Peng, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)

---

[**Contrastive Learning of Sentence Embeddings from Scratch**](https://arxiv.org/abs/2305.15077) Ôºà**2023.05.24**Ôºâ

<font color="gray">Junlei Zhang, Zhenzhong Lan, Junxian He </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-33-blue)](https://github.com/hkust-nlp/syncse)

---

[**Meta-Learning Online Adaptation of Language Models**](https://arxiv.org/abs/2305.15076) Ôºà**2023.05.24**Ôºâ

<font color="gray">Nathan J. Hu, Eric Mitchell, Christopher D. Manning, Chelsea Finn </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-25-red)  [![](https://img.shields.io/badge/Github%20Stars-21-blue)](https://github.com/nathanhu0/CaMeLS)

---

[**Who Wrote this Code? Watermarking for Code Generation**](https://arxiv.org/abs/2305.15060) Ôºà**2023.05.24**Ôºâ

<font color="gray">Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-17-blue)](https://github.com/hongcheki/sweet-watermark)

---

[**Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering**](https://arxiv.org/abs/2305.15056) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jiajie Zhang, Shulin Cao, Tingjia Zhang, Xin Lv, Jiaxin Shi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)

---

[**Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis**](https://arxiv.org/abs/2305.15054) Ôºà**2023.05.24**Ôºâ

<font color="gray">Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)

---

[**Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation**](https://arxiv.org/abs/2305.15048) Ôºà**2023.05.24**Ôºâ

<font color="gray">Mete Sertkan, Sophia Althammer, Sebastian Hofstatter </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/metesertkan/ranger)

---

[**Ghostbuster: Detecting Text Ghostwritten by Large Language Models**](https://arxiv.org/abs/2305.15047) Ôºà**2023.05.24**Ôºâ

<font color="gray">Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)  [![](https://img.shields.io/badge/Github%20Stars-123-blue)](https://github.com/vivek3141/ghostbuster)

---

[**Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science**](https://arxiv.org/abs/2305.15041) Ôºà**2023.05.24**Ôºâ

<font color="gray">Veniamin Veselovsky, Manoel Horta Ribeiro, Akhil Arora, Martin Josifoski, Ashton Anderson, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)

---

[**Active Learning for Natural Language Generation**](https://arxiv.org/abs/2305.15040) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

---

[**SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models**](https://arxiv.org/abs/2305.15033) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Ming Liu, Bing Qin </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives**](https://arxiv.org/abs/2305.15032) Ôºà**2023.05.24**Ôºâ

<font color="gray">Xinpeng Wang, Leonie Weissweiler, Hinrich Schutze, Barbara Plank </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-9-blue)](https://github.com/mainlp/how-to-distill-your-bert)

---

[**ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification**](https://arxiv.org/abs/2305.15024) Ôºà**2023.05.24**Ôºâ

<font color="gray">Biao Zhao, Weiqiang Jin, Javier Del Ser, Guang Yang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-93-red)  [![](https://img.shields.io/badge/Github%20Stars-38-blue)](https://github.com/albert-jin/agricultural_textual_classification_chatgpt)

---

[**Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models**](https://arxiv.org/abs/2305.15023) Ôºà**2023.05.24**Ôºâ

<font color="gray">Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-63-red)  [![](https://img.shields.io/badge/Github%20Stars-491-blue)](https://github.com/luogen1996/lavin)

---

[**Measuring Faithful and Plausible Visual Grounding in VQA**](https://arxiv.org/abs/2305.15015) Ôºà**2023.05.24**Ôºâ

<font color="gray">Daniel Reich, Felix Putze, Tanja Schultz </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Unlocking Temporal Question Answering for Large Language Models Using Code Execution**](https://arxiv.org/abs/2305.15014) Ôºà**2023.05.24**Ôºâ

<font color="gray">Xingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou Ng, Shafiq Joty, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/damo-nlp-sg/mvcr)

---

[**Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation**](https://arxiv.org/abs/2305.15011) Ôºà**2023.05.24**Ôºâ

<font color="gray">Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, Timothy Baldwin </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution**](https://arxiv.org/abs/2305.15010) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hongbo Zhang, Xiang Wan, Benyou Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/stevenzhb/bioplm_injectingknowledge)

---

[**LLMDet: A Large Language Models Detection Tool**](https://arxiv.org/abs/2305.15004) Ôºà**2023.05.24**Ôºâ

<font color="gray">Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

---

[**The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning**](https://arxiv.org/abs/2305.14999) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)

---

[**Reasoning with Language Model is Planning with World Model**](https://arxiv.org/abs/2305.14992) Ôºà**2023.05.24**Ôºâ

<font color="gray">Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-149-red)  [![](https://img.shields.io/badge/Github%20Stars-965-blue)](https://github.com/ber666/llm-reasoners)

---

[**MuLER: Detailed and Scalable Reference-based Evaluation**](https://arxiv.org/abs/2305.14991) Ôºà**2023.05.24**Ôºâ

<font color="gray">Taelin Karidi, Leshem Choshen, Gal Patel, Omri Abend </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Large Language Models are Effective Table-to-Text Generators, Evaluators, and Feedback Providers**](https://arxiv.org/abs/2305.14987) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)

---

[**Non-adversarial Robustness of Deep Learning Methods for Computer Vision**](https://arxiv.org/abs/2305.14986) Ôºà**2023.05.24**Ôºâ

<font color="gray">Gorana Goji'c, Vladimir Vincan, Ognjen Kundavcina, Dragivsa Mivskovi'c, Dinu Dragan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)

---

[**Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality**](https://arxiv.org/abs/2305.14981) Ôºà**2023.05.24**Ôºâ

<font color="gray">Tanay Dixit, Fei Wang, Muhao Chen </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)  [![](https://img.shields.io/badge/Github%20Stars-7-blue)](https://github.com/tanay2001/efactsum)

---

[**Sampling-based Uncertainty Estimation for an Instance Segmentation Network**](https://arxiv.org/abs/2305.14977) Ôºà**2023.05.24**Ôºâ

<font color="gray">Florian Heidecker, Ahmad El-Khateeb, Bernhard Sick </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**MMNet: Multi-Mask Network for Referring Image Segmentation**](https://arxiv.org/abs/2305.14969) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yichen Yan, Xingjian He, Wenxuan Wan, Jing Liu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks**](https://arxiv.org/abs/2305.14965) Ôºà**2023.05.24**Ôºâ

<font color="gray">Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Editing Commonsense Knowledge in GPT**](https://arxiv.org/abs/2305.14956) Ôºà**2023.05.24**Ôºâ

<font color="gray">Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Lorraine Li, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Focus Your Attention (with Adaptive IIR Filters)**](https://arxiv.org/abs/2305.14952) Ôºà**2023.05.24**Ôºâ

<font color="gray">Shahar Lutati, Itamar Zimerman, Lior Wolf </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low Resource Languages**](https://arxiv.org/abs/2305.14949) Ôºà**2023.05.24**Ôºâ

<font color="gray">Qi Gou, Zehua Xia, Wen-Hau Du </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Trade-Offs Between Fairness and Privacy in Language Modeling**](https://arxiv.org/abs/2305.14936) Ôºà**2023.05.24**Ôºâ

<font color="gray">Cleo Matzken, Steffen Eger, Ivan Habernal </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/cleolotta/fair-and-private-lm)

---

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

<font color="gray">Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/bsantraigi/frugal-prompting)

---

[**Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning**](https://arxiv.org/abs/2305.14909) Ôºà**2023.05.24**Ôºâ

<font color="gray">L. Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-43-red)

---

[**M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection**](https://arxiv.org/abs/2305.14902) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)  [![](https://img.shields.io/badge/Github%20Stars-65-blue)](https://github.com/mbzuai-nlp/semeval2024-task8)

---

[**PIVOINE: Instruction Tuning for Open-world Information Extraction**](https://arxiv.org/abs/2305.14898) Ôºà**2023.05.24**Ôºâ

<font color="gray">Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/lukeming-tsinghua/instruction-tuning-for-open-world-ie)

---

[**Text encoders are performance bottlenecks in contrastive vision-language models**](https://arxiv.org/abs/2305.14897) Ôºà**2023.05.24**Ôºâ

<font color="gray">Amita Kamath, Jack Hessel, Kai-Wei Chang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**HARD: Hard Augmentations for Robust Distillation**](https://arxiv.org/abs/2305.14890) Ôºà**2023.05.24**Ôºâ

<font color="gray">Arne F. Nix, Max F. Burg, Fabian H Sinz </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**Privacy Implications of Retrieval-Based Language Models**](https://arxiv.org/abs/2305.14888) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-34-blue)](https://github.com/princeton-sysml/knnlm_privacy)

---

[**Interpretable by Design Visual Question Answering**](https://arxiv.org/abs/2305.14882) Ôºà**2023.05.24**Ôºâ

<font color="gray">Xingyu Fu, Ben Zhou, Sihao Chen, Mark Yatskar, D. Roth </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**Leveraging GPT-4 for Automatic Translation Post-Editing**](https://arxiv.org/abs/2305.14878) Ôºà**2023.05.24**Ôºâ

<font color="gray">Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, Arul Menezes </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-26-red)

---

[**ClusterLLM: Large Language Models as a Guide for Text Clustering**](https://arxiv.org/abs/2305.14871) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yuwei Zhang, Zihan Wang, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-33-red)  [![](https://img.shields.io/badge/Github%20Stars-47-blue)](https://github.com/zhang-yu-wei/clusterllm)

---

[**CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering**](https://arxiv.org/abs/2305.14869) Ôºà**2023.05.24**Ôºâ

<font color="gray">Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-7-blue)](https://github.com/hkust-knowcomp/car)

---

[**Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers**](https://arxiv.org/abs/2305.14858) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, D. Pan </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-18-blue)](https://github.com/zixuanjiang/pre-rmsnorm-transformer)

---

[**SWAMP: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning**](https://arxiv.org/abs/2305.14852) Ôºà**2023.05.24**Ôºâ

<font color="gray">Moonseok Choi, Hyungi Lee, Giung Nam, Juho Lee </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Predicting Token Impact Towards Efficient Vision Transformer**](https://arxiv.org/abs/2305.14840) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hong Wang, Su Yang, Xiaoke Huang, Weishan Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation**](https://arxiv.org/abs/2305.14838) Ôºà**2023.05.24**Ôºâ

<font color="gray">Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Michael Zeng, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/nethermanpro/comsl)

---

[**NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario**](https://arxiv.org/abs/2305.14836) Ôºà**2023.05.24**Ôºâ

<font color="gray">Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)  [![](https://img.shields.io/badge/Github%20Stars-138-blue)](https://github.com/qiantianwen/nuscenes-qa)

---

[**Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation**](https://arxiv.org/abs/2305.14828) Ôºà**2023.05.24**Ôºâ

<font color="gray">Prashant Krishnan, Zilong Wang, Yangkun Wang, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification**](https://arxiv.org/abs/2305.14827) Ôºà**2023.05.24**Ôºâ

<font color="gray">Mujeen Sung, James Gung, Elman Mansimov, Nikolaos Pappas, Raphael Shu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/amazon-science/intent-aware-encoder)

---

[**Machine Reading Comprehension using Case-based Reasoning**](https://arxiv.org/abs/2305.14815) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dung Thai, Dhruv Agarwal, Mudit Chaudhary, R. Das, M. Zaheer, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)

---

[**Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification**](https://arxiv.org/abs/2305.14794) Ôºà**2023.05.24**Ôºâ

<font color="gray">Chengyu Dong, Zihan Wang, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Text Conditional Alt-Text Generation for Twitter Images**](https://arxiv.org/abs/2305.14779) Ôºà**2023.05.24**Ôºâ

<font color="gray">Nikita Srivatsan, Sofia Samaniego, Omar Florez, Taylor Berg-Kirkpatrick </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**A Controllable QA-based Framework for Decontextualization**](https://arxiv.org/abs/2305.14772) Ôºà**2023.05.24**Ôºâ

<font color="gray">Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, Kyle Lo </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models**](https://arxiv.org/abs/2305.14771) Ôºà**2023.05.24**Ôºâ

<font color="gray">Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Dual Path Transformer with Partition Attention**](https://arxiv.org/abs/2305.14768) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zhengkai Jiang, Liang Liu, Jiangning Zhang, Yabiao Wang, Mingang Chen, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning**](https://arxiv.org/abs/2305.14761) Ôºà**2023.05.24**Ôºâ

<font color="gray">Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, Shafiq Joty </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)  [![](https://img.shields.io/badge/Github%20Stars-53-blue)](https://github.com/vis-nlp/unichart)

---

[**SUVR: A Search-Based Approach to Unsupervised Visual Representation Learning**](https://doi.org/10.1109/ICASSP49357.2023.10096936) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yizhan Xu, Chih-Yao Chen, Cheng Li .  - „ÄêIEEE International Conference on Acoustics, Speech, and Signal Processing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation**](https://arxiv.org/abs/2305.14742) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dongxu Yue, Qin Guo, Munan Ning, Jiaxi Cui, Yuesheng Zhu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)

---

[**Trusting Your Evidence: Hallucinate Less with Context-aware Decoding**](https://arxiv.org/abs/2305.14739) Ôºà**2023.05.24**Ôºâ

<font color="gray">Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-52-red)

---

[**BinaryViT: Towards Efficient and Accurate Binary Vision Transformers**](https://arxiv.org/abs/2305.14730) Ôºà**2023.05.24**Ôºâ

<font color="gray">Junrui Xiao, Zhikai Li, Lianwei Yang, Qingyi Gu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-3.4k-blue)](https://github.com/microsoft/lmops)

---

[**GlobalBench: A Benchmark for Global Progress in Natural Language Processing**](https://arxiv.org/abs/2305.14716) Ôºà**2023.05.24**Ôºâ

<font color="gray">Y. Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)

---

[**The student becomes the master: Matching GPT3 on Scientific Factual Error Correction**](https://arxiv.org/abs/2305.14707) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dhananjay Ashok, Atharva Kulkarni, Hai Pham, Barnab'as P'oczos </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**PruMUX: Augmenting Data Multiplexing with Model Compression**](https://arxiv.org/abs/2305.14706) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yushan Su, Vishvak S. Murahari, Karthik Narasimhan, Kai Li </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/yushansu/prumux)

---

[**Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts**](https://arxiv.org/abs/2305.14705) Ôºà**2023.05.24**Ôºâ

<font color="gray">Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-90-red)

---

[**AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness**](https://arxiv.org/abs/2305.14700) Ôºà**2023.05.24**Ôºâ

<font color="gray">Ziuhi Wu, Haichang Gao, Bingqian Zhou, Ping Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank**](https://arxiv.org/abs/2305.14696) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**A Causal View of Entity Bias in (Large) Language Models**](https://arxiv.org/abs/2305.14695) Ôºà**2023.05.24**Ôºâ

<font color="gray">Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/luka-group/causal-view-of-entity-bias)

---

[**Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval**](https://arxiv.org/abs/2305.14685) Ôºà**2023.05.24**Ôºâ

<font color="gray">S. Yu, Chenghao Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/openmatch/fit5)

---

[**Emergent inabilities? Inverse scaling over the course of pretraining**](https://arxiv.org/abs/2305.14681) Ôºà**2023.05.24**Ôºâ

<font color="gray">James A. Michaelov, B. Bergen </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models**](https://arxiv.org/abs/2305.14677) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zhongjie Duan, Chengyu Wang, Cen Chen, Jun Huang, Weining Qian </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2.0k-blue)](https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler)

---

[**T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities**](https://arxiv.org/abs/2305.14674) Ôºà**2023.05.24**Ôºâ

<font color="gray">Kangfu Mei, Mo Zhou, Vishal M. Patel </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction**](https://arxiv.org/abs/2305.14659) Ôºà**2023.05.24**Ôºâ

<font color="gray">Ishani Mondal, Michelle Yuan, N Anandhavelu, Aparna Garimella, Francis Ferraro, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Dealing with Cross-Task Class Discrimination in Online Continual Learning**](https://arxiv.org/abs/2305.14657) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yiduo Guo, Bing Liu, Dongyan Zhao </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-26-red)  [![](https://img.shields.io/badge/Github%20Stars-7-blue)](https://github.com/gydpku/gsa)

---

[**Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion**](https://arxiv.org/abs/2305.14652) Ôºà**2023.05.24**Ôºâ

<font color="gray">Shaoxaing Wu, Damai Dai, Ziwei Qin, Tianyu Liu, Binghuai Lin, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/wsxrhfg/dbf)

---

[**A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting**](https://arxiv.org/abs/2305.14649) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yushu Chen, Shengzhuo Liu, Jinzhe Yang, Hao Jing, Wenlai Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/rationalspark/jtft)

---

[**Meta-review Generation with Checklist-guided Iterative Introspection**](https://arxiv.org/abs/2305.14647) Ôºà**2023.05.24**Ôºâ

<font color="gray">Qi Zeng, Mankeerat S. Sidhu, Hou Pong Chan, Lu Wang, Heng Ji </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation**](https://arxiv.org/abs/2305.14637) Ôºà**2023.05.24**Ôºâ

<font color="gray">Davit Soselia, Khalid Saifullah, Tianyi Zhou </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation**](https://arxiv.org/abs/2305.14635) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yan Zhou, Qingkai Fang, Yang Feng </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-14-blue)](https://github.com/ictnlp/cmot)

---

[**KNN-LM Does Not Improve Open-ended Text Generation**](https://arxiv.org/abs/2305.14625) Ôºà**2023.05.24**Ôºâ

<font color="gray">Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

<font color="gray">Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)

---

[**Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?**](https://arxiv.org/abs/2305.14578) Ôºà**2023.05.23**Ôºâ

<font color="gray">Margarita Bugueno, Gerard de Melo </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/buguemar/grtc_gnns)

---

[**Adversarial Defenses via Vector Quantization**](https://arxiv.org/abs/2305.13651) Ôºà**2023.05.23**Ôºâ

<font color="gray">Zhiyi Dong, Yongyi Mao </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Language Models with Rationality**](https://arxiv.org/abs/2305.14250) Ôºà**2023.05.23**Ôºâ

<font color="gray">Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Sch√ºtze, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)

---

[**A Trip Towards Fairness: Bias and De-Biasing in Large Language Models**](https://arxiv.org/abs/2305.13862) Ôºà**2023.05.23**Ôºâ

<font color="gray">Leonardo Ranaldi, Elena Sofia Ruzzetti, Davide Venditti, Dario Onorati, Fabio Massimo Zanzotto </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)

---

[**Question Answering as Programming for Solving Time-Sensitive Questions**](https://arxiv.org/abs/2305.14221) Ôºà**2023.05.23**Ôºâ

<font color="gray">Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/tianhongzxy/qaap)

---

[**PaD: Program-aided Distillation Specializes Large Models in Reasoning**](https://arxiv.org/abs/2305.13888) Ôºà**2023.05.23**Ôºâ

<font color="gray">Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen Zhou </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Aligning Large Language Models through Synthetic Feedback**](https://arxiv.org/abs/2305.13735) Ôºà**2023.05.23**Ôºâ

<font color="gray">Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-38-red)  [![](https://img.shields.io/badge/Github%20Stars-22-blue)](https://github.com/naver-ai/almost)

---

[**LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models**](https://arxiv.org/abs/2305.13718) Ôºà**2023.05.23**Ôºâ

<font color="gray">Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**](https://arxiv.org/abs/2305.14323) Ôºà**2023.05.23**Ôºâ

<font color="gray">Z. Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-27-red)  [![](https://img.shields.io/badge/Github%20Stars-41-blue)](https://github.com/rucaibox/chatcot)

---

[**ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer**](https://doi.org/10.48550/arXiv.2305.12708) Ôºà**2023.05.22**Ôºâ

<font color="gray">Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules**](https://arxiv.org/abs/2305.13406) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yanchen Liu, William Held, Diyi Yang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/salt-nlp/dada)

---

[**Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision**](https://arxiv.org/abs/2305.13199) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yucheng Cai, Hong Liu, Zhijian Ou, Y. Huang, Junlan Feng </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/thu-spmi/jsa-krtod)

---

[**Sentence Representations via Gaussian Embedding**](https://arxiv.org/abs/2305.12990) Ôºà**2023.05.22**Ôºâ

<font color="gray">Shohei Yoda, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space**](https://arxiv.org/abs/2305.12798) Ôºà**2023.05.22**Ôºâ

<font color="gray">Chi Han, Jialiang Xu, Manling Li, Y. Fung, Chenkai Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

---

[**MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space**](https://arxiv.org/abs/2305.12785) Ôºà**2023.05.22**Ôºâ

<font color="gray">Hanxing Ding, Liang Pang, Z. Wei, Huawei Shen, Xueqi Cheng, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/trustedllm/maclasa)

---

[**Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer**](https://arxiv.org/abs/2305.12761) Ôºà**2023.05.22**Ôºâ

<font color="gray">Shuang Li, Xuming Hu, Aiwei Liu, Yawen Yang, Fukun Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/thu-bpm/softmv)

---

[**A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches**](https://arxiv.org/abs/2305.12749) Ôºà**2023.05.22**Ôºâ

<font color="gray">Zihan Wang, Tianle Wang, Dheeraj Mekala, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/zihanwangki/x-tc)

---

[**Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data and Models**](https://arxiv.org/abs/2305.12620) Ôºà**2023.05.22**Ôºâ

<font color="gray">Ioana Baldini, Chhavi Yadav, Payel Das, K. Varshney </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis**](https://arxiv.org/abs/2305.13230) Ôºà**2023.05.22**Ôºâ

<font color="gray">Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-56-red)

---

[**Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance**](https://arxiv.org/abs/2305.13225) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)

---

[**InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT**](https://arxiv.org/abs/2305.13083) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yichong Xu, Ruochen Xu, Dan Iter, Yang Liu, Shuo Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Making Language Models Better Tool Learners with Execution Feedback**](https://arxiv.org/abs/2305.13068) Ôºà**2023.05.22**Ôºâ

<font color="gray">Shuofei Qiao, Honghao Gui, Huajun Chen, Ningyu Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-32-blue)](https://github.com/zjunlp/trice)

---

[**GPT-SW3: An Autoregressive Language Model for the Nordic Languages**](https://arxiv.org/abs/2305.12987) Ôºà**2023.05.22**Ôºâ

<font color="gray">Ariel Ekgren, Amaru Cuba Gyllensten, F. Stollenwerk, Joey Ohman, Tim Isbister, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination**](https://arxiv.org/abs/2305.12945) Ôºà**2023.05.22**Ôºâ

<font color="gray">Dongfang Li, Jindi Yu, Baotian Hu, Zhenran Xu, Min Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/hitsz-tmg/explaincpe)

---

[**Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model**](https://arxiv.org/abs/2305.12458) Ôºà**2023.05.21**Ôºâ

<font color="gray">Wenxin Tan </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Contrastive Learning with Logic-driven Data Augmentation for Logical Reasoning over Text**](https://arxiv.org/abs/2305.12599) Ôºà**2023.05.21**Ôºâ

<font color="gray">Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Neset Tan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Retrieving Texts based on Abstract Descriptions**](https://arxiv.org/abs/2305.12517) Ôºà**2023.05.21**Ôºâ

<font color="gray">Shauli Ravfogel, Valentina Pyatkin, Amir D. N. Cohen, Avshalom Manevich, Yoav Goldberg </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Pruning Pre-trained Language Models with Principled Importance and Self-regularization**](https://arxiv.org/abs/2305.12394) Ôºà**2023.05.21**Ôºâ

<font color="gray">Siyu Ren, Kenny Q. Zhu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/drsy/pins)

---

[**Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers**](https://arxiv.org/abs/2305.12567) Ôºà**2023.05.21**Ôºâ

<font color="gray">Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/gonglinyuan/metro_t0)

---

[**Movie101: A New Movie Understanding Benchmark**](https://arxiv.org/abs/2305.12140) Ôºà**2023.05.20**Ôºâ

<font color="gray">Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-50-blue)](https://github.com/yuezih/movie101)

---

[**The Scope of ChatGPT in Software Engineering: A Thorough Investigation**](https://arxiv.org/abs/2305.12138) Ôºà**2023.05.20**Ôºâ

<font color="gray">Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs**](https://arxiv.org/abs/2305.12191) Ôºà**2023.05.20**Ôºâ

<font color="gray">Yatin Nandwani, Vineet Kumar, Dinesh Raghu, Sachindra Joshi, L. Lastras </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/ynandwan/pmi-faith)

---

[**Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**](https://arxiv.org/abs/2305.12295) Ôºà**2023.05.20**Ôºâ

<font color="gray">Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-74-red)  [![](https://img.shields.io/badge/Github%20Stars-201-blue)](https://github.com/teacherpeterpan/logic-llm)

---

[**LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4**](https://arxiv.org/abs/2305.12147) Ôºà**2023.05.20**Ôºâ

<font color="gray">Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Late-Constraint Diffusion Guidance for Controllable Image Synthesis**](https://doi.org/10.48550/arXiv.2305.11520) Ôºà**2023.05.19**Ôºâ

<font color="gray">Chang Liu, Dong Liu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation**](https://doi.org/10.48550/arXiv.2305.11918) Ôºà**2023.05.19**Ôºâ

<font color="gray">Liuyi Wang, Chengju Liu, Zongtao He, Shu Li, Qingqing Yan, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-QA: Unsupervised Knowledge Guided Language Model Alignment**](https://arxiv.org/abs/2305.11952) Ôºà**2023.05.19**Ôºâ

<font color="gray">Xuanyu Zhang, Qing Yang </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-930-blue)](https://github.com/duxiaoman-di/xuanyuan)

---

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) Ôºà**2023.05.19**Ôºâ

<font color="gray">IokTong Lei, ZhiDong Deng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions**](https://doi.org/10.48550/arXiv.2305.11460) Ôºà**2023.05.19**Ôºâ

<font color="gray">Shiyao Ding, Takayuki Ito .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MaGIC: Multi-modality Guided Image Completion**](https://doi.org/10.48550/arXiv.2305.11818) Ôºà**2023.05.19**Ôºâ

<font color="gray">Yongsheng Yu, Hao Wang, Tiejian Luo, Hengrui Fan, Libo Zhang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization**](https://doi.org/10.48550/arXiv.2305.11718) Ôºà**2023.05.19**Ôºâ

<font color="gray">Mengqi Huang, Zhendong Mao, Zhuowei Chen, Yongdong Zhang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-121-blue)](https://github.com/crossmodalgroup/dynamicvectorquantization)

---

[**BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases**](https://arxiv.org/abs/2305.12018) Ôºà**2023.05.19**Ôºâ

<font color="gray">Xin Liu, Muhammad Khalifa, Lu Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-14-blue)](https://github.com/launchnlp/bolt)

---

[**STOAT: Structured Data to Analytical Text With Controls**](https://doi.org/10.48550/arXiv.2305.11826) Ôºà**2023.05.19**Ôºâ

<font color="gray">Deepanway Ghosal, Preksha Nema, A. Raghuveer .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Decouple knowledge from paramters for plug-and-play language modeling**](https://doi.org/10.48550/arXiv.2305.11564) Ôºà**2023.05.19**Ôºâ

<font color="gray">Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, Rui Yan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona**](https://doi.org/10.48550/arXiv.2305.11482) Ôºà**2023.05.19**Ôºâ

<font color="gray">Yihong Tang, Bo Wang, Miao Fang, Dongming Zhao, Kun Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/toyhom/clv)

---

[**XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters**](https://arxiv.org/abs/2305.12002) Ôºà**2023.05.19**Ôºâ

<font color="gray">Xuanyu Zhang, Qing Yang, Dongliang Xu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-23-red)  [![](https://img.shields.io/badge/Github%20Stars-930-blue)](https://github.com/duxiaoman-di/xuanyuan)

---

[**Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning**](https://doi.org/10.48550/arXiv.2305.11759) Ôºà**2023.05.19**Ôºâ

<font color="gray">Mustafa Safa Ozdayi, Charith S. Peris, Jack G. M. FitzGerald, Christophe Dupuy, Jimit Majmudar, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-24-blue)](https://github.com/amazon-science/controlling-llm-memorization)

---

[**RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought**](https://doi.org/10.48550/arXiv.2305.11499) Ôºà**2023.05.19**Ôºâ

<font color="gray">Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LLM Itself Can Read and Generate CXR Images**](https://doi.org/10.48550/arXiv.2305.11490) Ôºà**2023.05.19**Ôºâ

<font color="gray">Suhyeon Lee, Won Jun Kim, Jong-Chul Ye .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Post Hoc Explanations of Language Models Can Improve Language Models**](https://doi.org/10.48550/arXiv.2305.11426) Ôºà**2023.05.19**Ôºâ

<font color="gray">Satyapriya, Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models**](https://doi.org/10.48550/arXiv.2305.11414) Ôºà**2023.05.19**Ôºâ

<font color="gray">Sixing Yu, J. P. Mu√±oz, A. Jannesari .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning**](https://doi.org/10.48550/arXiv.2305.11383) Ôºà**2023.05.19**Ôºâ

<font color="gray">Po-Nien Kung, Nanyun Peng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**AutoTrial: Prompting Language Models for Clinical Trial Design**](https://doi.org/10.48550/arXiv.2305.11366) Ôºà**2023.05.19**Ôºâ

<font color="gray">Zifeng Wang, Cao Xiao, Jimeng Sun .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Democratized Diffusion Language Model**](https://doi.org/10.48550/arXiv.2305.10818) Ôºà**2023.05.18**Ôºâ

<font color="gray">Nikita Balagansky, Daniil Gavrilov .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Ahead-of-Time P-Tuning**](https://doi.org/10.48550/arXiv.2305.10835) Ôºà**2023.05.18**Ôºâ

<font color="gray">Daniil Gavrilov, Nikita Balagansky .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation**](https://doi.org/10.48550/arXiv.2305.10874) Ôºà**2023.05.18**Ôºâ

<font color="gray">Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**TextDiffuser: Diffusion Models as Text Painters**](https://doi.org/10.48550/arXiv.2305.10855) Ôºà**2023.05.18**Ôºâ

<font color="gray">Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LDM3D: Latent Diffusion Model for 3D**](https://doi.org/10.48550/arXiv.2305.10853) Ôºà**2023.05.18**Ôºâ

<font color="gray">Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4.3k-blue)](https://github.com/intel-isl/MiDaS)

---

[**Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling**](https://doi.org/10.48550/arXiv.2305.10769) Ôºà**2023.05.18**Ôºâ

<font color="gray">Shitong Shao, Xu Dai, Shouyi Yin, Lujun Li, Huanran Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/shaoshitong/Catch-Up-Distillation)

---

[**Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend**](https://doi.org/10.48550/arXiv.2305.10766) Ôºà**2023.05.18**Ôºâ

<font color="gray">Chong Yu, Tao Chen, Zhongxue Gan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Boost Vision Transformer with GPU-Friendly Sparsity and Quantization**](https://doi.org/10.48550/arXiv.2305.10727) Ôºà**2023.05.18**Ôºâ

<font color="gray">Chong Yu, Tao Chen, Zhongxue Gan, Jiayuan Fan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization**](https://doi.org/10.48550/arXiv.2305.10701) Ôºà**2023.05.18**Ôºâ

<font color="gray">Yihao Huang, Qing Guo, Felix Juefei-Xu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Tuned Contrastive Learning**](https://doi.org/10.48550/arXiv.2305.10675) Ôºà**2023.05.18**Ôºâ

<font color="gray">Chaitanya Animesh, Manmohan Chandraker .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Content-based Unrestricted Adversarial Attack**](https://doi.org/10.48550/arXiv.2305.10665) Ôºà**2023.05.18**Ôºâ

<font color="gray">Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation**](https://doi.org/10.48550/arXiv.2305.11130) Ôºà**2023.05.18**Ôºâ

<font color="gray">Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/934865517zjk/simoap)

---

[**How does the task complexity of masked pretraining objectives affect downstream performance?**](https://doi.org/10.48550/arXiv.2305.10992) Ôºà**2023.05.18**Ôºâ

<font color="gray">Atsuki Yamaguchi, Hiroaki Ozaki, Terufumi Morishita, Gaku Morio, Yasuhiro Sogawa .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/hitachi-nlp/mlm-probe-acl2023)

---

[**Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings**](https://doi.org/10.48550/arXiv.2305.10786) Ôºà**2023.05.18**Ôºâ

<font color="gray">Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Chong Deng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-97-blue)](https://github.com/alibaba-damo-academy/spokennlp)

---

[**ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval**](https://doi.org/10.48550/arXiv.2305.10703) Ôºà**2023.05.18**Ôºâ

<font color="gray">Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-18-blue)](https://github.com/yueyu1030/ReGen)

---

[**LIMA: Less Is More for Alignment**](https://doi.org/10.48550/arXiv.2305.11206) Ôºà**2023.05.18**Ôºâ

<font color="gray">Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-2.6k-blue)](https://github.com/alpha-vllm/llama2-accessory)

---

[**Efficient Prompting via Dynamic In-Context Learning**](https://doi.org/10.48550/arXiv.2305.11170) Ôºà**2023.05.18**Ôºâ

<font color="gray">Wangchunshu Zhou, Yuchen Jiang, Ryan Cotterell, Mrinmaya Sachan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities**](https://doi.org/10.48550/arXiv.2305.11000) Ôºà**2023.05.18**Ôºâ

<font color="gray">Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, P. Wang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/0nutation/speechgpt)

---

[**The Web Can Be Your Oyster for Improving Large Language Models**](https://doi.org/10.48550/arXiv.2305.10998) Ôºà**2023.05.18**Ôºâ

<font color="gray">Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, J. Nie, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/rucaibox/uniweb)

---

[**TOME: A Two-stage Approach for Model-based Retrieval**](https://doi.org/10.48550/arXiv.2305.11161) Ôºà**2023.05.18**Ôºâ

<font color="gray">Ruiyang Ren, Wayne Xin Zhao, J. Liu, Huaqin Wu, Ji-rong Wen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Inverted Non-maximum Suppression for more Accurate and Neater Face Detection**](https://doi.org/10.48550/arXiv.2305.10593) Ôºà**2023.05.17**Ôºâ

<font color="gray">Lian Liu, Liguo Zhou .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models**](https://doi.org/10.48550/arXiv.2305.10474) Ôºà**2023.05.17**Ôºâ

<font color="gray">Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks**](https://doi.org/10.48550/arXiv.2305.10329) Ôºà**2023.05.17**Ôºâ

<font color="gray">Anchun Gui, Jinqiang Ye, Han Xiao .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario**](https://doi.org/10.48550/arXiv.2305.10013) Ôºà**2023.05.17**Ôºâ

<font color="gray">Chengcheng Han, Liqing Cui, Renyu Zhu, J. Wang, Nuo Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression**](https://doi.org/10.48550/arXiv.2305.10010) Ôºà**2023.05.17**Ôºâ

<font color="gray">Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, Rui Wang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-9-blue)](https://github.com/brucewsy/ad-kd)

---

[**CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge**](https://doi.org/10.48550/arXiv.2305.09955) Ôºà**2023.05.17**Ôºâ

<font color="gray">Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SLiC-HF: Sequence Likelihood Calibration with Human Feedback**](https://doi.org/10.48550/arXiv.2305.10425) Ôºà**2023.05.17**Ôºâ

<font color="gray">Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**LeTI: Learning to Generate from Textual Interactions**](https://doi.org/10.48550/arXiv.2305.10314) Ôºà**2023.05.17**Ôºâ

<font color="gray">Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, Heng Ji .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-61-blue)](https://github.com/xingyaoww/leti)

---

[**M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models**](https://doi.org/10.48550/arXiv.2305.10263) Ôºà**2023.05.17**Ôºâ

<font color="gray">Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-90-blue)](https://github.com/tjunlp-lab/m3ke)

---

[**Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling**](https://doi.org/10.48550/arXiv.2305.09993) Ôºà**2023.05.17**Ôºâ

<font color="gray">Weijia Xu, Andrzej Banburski-Fahey, N. Jojic .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**On Dataset Transferability in Active Learning for Transformers**](https://doi.org/10.48550/arXiv.2305.09807) Ôºà**2023.05.16**Ôºâ

<font color="gray">Fran Jeleniƒá, Josip Jukic, Nina Drobac, Jan vSnajder .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages**](https://doi.org/10.48550/arXiv.2305.04160) Ôºà**2023.05.07**Ôºâ

<font color="gray">Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/0nutation/speechgpt)

---

[**Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision**](https://doi.org/10.48550/arXiv.2305.03047) Ôºà**2023.05.04**Ôºâ

<font color="gray">Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1.1k-blue)](https://github.com/IBM/Dromedary)

---

[**AutoML-GPT: Automatic Machine Learning with GPT**](https://doi.org/10.48550/arXiv.2305.02499) Ôºà**2023.05.04**Ôºâ

<font color="gray">Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mi Zhou .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**](https://doi.org/10.48550/arXiv.2305.02301) Ôºà**2023.05.03**Ôºâ

<font color="gray">Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-359-blue)](https://github.com/google-research/distilling-step-by-step)

---

[**Unlimiformer: Long-Range Transformers with Unlimited Length Input**](https://doi.org/10.48550/arXiv.2305.01625) Ôºà**2023.05.02**Ôºâ

<font color="gray">Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/abertsch72/unlimiformer)

---

[**Transfer Visual Prompt Generator across LLMs**](https://doi.org/10.48550/arXiv.2305.01278) Ôºà**2023.05.02**Ôºâ

<font color="gray">Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback**](https://arxiv.org/abs/2304.10750) Ôºà**2023.04.21**Ôºâ

<font color="gray">Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xinwei Deng, A. Awadallah, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)

---

[**Segment Anything Model for Medical Image Analysis: an Experimental Study**](https://arxiv.org/abs/2304.10517) Ôºà**2023.04.20**Ôºâ

<font color="gray">Maciej A. Mazurowski, Haoyu Dong, Han Gu, Jichen Yang, N. Konz, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-178-red)  [![](https://img.shields.io/badge/Github%20Stars-134-blue)](https://github.com/mazurowski-lab/segment-anything-medical-evaluation)

---

[**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**](https://arxiv.org/abs/2304.09842) Ôºà**2023.04.19**Ôºâ

<font color="gray">Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-174-red)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/lupantech/chameleon-llm)

---

[**Accuracy of Segment-Anything Model (SAM) in medical image segmentation tasks**](https://arxiv.org/abs/2304.09324) Ôºà**2023.04.18**Ôºâ

<font color="gray">Sheng He, Rina Bao, Jingpeng Li, P. Grant, Yangming Ou </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-54-red)

---

[**When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation**](https://arxiv.org/abs/2304.08506) Ôºà**2023.04.17**Ôºâ

<font color="gray">Chuanfei Hu, Xinde Li </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)

---

[**The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning**](https://arxiv.org/abs/2304.07875) Ôºà**2023.04.16**Ôºâ

<font color="gray">F. Putz, Johanna Grigo, T. Weissmann, P. Schubert, D. Hoefler, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Deep learning universal crater detection using Segment Anything Model (SAM)**](https://doi.org/10.48550/arXiv.2304.07764) Ôºà**2023.04.16**Ôºâ

<font color="gray">I. Giannakis, A. Bhardwaj, L. Sam, G. Leontidis .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging**](https://doi.org/10.48550/arXiv.2304.04155) Ôºà**2023.04.09**Ôºâ

<font color="gray">Ruining Deng, C. Cui, Quan Liu, Tianyuan Yao, L. W. Remedios, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)

---

[**TagGPT: Large Language Models are Zero-shot Multimodal Taggers**](https://arxiv.org/abs/2304.03022) Ôºà**2023.04.06**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)  [![](https://img.shields.io/badge/Github%20Stars-53-blue)](https://github.com/tencentarc/taggpt)

---

[**Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling**](https://arxiv.org/abs/2304.01373) Ôºà**2023.04.03**Ôºâ

<font color="gray">Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, etc </font>

![](https://img.shields.io/badge/Citations-3-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-229-red)  [![](https://img.shields.io/badge/Github%20Stars-6.7k-blue)](https://github.com/eleutherai/gpt-neox)

---

[**BloombergGPT: A Large Language Model for Finance**](https://arxiv.org/abs/2303.17564) Ôºà**2023.03.30**Ôºâ

<font color="gray">Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-555-red)

---

[**Scaling Expert Language Models with Unsupervised Domain Discovery**](https://arxiv.org/abs/2303.14177) Ôºà**2023.03.24**Ôºâ

<font color="gray">Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-37-red)  [![](https://img.shields.io/badge/Github%20Stars-105-blue)](https://github.com/kernelmachine/cbtm)

---

[**Sparks of Artificial General Intelligence: Early experiments with GPT-4**](https://arxiv.org/abs/2303.12712) Ôºà**2023.03.22**Ôºâ

<font color="gray">S'ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, etc </font>

![](https://img.shields.io/badge/Citations-5-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1.5k-red)  [![](https://img.shields.io/badge/Github%20Stars-18.1k-blue)](https://github.com/microsoft/guidance)

---

[**CoLT5: Faster Long-Range Transformers with Conditional Computation**](https://doi.org/10.48550/arXiv.2303.09752) Ôºà**2023.03.17**Ôºâ

<font color="gray">J. Ainslie, Tao Lei, Michiel de Jong, Santiago Ontan'on, Siddhartha Brahma, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Meet in the Middle: A New Pre-training Paradigm**](https://doi.org/10.48550/arXiv.2303.07295) Ôºà**2023.03.13**Ôºâ

<font color="gray">A. Nguyen, Nikos Karampatziakis, Weizhu Chen .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**High-throughput Generative Inference of Large Language Models with a Single GPU**](https://doi.org/10.48550/arXiv.2303.06865) Ôºà**2023.03.13**Ôºâ

<font color="gray">Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement**](https://doi.org/10.48550/arXiv.2303.06705) Ôºà**2023.03.12**Ôºâ

<font color="gray">Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, R. Timofte, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-640-blue)](https://github.com/caiyuanhao1998/retinexformer)

---

[**Stabilizing Transformer Training by Preventing Attention Entropy Collapse**](https://doi.org/10.48550/arXiv.2303.06296) Ôºà**2023.03.11**Ôºâ

<font color="gray">Shuangfei Zhai, T. Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-278-blue)](https://github.com/apple/ml-sigma-reparam)

---

[**An Overview on Language Models: Recent Developments and Outlook**](https://doi.org/10.48550/arXiv.2303.05759) Ôºà**2023.03.10**Ôºâ

<font color="gray">Chen Wei, Yun Cheng Wang, Bin Wang, C.-C. Jay Kuo .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Foundation Models for Decision Making: Problems, Methods, and Opportunities**](https://doi.org/10.48550/arXiv.2303.04129) Ôºà**2023.03.07**Ôºâ

<font color="gray">Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, P. Abbeel, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding**](https://doi.org/10.48550/arXiv.2303.04245) Ôºà**2023.03.07**Ôºâ

<font color="gray">Yuchen Li, Yuan-Fang Li, Andrej Risteski .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/yuchenli01/transformer_topic_model_lda)

---

[**LLaMA: Open and Efficient Foundation Language Models**](https://doi.org/10.48550/arXiv.2302.13971) Ôºà**2023.02.27**Ôºâ

<font color="gray">Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-12-green)  [![](https://img.shields.io/badge/Github%20Stars-54.2k-blue)](https://github.com/facebookresearch/llama)

---

[**Complex QA and language models hybrid architectures, Survey**](https://doi.org/10.48550/arXiv.2302.09051) Ôºà**2023.02.17**Ôºâ

<font color="gray">Xavier Daull, P. Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**OVO: One-shot Vision Transformer Search with Online distillation**](https://doi.org/10.48550/arXiv.2212.13766) Ôºà**2022.12.28**Ôºâ

<font color="gray">Zimian Wei, H. Pan, Xin-Yi Niu, Dongsheng Li .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-Instruct: Aligning Language Model with Self Generated Instructions**](https://doi.org/10.48550/arXiv.2212.10560) Ôºà**2022.12.20**Ôºâ

<font color="gray">Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)

---

[**Solving Math Word Problem via Cooperative Reasoning induced Language Models**](https://doi.org/10.48550/arXiv.2210.16257) Ôºà**2022.10.28**Ôºâ

<font color="gray">Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback**](https://doi.org/10.48550/arXiv.2204.05862) Ôºà**2022.04.12**Ôºâ

<font color="gray">Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-61-green)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/anthropics/hh-rlhf)

---

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2204.02311) Ôºà**2022.04.05**Ôºâ

<font color="gray">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-624-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1.4k-red)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/lucidrains/CoCa-pytorch)

---

[**Training language models to follow instructions with human feedback**](https://doi.org/10.48550/arXiv.2203.02155) Ôºà**2022.03.04**Ôºâ

<font color="gray">Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-444-green)  [![](https://img.shields.io/badge/Github%20Stars-1.2k-blue)](https://github.com/openai/following-instructions-human-feedback)

---

[**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685) Ôºà**2021.06.17**Ôºâ

<font color="gray">Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, etc .  - „ÄêInternational Conference on Learning Representations„Äë</font>

![](https://img.shields.io/badge/Citations-244-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2.5k-red)  [![](https://img.shields.io/badge/Github%20Stars-9.7k-blue)](https://github.com/microsoft/LoRA)

---

[**Transformers in Vision: A Survey**](https://doi.org/10.1145/3505244) Ôºà**2021.01.04**Ôºâ

<font color="gray">Salman Hameed Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, F. Khan, etc .  - „ÄêACM Computing Surveys„Äë</font>

![](https://img.shields.io/badge/Citations-1692-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2.6k-red)

---

[**Unsupervised embedding of trajectories captures the latent structure of mobility**](https://doi.org/10.21203/rs.3.rs-1062258/v1) Ôºà**2020.12.04**Ôºâ

<font color="gray">Dakota S. Murray, Jisung Yoon, Sadamori Kojaku, R. Costas, Woo-Sung Jung, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)

---

[**Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model**](https://doi.org/10.18653/v1/P19-1102) Ôºà**2019.06.04**Ôºâ

<font color="gray">Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R. Radev .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-276-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-334-red)  [![](https://img.shields.io/badge/Github%20Stars-264-blue)](https://github.com/Alex-Fabbri/Multi-News)

---

[**Social Boundaries of Appropriate Speech in HCI: A Politeness Perspective**](https://doi.org/10.14236/EWIC/HCI2018.76) Ôºà**2018.07.01**Ôºâ

<font color="gray">L. Clark </font>

![](https://img.shields.io/badge/Citations-8-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)

---

[**Steering the conversation: A linguistic exploration of natural language interactions with a digital assistant during simulated driving.**](https://doi.org/10.1016/j.apergo.2017.04.003) Ôºà**2017.09.01**Ôºâ

<font color="gray">D. Large, L. Clark, Annie Quandt, G. Burnett, L. Skrypchuk .  - „ÄêApplied Ergonomics„Äë</font>

![](https://img.shields.io/badge/Citations-55-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-170-red)

---

[**Teaching Machines to Read and Comprehend**](https://arxiv.org/abs/1506.03340) Ôºà**2015.06.10**Ôºâ

<font color="gray">K. Hermann, Tom√°s Kocisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, etc .  - „ÄêNIPS„Äë</font>

![](https://img.shields.io/badge/Citations-2795-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2.3k-red)  [![](https://img.shields.io/badge/Github%20Stars-1.3k-blue)](https://github.com/deepmind/rc-data)

---

[**From discourse structures to text summaries**](https://api.semanticscholar.org/1daf375141571501ca8c30b62d7c14269d566762) 

<font color="gray">D. Marcu .  - „ÄêWorkshop On Intelligent Scalable Text Summarization„Äë</font>

![](https://img.shields.io/badge/Citations-330-green)

---

[**Language Models are Unsupervised Multitask Learners**](https://api.semanticscholar.org/9405cc0d6169988371b2755e573cc28650d14dfe) 

<font color="gray">Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, etc </font>

![](https://img.shields.io/badge/Citations-8935-green)  [![](https://img.shields.io/badge/Github%20Stars-21.9k-blue)](https://github.com/openai/gpt-2)

---

[**SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization**](https://api.semanticscholar.org/5c6a17850c9ad6bf6dc8992ec598cd932ce42208) 

<font color="gray">Mingshu Zhai, Jiaao He, Zixuan Ma, Zan Zong, Runqing Zhang, etc .  - „ÄêUSENIX Annual Technical Conference„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**LightVLP: A Lightweight Vision-Language Pre-training via Gated Interactive Masked AutoEncoders**](https://api.semanticscholar.org/a93847e538ca8a58ed8c2cd60d377153ce0b2a9a) 

<font color="gray">Xingwu Sun, Zhen Yang, Ruobing Xie, Fengzong Lian, Zhanhui Kang, etc .  - „ÄêInternational Conference on Language Resources and Evaluation„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MetaCheckGPT - A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models**](https://doi.org/10.48550/arXiv.2404.06948) 

<font color="gray">Rahul Mehta, Andrew Hoblitzell, Jack O‚Äôkeefe, Hyeju Jang, Vasudeva Varma .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Entropy-Regularized Token-Level Policy Optimization for Large Language Models**](https://doi.org/10.48550/arXiv.2402.06700) 

<font color="gray">Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang, Ying Wen .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ**](https://doi.org/10.48550/arXiv.2402.04902) 

<font color="gray">Hyesung Jeon, Yulhwa Kim, Jae-Joon Kim .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**PointNAT: Large-Scale Point Cloud Semantic Segmentation via Neighbor Aggregation With Transformer**](https://doi.org/10.1109/TGRS.2024.3407761) 

<font color="gray">Ziyin Zeng, Huan Qiu, Jian Zhou, Z. Dong, Jinsheng Xiao, etc .  - „ÄêIEEE Transactions on Geoscience and Remote Sensing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)


</div>

# CONTINUE...