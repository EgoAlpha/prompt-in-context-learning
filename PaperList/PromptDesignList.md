# üìÑ Prompt Design

## Paper List

<div style="line-height:0.2em;">


[**LLaRA: Supercharging Robot Learning Data for Vision-Language Policy**](https://arxiv.org/abs/2406.20095) Ôºà**2024.06.28**Ôºâ

<font color="gray">Xiang Li, Cristina Mata, Jong Sung Park, Kumara Kahatapitiya, Yoo Sung Jang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-77-blue)](https://github.com/lostxine/llara)

---

[**Dataset Size Recovery from LoRA Weights**](https://arxiv.org/abs/2406.19395) Ôºà**2024.06.27**Ôºâ

<font color="gray">Mohammad Salama, Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Dual-Phase Accelerated Prompt Optimization**](https://arxiv.org/abs/2406.13443) Ôºà**2024.06.19**Ôºâ

<font color="gray">Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries**](https://arxiv.org/abs/2406.12824) Ôºà**2024.06.18**Ôºâ

<font color="gray">Hitesh Wadhwa, Rahul Seetharaman, Somyaa Aggarwal, Reshmi Ghosh, Samyadeep Basu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**VoCo-LLaMA: Towards Vision Compression with Large Language Models**](https://arxiv.org/abs/2406.12275) Ôºà**2024.06.18**Ôºâ

<font color="gray">Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-58-blue)](https://github.com/Yxxxb/VoCo-LLaMA)

---

[**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**](https://arxiv.org/abs/2406.12832) Ôºà**2024.06.18**Ôºâ

<font color="gray">Seyedarmin Azizi, Souvik Kundu, M. Pedram </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**The Impact of Initialization on LoRA Finetuning Dynamics**](https://arxiv.org/abs/2406.08447) Ôºà**2024.06.12**Ôºâ

<font color="gray">Soufiane Hayou, Nikhil Ghosh, Bin Yu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models**](https://arxiv.org/abs/2406.05130) Ôºà**2024.06.07**Ôºâ

<font color="gray">Xiongtao Zhou, Jie He, Yuhua Ke, Guangyao Zhu, V'ictor Guti'errez-Basulto, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Cross-Context Backdoor Attacks against Graph Prompt Learning**](https://doi.org/10.48550/arXiv.2405.17984) Ôºà**2024.05.28**Ôºâ

<font color="gray">Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Yuan 2.0-M32: Mixture of Experts with Attention Router**](https://doi.org/10.48550/arXiv.2405.17976) Ôºà**2024.05.28**Ôºâ

<font color="gray">Shaohua Wu, Jiangang Luo, Xi Chen, Lingjun Li, Xudong Zhao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-160-blue)](https://github.com/ieit-yuan/yuan2.0-m32)

---

[**Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion**](https://doi.org/10.48550/arXiv.2405.11464) Ôºà**2024.05.19**Ôºâ

<font color="gray">Pengxiang Lan, Enneng Yang, Yuting Liu, Guibing Guo, Linying Jiang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Unsupervised Image Prior via Prompt Learning and CLIP Semantic Guidance for Low-Light Image Enhancement**](https://doi.org/10.48550/arXiv.2405.11478) Ôºà**2024.05.19**Ôºâ

<font color="gray">Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers**](https://doi.org/10.48550/arXiv.2405.10612) Ôºà**2024.05.17**Ôºâ

<font color="gray">Shengyuan Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-9-blue)](https://github.com/20000yshust/swarm)

---

[**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**](https://doi.org/10.48550/arXiv.2405.10276) Ôºà**2024.05.16**Ôºâ

<font color="gray">Tuo Zhang, Jinyue Yuan, A. Avestimehr .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification**](https://doi.org/10.48550/arXiv.2405.06468) Ôºà**2024.05.10**Ôºâ

<font color="gray">Yaoqin Ye, Junjie Zhang, Hongwei Shi .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning**](https://doi.org/10.48550/arXiv.2405.05615) Ôºà**2024.05.09**Ôºâ

<font color="gray">Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-38-blue)](https://github.com/jieshibo/memvp)

---

[**On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?**](https://doi.org/10.48550/arXiv.2405.02266) Ôºà**2024.05.03**Ôºâ

<font color="gray">Maxime Zanella, Ismail Ben Ayed .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-25-blue)](https://github.com/maxzanella/mta)

---

[**AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts**](https://doi.org/10.48550/arXiv.2405.00361) Ôºà**2024.05.01**Ôºâ

<font color="gray">Zefang Liu, Jiahua Luo .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/zefang-liu/adamole)

---

[**Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2405.00402) Ôºà**2024.05.01**Ôºâ

<font color="gray">Leonardo Ranaldi, Andr√© Freitas .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization**](https://doi.org/10.48550/arXiv.2405.00657) Ôºà**2024.05.01**Ôºâ

<font color="gray">Dongqi Pu, Vera Demberg .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Training-Free Unsupervised Prompt for Vision-Language Models**](https://doi.org/10.48550/arXiv.2404.16339) Ôºà**2024.04.25**Ôºâ

<font color="gray">Sifan Long, Linbin Wang, Zhen Zhao, Zichang Tan, Yiming Wu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/wlb12345/tfup)

---

[**AAPL: Adding Attributes to Prompt Learning for Vision-Language Models**](https://doi.org/10.48550/arXiv.2404.16804) Ôºà**2024.04.25**Ôºâ

<font color="gray">Gahyeon Kim, Sohee Kim, Seokju Lee .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-26-blue)](https://github.com/Gahyeonkim09/AAPL)

---

[**V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning**](https://doi.org/10.48550/arXiv.2404.12353) Ôºà**2024.04.18**Ôºâ

<font color="gray">Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**LLMTune: Accelerate Database Knob Tuning with Large Language Models**](https://doi.org/10.48550/arXiv.2404.11581) Ôºà**2024.04.17**Ôºâ

<font color="gray">Xinmei Huang, Haoyang Li, Jing Zhang, Xinxin Zhao, Zhiming Yao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-25.7k-blue)](https://github.com/hiyouga/llama-factory)

---

[**DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model**](https://arxiv.org/abs/2404.05182) Ôºà**2024.04.08**Ôºâ

<font color="gray">Chao Gao, Sai Qian Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation**](https://doi.org/10.48550/arXiv.2403.18778) Ôºà**2024.03.27**Ôºâ

<font color="gray">Ehsan Latif .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts**](https://arxiv.org/abs/2403.13258) Ôºà**2024.03.20**Ôºâ

<font color="gray">Xian Lin, Yangyang Xiang, Zhehao Wang, Kwang-Ting Cheng, Zengqiang Yan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/xianlin7/samct)

---

[**AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models**](https://arxiv.org/abs/2403.13269) Ôºà**2024.03.20**Ôºâ

<font color="gray">Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt**](https://arxiv.org/abs/2403.09857) Ôºà**2024.03.14**Ôºâ

<font color="gray">Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Unveiling the Generalization Power of Fine-Tuned Large Language Models**](https://arxiv.org/abs/2403.09162) Ôºà**2024.03.14**Ôºâ

<font color="gray">Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, P. Heng, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/lhryang/generalization_of_ft-llm)

---

[**Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling**](https://arxiv.org/abs/2403.06978) Ôºà**2024.03.11**Ôºâ

<font color="gray">W. G. C. Bandara, Vishal M. Patel </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/wgcban/apt)

---

[**VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models**](https://arxiv.org/abs/2403.06098) Ôºà**2024.03.10**Ôºâ

<font color="gray">Wenhao Wang, Yi Yang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-83-blue)](https://github.com/wangwenhao0716/vidprom)

---

[**Localized Zeroth-Order Prompt Optimization**](https://arxiv.org/abs/2403.02993) Ôºà**2024.03.05**Ôºâ

<font color="gray">Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models**](https://arxiv.org/abs/2403.02271) Ôºà**2024.03.04**Ôºâ

<font color="gray">Saeed Najafi, Alona Fyshe </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization**](https://arxiv.org/abs/2402.18447) Ôºà**2024.02.28**Ôºâ

<font color="gray">Deng Li, Aming Wu, Yaowei Wang, Yahong Han </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**Meta-Task Prompting Elicits Embedding from Large Language Models**](https://arxiv.org/abs/2402.18458) Ôºà**2024.02.28**Ôºâ

<font color="gray">Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning**](https://doi.org/10.48550/arXiv.2402.16829) Ôºà**2024.02.26**Ôºâ

<font color="gray">Aivin V. Solatorio .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-27-blue)](https://github.com/avsolatorio/gistembed)

---

[**OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)**](https://doi.org/10.48550/arXiv.2402.16810) Ôºà**2024.02.26**Ôºâ

<font color="gray">Fujian Jia, Xin Liu, Lixi Deng, Jiwen Gu, Chunchao Pu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Stepwise Self-Consistent Mathematical Reasoning with Large Language Models**](https://doi.org/10.48550/arXiv.2402.17786) Ôºà**2024.02.24**Ôºâ

<font color="gray">Zilong Zhao, Yao Rong, Dongyang Guo, Emek G√∂zl√ºkl√º, Emir G√ºlboy, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/zhao-zilong/ssc-cot)

---

[**Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning**](https://doi.org/10.48550/arXiv.2402.14883) Ôºà**2024.02.22**Ôºâ

<font color="gray">Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs**](https://doi.org/10.48550/arXiv.2402.14872) Ôºà**2024.02.21**Ôºâ

<font color="gray">Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Hansheng Fang, Aishan Liu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding**](https://doi.org/10.48550/arXiv.2402.11889) Ôºà**2024.02.19**Ôºâ

<font color="gray">Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models**](https://doi.org/10.48550/arXiv.2402.11417) Ôºà**2024.02.18**Ôºâ

<font color="gray">Yifan Yang, Jiajun Zhou, Ngai Wong, Zheng Zhang .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-16-blue)](https://github.com/yifanycc/loretta)

---

[**MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning**](https://doi.org/10.48550/arXiv.2402.11260) Ôºà**2024.02.17**Ôºâ

<font color="gray">Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-13-green)

---

[**X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design**](https://doi.org/10.48550/arXiv.2402.07148) Ôºà**2024.02.11**Ôºâ

<font color="gray">E. Buehler, M. J. Buehler .  - „ÄêAPL Machine Learning„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Self-Discover: Large Language Models Self-Compose Reasoning Structures**](https://doi.org/10.48550/arXiv.2402.03620) Ôºà**2024.02.06**Ôºâ

<font color="gray">Pei Zhou, J. Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)  [![](https://img.shields.io/badge/Github%20Stars-244-blue)](https://github.com/catid/self-discover)

---

[**Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding**](https://doi.org/10.48550/arXiv.2401.12954) Ôºà**2024.01.23**Ôºâ

<font color="gray">Mirac Suzgun, A. Kalai .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-8-green)  [![](https://img.shields.io/badge/Github%20Stars-257-blue)](https://github.com/suzgunmirac/meta-prompting)

---

[**Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding**](https://doi.org/10.48550/arXiv.2401.04398) Ôºà**2024.01.09**Ôºâ

<font color="gray">Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves**](https://doi.org/10.48550/arXiv.2311.04205) Ôºà**2023.11.07**Ôºâ

<font color="gray">Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-19-green)  [![](https://img.shields.io/badge/Github%20Stars-87-blue)](https://github.com/uclaml/Rephrase-and-Respond)

---

[**Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models**](https://doi.org/10.48550/arXiv.2310.06117) Ôºà**2023.10.09**Ôºâ

<font color="gray">Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, E. Chi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)

---

[**Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading**](https://doi.org/10.48550/arXiv.2310.05029) Ôºà**2023.10.08**Ôºâ

<font color="gray">Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Chain-of-Verification Reduces Hallucination in Large Language Models**](https://doi.org/10.48550/arXiv.2309.11495) Ôºà**2023.09.20**Ôºâ

<font color="gray">S. Dhuliawala, M. Komeili, Jing Xu, Roberta Raileanu, Xian Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-898-blue)](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)

---

[**Giraffe: Adventures in Expanding Context Lengths in LLMs**](https://doi.org/10.48550/arXiv.2308.10882) Ôºà**2023.08.21**Ôºâ

<font color="gray">Arka Pal, Deep Karkhanis, Manley Roberts, S. Dooley, Arvind Sundararajan, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-567-blue)](https://github.com/abacusai/long-context)

---

[**Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering**](https://doi.org/10.48550/arXiv.2308.07411) Ôºà**2023.08.14**Ôºâ

<font color="gray">Edward Junprung .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Cumulative Reasoning with Large Language Models**](https://doi.org/10.48550/arXiv.2308.04371) Ôºà**2023.08.08**Ôºâ

<font color="gray">Yifan Zhang, Jingqin Yang, Yang Yuan, A. Yao .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-265-blue)](https://github.com/iiis-ai/cumulative-reasoning)

---

[**Scaling Relationship on Learning Mathematical Reasoning with Large Language Models**](https://doi.org/10.48550/arXiv.2308.01825) Ôºà**2023.08.03**Ôºâ

<font color="gray">Zheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong, Chuanqi Tan, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-11-green)  [![](https://img.shields.io/badge/Github%20Stars-187-blue)](https://github.com/ofa-sys/gsm8k-screl)

---

[**Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding**](https://doi.org/10.48550/arXiv.2307.15337) Ôºà**2023.07.28**Ôºâ

<font color="gray">Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-24-green)

---

[**ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning**](https://doi.org/10.48550/arXiv.2307.09474) Ôºà**2023.07.18**Ôºâ

<font color="gray">Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Hao-Ran Wei, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration**](https://arxiv.org/abs/2307.05300) Ôºà**2023.07.11**Ôºâ

<font color="gray">Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, etc </font>

![](https://img.shields.io/badge/Citations-39-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-66-red)  [![](https://img.shields.io/badge/Github%20Stars-294-blue)](https://github.com/mikewangwzhl/solo-performance-prompting)

---

[**Self-consistency for open-ended generations**](https://arxiv.org/abs/2307.06857) Ôºà**2023.07.11**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models**](https://doi.org/10.48550/arXiv.2307.03762) Ôºà**2023.07.07**Ôºâ

<font color="gray">Yuxi Ma, Chi Zhang, Song-Chun Zhu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Conformer LLMs - Convolution Augmented Large Language Models**](https://doi.org/10.48550/arXiv.2307.00461) Ôºà**2023.07.02**Ôºâ

<font color="gray">Prateek Verma .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Learning Multi-Step Reasoning by Solving Arithmetic Tasks**](https://doi.org/10.48550/arXiv.2306.01707) Ôºà**2023.06.02**Ôºâ

<font color="gray">Tianduo Wang, Wei Lu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models**](https://doi.org/10.18653/v1/2023.findings-emnlp.205) Ôºà**2023.05.24**Ôºâ

<font color="gray">Siqi Ouyang, Lei Li .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/owaski/autoplan)

---

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**In-Context Impersonation Reveals Large Language Models' Strengths and Biases**](https://arxiv.org/abs/2305.14930) Ôºà**2023.05.24**Ôºâ

<font color="gray">Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-47-red)  [![](https://img.shields.io/badge/Github%20Stars-16-blue)](https://github.com/ExplainableML/in-context-impersonation)

---

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

<font color="gray">Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/bsantraigi/frugal-prompting)

---

[**Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering**](https://arxiv.org/abs/2305.14901) Ôºà**2023.05.24**Ôºâ

<font color="gray">Wang Zhu, Jesse Thomason, Robin Jia </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis**](https://arxiv.org/abs/2305.14877) Ôºà**2023.05.24**Ôºâ

<font color="gray">Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-9-blue)](https://github.com/soheeyang/unified-prompt-selection)

---

[**MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions**](https://arxiv.org/abs/2305.14795) Ôºà**2023.05.24**Ôºâ

<font color="gray">Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, Danqi Chen </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-29-red)  [![](https://img.shields.io/badge/Github%20Stars-84-blue)](https://github.com/princeton-nlp/mquake)

---

[**BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver**](https://arxiv.org/abs/2305.14766) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hao Sun, Xiao Liu, Yeyun Gong, Yan Zhang, Nan Duan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering**](https://arxiv.org/abs/2305.14682) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jian Wu, Yicheng Xu, Yan Gao, Jian-Guang Lou, B√∂rje F. Karlsson, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Mixture of Prompt Experts for Generalizable and Interpretable Question Answering**](https://arxiv.org/abs/2305.14628) Ôºà**2023.05.24**Ôºâ

<font color="gray">Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan L. Boyd-Graber </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources**](https://arxiv.org/abs/2305.13269) Ôºà**2023.05.22**Ôºâ

<font color="gray">Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq R. Joty, etc </font>

![](https://img.shields.io/badge/Citations-7-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-38-red)  [![](https://img.shields.io/badge/Github%20Stars-41-blue)](https://github.com/damo-nlp-sg/chain-of-knowledge)

---

[**Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**](https://arxiv.org/abs/2305.12295) Ôºà**2023.05.20**Ôºâ

<font color="gray">Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-74-red)  [![](https://img.shields.io/badge/Github%20Stars-201-blue)](https://github.com/teacherpeterpan/logic-llm)

---

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) Ôºà**2023.05.19**Ôºâ

<font color="gray">IokTong Lei, ZhiDong Deng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs**](https://doi.org/10.48550/arXiv.2305.11334) Ôºà**2023.05.18**Ôºâ

<font color="gray">Giorgi Kokaia, Pratyush Sinha, Yutong Jiang, N. Boujemaa .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Efficient Prompting via Dynamic In-Context Learning**](https://doi.org/10.48550/arXiv.2305.11170) Ôºà**2023.05.18**Ôºâ

<font color="gray">Wangchunshu Zhou, Yuchen Jiang, Ryan Cotterell, Mrinmaya Sachan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**](https://doi.org/10.48550/arXiv.2305.02301) Ôºà**2023.05.03**Ôºâ

<font color="gray">Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, etc .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-59-green)

---

[**WizardLM: Empowering Large Language Models to Follow Complex Instructions**](https://arxiv.org/abs/2304.12244) Ôºà**2023.04.24**Ôºâ

<font color="gray">Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-240-red)  [![](https://img.shields.io/badge/Github%20Stars-9.1k-blue)](https://github.com/nlpxucan/wizardlm)

---

[**LLM+P: Empowering Large Language Models with Optimal Planning Proficiency**](https://arxiv.org/abs/2304.11477) Ôºà**2023.04.22**Ôºâ

<font color="gray">B. Liu, Yuqian Jiang, Xiaohan Zhang, Qian Liu, Shiqi Zhang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-132-red)  [![](https://img.shields.io/badge/Github%20Stars-340-blue)](https://github.com/Cranial-XIX/llm-pddl)

---

[**Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts**](https://doi.org/10.1145/3544548.3581388) Ôºà**2023.04.19**Ôºâ

<font color="gray">J. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, Qiang Yang .  - „ÄêInternational Conference on Human Factors in Computing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-24-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-202-red)

---

[**Progressive-Hint Prompting Improves Reasoning in Large Language Models**](https://arxiv.org/abs/2304.09797) Ôºà**2023.04.19**Ôºâ

<font color="gray">Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-75-red)  [![](https://img.shields.io/badge/Github%20Stars-193-blue)](https://github.com/chuanyang-Zheng/Progressive-Hint)

---

[**Boosted Prompt Ensembles for Large Language Models**](https://doi.org/10.48550/arXiv.2304.05970) Ôºà**2023.04.12**Ôºâ

<font color="gray">Silviu Pitis, Michael Ruogu Zhang, Andrew Wang, Jimmy Ba .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-28-blue)](https://github.com/awwang10/llmpromptboosting)

---

[**Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition**](https://arxiv.org/abs/2304.04704) Ôºà**2023.04.10**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-58-red)  [![](https://img.shields.io/badge/Github%20Stars-249-blue)](https://github.com/amazon-science/prompt-pretraining)

---

[**REFINER: Reasoning Feedback on Intermediate Representations**](https://arxiv.org/abs/2304.01904) Ôºà**2023.04.04**Ôºâ

<font color="gray">Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-36-red)  [![](https://img.shields.io/badge/Github%20Stars-62-blue)](https://github.com/debjitpaul/refiner)

---

[**Self-Refine: Iterative Refinement with Self-Feedback**](https://arxiv.org/abs/2303.17651) Ôºà**2023.03.30**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-268-red)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/jina-ai/thinkgpt)

---

[**Context-faithful Prompting for Large Language Models**](https://doi.org/10.48550/arXiv.2303.11315) Ôºà**2023.03.20**Ôºâ

<font color="gray">Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-36-blue)](https://github.com/wzhouad/context-faithful-llm)

---

[**Reflexion: an autonomous agent with dynamic memory and self-reflection**](https://doi.org/10.48550/arXiv.2303.11366) Ôºà**2023.03.20**Ôºâ

<font color="gray">Noah Shinn, Beck Labash, Ashwin Gopinath .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)

---

[**SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models**](https://doi.org/10.48550/arXiv.2303.10464) Ôºà**2023.03.18**Ôºâ

<font color="gray">Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, etc .  - „ÄêConference on Uncertainty in Artificial Intelligence„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) Ôºà**2023.02.21**Ôºâ

<font color="gray">Jules White, Quchen Fu, Sam Hays, M. Sandborn, Carlos Olea, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks**](https://doi.org/10.48550/arXiv.2302.08043) Ôºà**2023.02.16**Ôºâ

<font color="gray">Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-128-blue)](https://github.com/Starlien95/GraphPrompt)

---

[**Progressive Prompts: Continual Learning for Language Models**](https://doi.org/10.48550/arXiv.2301.12314) Ôºà**2023.01.29**Ôºâ

<font color="gray">Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, M. Lewis, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-84-blue)](https://github.com/arazd/ProgressivePrompts)

---

[**Batch Prompting: Efficient Inference with Large Language Model APIs**](https://doi.org/10.48550/arXiv.2301.08721) Ôºà**2023.01.19**Ôºâ

<font color="gray">Zhoujun Cheng, Jungo Kasai, Tao Yu .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-30-blue)](https://github.com/hkunlp/batch-prompting)

---

[**Successive Prompting for Decomposing Complex Questions**](https://doi.org/10.48550/arXiv.2212.04092) Ôºà**2022.12.08**Ôºâ

<font color="gray">Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)

---

[**PAL: Program-aided Language Models**](https://doi.org/10.48550/arXiv.2211.10435) Ôºà**2022.11.18**Ôºâ

<font color="gray">Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-25-green)  [![](https://img.shields.io/badge/Github%20Stars-1.2k-blue)](https://github.com/srush/minichain)

---

[**Measuring and Narrowing the Compositionality Gap in Language Models**](https://doi.org/10.48550/arXiv.2210.03350) Ôºà**2022.10.07**Ôºâ

<font color="gray">Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-28-green)  [![](https://img.shields.io/badge/Github%20Stars-288-blue)](https://github.com/ofirpress/self-ask)

---

[**ReAct: Synergizing Reasoning and Acting in Language Models**](https://doi.org/10.48550/arXiv.2210.03629) Ôºà**2022.10.06**Ôºâ

<font color="gray">Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, I. Shafran, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-29-green)  [![](https://img.shields.io/badge/Github%20Stars-1.7k-blue)](https://github.com/ysymyth/ReAct)

---

[**Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models**](https://doi.org/10.1109/TVCG.2022.3209479) Ôºà**2022.08.16**Ôºâ

<font color="gray">Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, J. Beyer, etc .  - „ÄêIEEE Transactions on Visualization and Computer Graphics„Äë</font>

![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-102-red)

---

[**Black-box Prompt Learning for Pre-trained Language Models**](https://arxiv.org/abs/2201.08531) Ôºà**2022.01.21**Ôºâ

<font color="gray">Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, Xiao Zhou, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-17-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-55-red)  [![](https://img.shields.io/badge/Github%20Stars-55-blue)](https://github.com/shizhediao/black-box-prompt-learning)

---

[**Design Guidelines for Prompt Engineering Text-to-Image Generative Models**](https://doi.org/10.1145/3491102.3501825) Ôºà**2021.09.14**Ôºâ

<font color="gray">Vivian Liu, Lydia B. Chilton .  - „ÄêInternational Conference on Human Factors in Computing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-44-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-245-red)

---

[**Do Prompt-Based Models Really Understand the Meaning of Their Prompts?**](https://doi.org/10.18653/v1/2022.naacl-main.167) Ôºà**2021.09.02**Ôºâ

<font color="gray">Albert Webson, Ellie Pavlick .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-71-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-187-red)  [![](https://img.shields.io/badge/Github%20Stars-83-blue)](https://github.com/awebson/prompt_semantics)

---

[**PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains**](https://doi.org/10.1162/tacl_a_00468) Ôºà**2021.02.24**Ôºâ

<font color="gray">Eyal Ben-David, Nadav Oved, Roi Reichart .  - „ÄêInternational Conference on Topology, Algebra and Categories in Logic„Äë</font>

![](https://img.shields.io/badge/Citations-28-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-110-red)  [![](https://img.shields.io/badge/Github%20Stars-50-blue)](https://github.com/eyalbd2/PADA)

---

[**Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm**](https://doi.org/10.1145/3411763.3451760) Ôºà**2021.02.15**Ôºâ

<font color="gray">Laria Reynolds, Kyle McDonell .  - „ÄêCHI Extended Abstracts„Äë</font>

![](https://img.shields.io/badge/Citations-149-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-364-red)

---

[**Prompt Engineering for Text-Based Generative Art**](https://doi.org/10.48550/arXiv.2204.13988) 

<font color="gray">J. Oppenlaender .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)

---

[**L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ**](https://doi.org/10.48550/arXiv.2402.04902) 

<font color="gray">Hyesung Jeon, Yulhwa Kim, Jae-Joon Kim .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)


</div>

# CONTINUE...