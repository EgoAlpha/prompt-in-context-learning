# ğŸ“„ Soft Prompt

## Paper List

[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) ğŸ‘¨â€ğŸ“G. Zuccon,B. Koopman 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://doi.org/10.48550/arXiv.2302.09236) ğŸ‘¨â€ğŸ“Yuhang Zhou,Suraj Maharjan,Bei Liu 2023 ![](https://img.shields.io/badge/pub-2023--02--18-green)

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) ğŸ‘¨â€ğŸ“Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) ğŸ‘¨â€ğŸ“Koustava Goswami,Lukas Lange,J. Araki,Heike Adel 2023 ![](https://img.shields.io/badge/pub-2023--02--14-green)

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) ğŸ‘¨â€ğŸ“Mingyu Derek Ma,Jiun-Yu Kao,Shuyang Gao,Arpit Gupta,Di Jin,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--26-green)

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) ğŸ‘¨â€ğŸ“M Saiful Bari,Aston Zhang,Shuai Zheng,Xingjian Shi,Yi Zhu,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) ğŸ‘¨â€ğŸ“Weijia Shi,Xiaochuang Han,Hila Gonen,Ari Holtzman,Yulia Tsvetkov,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-1-red)

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) ğŸ‘¨â€ğŸ“Damith Chamalke Senadeera,Julia Ive 2022 ![](https://img.shields.io/badge/pub-2022--12--06-green)

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2211.10681) ğŸ‘¨â€ğŸ“Xiaocheng Lu,Ziming Liu,Song Guo,Jingcai Guo 2022 ![](https://img.shields.io/badge/pub-2022--11--19-green)

[**FPT: Improving Prompt Tuning Efficiency via Progressive Training**](https://doi.org/10.48550/arXiv.2211.06840) ğŸ‘¨â€ğŸ“Yufei Huang,Yujia Qin,Huadong Wang,Yichun Yin,Maosong Sun,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--13-green)

[**Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning**](https://doi.org/10.48550/arXiv.2210.12587) ğŸ‘¨â€ğŸ“Xiangyu Peng,Chen Xing,Prafulla Kumar Choubey,Chien-Sheng Wu,Caiming Xiong 2022 ![](https://img.shields.io/badge/pub-2022--10--23-green)

[**Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts**](https://doi.org/10.48550/arXiv.2210.11292) ğŸ‘¨â€ğŸ“Xiangyang Liu,Tianxiang Sun,Xuanjing Huang,Xipeng Qiu 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models**](https://doi.org/10.48550/arXiv.2210.10841) ğŸ‘¨â€ğŸ“Yue Zhang,Yueping Zhang,Hongliang Fei,Dingcheng Li,Tan Yu,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--19-green)![](https://img.shields.io/badge/cite-2-red)

[**XPrompt: Exploring the Extreme of Prompt Tuning**](https://doi.org/10.48550/arXiv.2210.04457) ğŸ‘¨â€ğŸ“Fang Ma,Chen Zhang,Lei Ren,Jingang Wang,Qifan Wang,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--10-green)![](https://img.shields.io/badge/cite-2-red)

[**Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts**](https://doi.org/10.48550/arXiv.2210.04726) ğŸ‘¨â€ğŸ“C. D. Santos,Zhe Dong,Daniel Matthew Cer,John Nham,Siamak Shakeri,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--10-green)

[**Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization**](https://doi.org/10.48550/arXiv.2210.03029) ğŸ‘¨â€ğŸ“Seonghyeon Ye,Joel Jang,Doyoung Kim,Yongrae Jo,Minjoon Seo 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-3-red)

[**Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation**](https://doi.org/10.48550/arXiv.2210.02952) ğŸ‘¨â€ğŸ“Xu Guo,Boyang Albert Li,Han Yu 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-3-red)

[**MetaPrompting: Learning to Learn Better Prompts**](https://doi.org/10.48550/arXiv.2209.11486) ğŸ‘¨â€ğŸ“Yutai Hou,Hongyuan Dong,Xinghao Wang,Bohan Li,Wanxiang Che 2022 ![](https://img.shields.io/badge/pub-2022--09--23-green)![](https://img.shields.io/badge/cite-2-red)

[**Prompt-based Conservation Learning for Multi-hop Question Answering**](https://doi.org/10.48550/arXiv.2209.06923) ğŸ‘¨â€ğŸ“Zhenyun Deng,Yonghua Zhu,Yang Chen,Qianqian Qi,M. Witbrock,etc 2022 ![](https://img.shields.io/badge/pub-2022--09--14-green)

[**Prompt Tuning with Soft Context Sharing for Vision-Language Models**](https://doi.org/10.48550/arXiv.2208.13474) ğŸ‘¨â€ğŸ“Kun Ding,Ying Wang,Pengzhang Liu,Qiang Yu,Hao Zhang,etc 2022 ![](https://img.shields.io/badge/pub-2022--08--29-green)![](https://img.shields.io/badge/cite-2-red)

[**FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning**](https://arxiv.org/abs/2302.135402208.12268) ğŸ‘¨â€ğŸ“Haodong Zhao,Wei Du,Fang Li,Peixuan Li,Gongshen Liu 2022 ![](https://img.shields.io/badge/pub-2022--08--25-green)

[**PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models - Federated Learning in Age of Foundation Model**](https://doi.org/10.48550/arXiv.2208.11625) ğŸ‘¨â€ğŸ“Tao Guo,Song Guo,Junxiao Wang,Wenchao Xu 2022 ![](https://img.shields.io/badge/pub-2022--08--24-green)![](https://img.shields.io/badge/cite-5-red)

[**PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation**](https://doi.org/10.48550/arXiv.2208.10160) ğŸ‘¨â€ğŸ“Qihuang Zhong,Liang Ding,Juhua Liu,Bo Du,Dacheng Tao 2022 ![](https://img.shields.io/badge/pub-2022--08--22-green)![](https://img.shields.io/badge/cite-11-red)

[**No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence**](https://doi.org/10.1145/3540250.3549113) ğŸ‘¨â€ğŸ“Chaozheng Wang,Yuanhang Yang,Cuiyun Gao,Yun Peng,Hongyu Zhang,etc 2022 ![](https://img.shields.io/badge/pub-2022--07--24-green)![](https://img.shields.io/badge/cite-9-red)

[**PTAU: Prompt Tuning for Attributing Unanswerable Questions**](https://doi.org/10.1145/3477495.3532048) ğŸ‘¨â€ğŸ“Jinzhi Liao,Xiang Zhao,Jianming Zheng,Xinyi Li,Fei Cai,etc 2022 ![](https://img.shields.io/badge/pub-2022--07--06-green)

[**Learning a Better Initialization for Soft Prompts via Meta-Learning**](https://doi.org/10.48550/arXiv.2205.12471) ğŸ‘¨â€ğŸ“Yukun Huang,Kun Qian,Zhou Yu 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-2-red)

[**Structured Prompt Tuning**](https://doi.org/10.48550/arXiv.2205.12309) ğŸ‘¨â€ğŸ“Chi-Liang Liu,Hung-yi Lee,Wen-tau Yih 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-1-red)

[**Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding**](https://doi.org/10.48550/arXiv.2205.11024) ğŸ‘¨â€ğŸ“Rishabh Bhardwaj,Amrita Saha,S. Hoi 2022 ![](https://img.shields.io/badge/pub-2022--05--23-green)

[**ProQA: Structural Prompt-based Pre-training for Unified Question Answering**](https://doi.org/10.48550/arXiv.2205.04040) ğŸ‘¨â€ğŸ“Wanjun Zhong,Yifan Gao,Ning Ding,Yujia Qin,Zhiyuan Liu,etc 2022 ![](https://img.shields.io/badge/pub-2022--05--09-green)![](https://img.shields.io/badge/cite-6-red)

[**Prompt Distribution Learning**](https://doi.org/10.1109/CVPR52688.2022.00514) ğŸ‘¨â€ğŸ“Yuning Lu,Jianzhuang Liu,Yonggang Zhang,Yajing Liu,Xinmei Tian 2022 ![](https://img.shields.io/badge/pub-2022--05--06-green)![](https://img.shields.io/badge/cite-24-red)

[**HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification**](https://doi.org/10.48550/arXiv.2204.13413) ğŸ‘¨â€ğŸ“Zihan Wang,Peiyi Wang,Tianyu Liu,Yunbo Cao,Zhifang Sui,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--28-green)

[**Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation**](https://doi.org/10.48550/arXiv.2204.13362) ğŸ‘¨â€ğŸ“Kexin Yang,Dayiheng Liu,Wenqiang Lei,Baosong Yang,Mingfeng Xue,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--28-green)![](https://img.shields.io/badge/cite-9-red)

[**PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization**](https://doi.org/10.48550/arXiv.2204.04413) ğŸ‘¨â€ğŸ“Xiaochen Liu,Yu Bai,Jiawei Li,Yinan Hu,Yang Gao 2022 ![](https://img.shields.io/badge/pub-2022--04--09-green)![](https://img.shields.io/badge/cite-7-red)

[**Learning to Compose Soft Prompts for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2204.03574) ğŸ‘¨â€ğŸ“Nihal V. Nayak,Peilin Yu,Stephen H. Bach 2022 ![](https://img.shields.io/badge/pub-2022--04--07-green)![](https://img.shields.io/badge/cite-9-red)

[**Unsupervised Prompt Learning for Vision-Language Models**](https://doi.org/10.48550/arXiv.2204.03649) ğŸ‘¨â€ğŸ“Hao Huang,Jack Chu,Fangyun Wei 2022 ![](https://img.shields.io/badge/pub-2022--04--07-green)![](https://img.shields.io/badge/cite-17-red)

[**Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning**](https://doi.org/10.1145/3539597.3570398) ğŸ‘¨â€ğŸ“Ziyun Xu,Chengyu Wang,Minghui Qiu,Fuli Luo,Runxin Xu,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--01-green)![](https://img.shields.io/badge/cite-3-red)

[**Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model**](https://doi.org/10.1109/CVPR52688.2022.01369) ğŸ‘¨â€ğŸ“Yu Du,Fangyun Wei,Zihe Zhang,Miaojing Shi,Yue Gao,etc 2022 ![](https://img.shields.io/badge/pub-2022--03--28-green)![](https://img.shields.io/badge/cite-31-red)

[**Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning**](https://doi.org/10.48550/arXiv.2203.11720) ğŸ‘¨â€ğŸ“Yuhui Zuo,Wei Zhu,Guoyong Cai 2022 ![](https://img.shields.io/badge/pub-2022--03--16-green)

[**Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning**](https://arxiv.org/abs/2302.135402203.06875) ğŸ‘¨â€ğŸ“Yu-Ying Jiang,Linhan Zhang,Wei Wang 2022 ![](https://img.shields.io/badge/pub-2022--03--14-green)

[**PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks**](https://doi.org/10.18653/v1/2022.acl-long.292) ğŸ‘¨â€ğŸ“Yufei Wang,Can Xu,Qingfeng Sun,Huang Hu,Chongyang Tao,etc 2022 ![](https://img.shields.io/badge/pub-2022--02--25-green)![](https://img.shields.io/badge/cite-17-red)

[**Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt**](https://arxiv.org/abs/2302.135402202.11451) ğŸ‘¨â€ğŸ“Lianzhe Huang,Shuming Ma,Dongdong Zhang,Furu Wei,Houfeng Wang 2022 ![](https://img.shields.io/badge/pub-2022--02--23-green)![](https://img.shields.io/badge/cite-4-red)

[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2302.135402202.07371) ğŸ‘¨â€ğŸ“Lei Li,Yongfeng Zhang,Li Chen 2022 ![](https://img.shields.io/badge/pub-2022--02--15-green)![](https://img.shields.io/badge/cite-10-red)

[**Toward Digital Twin Oriented Modeling of Complex Networked Systems and Their Dynamics: A Comprehensive Survey**](https://doi.org/10.1109/ACCESS.2022.3184801) ğŸ‘¨â€ğŸ“Jiaqi Wen,B. Gabrys,Katarzyna Musial 2022 ![](https://img.shields.io/badge/pub-2022--02--15-green)![](https://img.shields.io/badge/cite-5-red)

[**Context-Tuning: Learning Contextualized Prompts for Natural Language Generation**](https://arxiv.org/abs/2302.135402201.08670) ğŸ‘¨â€ğŸ“Tianyi Tang,Junyi Li,Wayne Xin Zhao 2022 ![](https://img.shields.io/badge/pub-2022--01--21-green)![](https://img.shields.io/badge/cite-9-red)

[**Instance-aware Prompt Learning for Language Understanding and Generation**](https://arxiv.org/abs/2302.135402201.07126) ğŸ‘¨â€ğŸ“Feihu Jin,Jinliang Lu,Jiajun Zhang,Chengqing Zong 2022 ![](https://img.shields.io/badge/pub-2022--01--18-green)![](https://img.shields.io/badge/cite-10-red)

[**Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer**](https://doi.org/10.1007/978-3-031-15931-2_19) ğŸ‘¨â€ğŸ“Yinyi Wei,Tong Mo,Yong Jiang,Weiping Li,Wen Zhao 2022 ![](https://img.shields.io/badge/pub-2022--01--14-green)![](https://img.shields.io/badge/cite-2-red)

[**On Transferability of Prompt Tuning for Natural Language Processing**](https://doi.org/10.18653/v1/2022.naacl-main.290) ğŸ‘¨â€ğŸ“Yusheng Su,Xiaozhi Wang,Yujia Qin,Chi-Min Chan,Yankai Lin,etc 2021 ![](https://img.shields.io/badge/pub-2021--11--12-green)![](https://img.shields.io/badge/cite-19-red)

[**OpenPrompt: An Open-source Framework for Prompt-learning**](https://doi.org/10.18653/v1/2022.acl-demo.10) ğŸ‘¨â€ğŸ“Ning Ding,Shengding Hu,Weilin Zhao,Yulin Chen,Zhiyuan Liu,etc 2021 ![](https://img.shields.io/badge/pub-2021--11--03-green)![](https://img.shields.io/badge/cite-59-red)

[**SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer**](https://doi.org/10.18653/v1/2022.acl-long.346) ğŸ‘¨â€ğŸ“Tu Vu,Brian Lester,Noah Constant,Rami Al-Rfou,Daniel Matthew Cer 2021 ![](https://img.shields.io/badge/pub-2021--10--15-green)![](https://img.shields.io/badge/cite-85-red)

[**P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**](https://arxiv.org/abs/2302.135402110.07602) ğŸ‘¨â€ğŸ“Xiao Liu,Kaixuan Ji,Yicheng Fu,Zhengxiao Du,Zhilin Yang,etc 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-148-red)

[**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://doi.org/10.18653/v1/2022.acl-long.576) ğŸ‘¨â€ğŸ“Yuxian Gu,Xu Han,Zhiyuan Liu,Minlie Huang 2021 ![](https://img.shields.io/badge/pub-2021--09--09-green)![](https://img.shields.io/badge/cite-107-red)

[**Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning**](https://arxiv.org/abs/2302.135402106.09226) ğŸ‘¨â€ğŸ“Colin Wei,Sang Michael Xie,Tengyu Ma 2021 ![](https://img.shields.io/badge/pub-2021--06--17-green)![](https://img.shields.io/badge/cite-33-red)

[**Learning How to Ask: Querying LMs with Mixtures of Soft Prompts**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.410) ğŸ‘¨â€ğŸ“Guanghui Qin,J. Eisner 2021 ![](https://img.shields.io/badge/pub-2021--04--14-green)![](https://img.shields.io/badge/cite-182-red)

[**P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**](https://doi.org/10.18653/v1/2022.acl-short.8) ğŸ‘¨â€ğŸ“Xiao Liu,Kaixuan Ji,Yicheng Fu,W. Tam,Zhengxiao Du,etc 2022 ![](https://img.shields.io/badge/cite-45-red)

[**Coherent Long Text Generation by Contrastive Soft Prompt**](https://api.semanticscholar.org/4dc3683fa223d160045bca575a8b5ecf94f61604) ğŸ‘¨â€ğŸ“ 2022 

[**Continuous Prompt Tuning for Russian: How to Learn Prompts Efficiently with RuGPT3?**](https://doi.org/10.1007/978-3-031-15168-2_3) ğŸ‘¨â€ğŸ“Nikita Konodyuk,M. Tikhonova 2021 ![](https://img.shields.io/badge/cite-3-red)

[**PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning**](https://api.semanticscholar.org/b5da0ffa7b60abefa036bc450e8e333087943787) ğŸ‘¨â€ğŸ“ 2022 

[**PTAU: Prompt Tuning for Attributing Unanswerable Questions**](https://api.semanticscholar.org/97d214a10afb84f3b94881a3ad8e90372ba5809c) ğŸ‘¨â€ğŸ“ 2022 

[**Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis**](https://doi.org/10.18653/v1/2022.acl-long.174) ğŸ‘¨â€ğŸ“Hui-Hsin Wu,Xiaodon Shi 2022 ![](https://img.shields.io/badge/cite-5-red)

[**Changing the Narrative Perspective: From Ranking to Prompt-Based Generation of Entity Mentions**](https://api.semanticscholar.org/b70363498cb912b31c4647f92e848047f01c2754) ğŸ‘¨â€ğŸ“ 2022 

[**Enhance Performance of Ad-hoc Search via Prompt Learning**](https://doi.org/10.1007/978-3-031-24755-2_3) ğŸ‘¨â€ğŸ“Shenghao Yang,Yiqun Liu,Xiaohui Xie,M. Zhang,Shaoping Ma 2022 


# CONTINUE...
