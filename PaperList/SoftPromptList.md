# üìÑ Soft Prompt

## Paper List

<div style="line-height:0.2em;">


[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) Ôºà**2023.02.23**Ôºâ

<font color="gray">G. Zuccon, B. Koopman .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) Ôºà**2023.02.22**Ôºâ

<font color="gray">Simeng Sun, Yang Liu, Dan Iter, Chenguang Zhu, Mohit Iyyer .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://doi.org/10.48550/arXiv.2302.09236) Ôºà**2023.02.18**Ôºâ

<font color="gray">Yuhang Zhou, Suraj Maharjan, Bei Liu .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) Ôºà**2023.02.17**Ôºâ

<font color="gray">Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/zhjohnchan/ptunifier)

---

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) Ôºà**2023.02.14**Ôºâ

<font color="gray">Koustava Goswami, Lukas Lange, J. Araki, Heike Adel .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-44-blue)](https://github.com/boschresearch/switchprompt)

---

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) Ôºà**2023.01.26**Ôºâ

<font color="gray">Mingyu Derek Ma, Jiun-Yu Kao, Shuyang Gao, Arpit Gupta, Di Jin, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) Ôºà**2022.12.20**Ôºâ

<font color="gray">Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) Ôºà**2022.12.06**Ôºâ

<font color="gray">Damith Chamalke Senadeera, Julia Ive .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2211.10681) Ôºà**2022.11.19**Ôºâ

<font color="gray">Xiaocheng Lu, Ziming Liu, Song Guo, Jingcai Guo .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/forest-art/dfsp)

---

[**FPT: Improving Prompt Tuning Efficiency via Progressive Training**](https://doi.org/10.48550/arXiv.2211.06840) Ôºà**2022.11.13**Ôºâ

<font color="gray">Yufei Huang, Yujia Qin, Huadong Wang, Yichun Yin, Maosong Sun, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/thunlp/fastprompttuning)

---

[**Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning**](https://doi.org/10.48550/arXiv.2210.12587) Ôºà**2022.10.23**Ôºâ

<font color="gray">Xiangyu Peng, Chen Xing, Prafulla Kumar Choubey, Chien-Sheng Wu, Caiming Xiong .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts**](https://doi.org/10.48550/arXiv.2210.11292) Ôºà**2022.10.20**Ôºâ

<font color="gray">Xiangyang Liu, Tianxiang Sun, Xuanjing Huang, Xipeng Qiu .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/xyltt/lpt)

---

[**Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models**](https://doi.org/10.48550/arXiv.2210.10841) Ôºà**2022.10.19**Ôºâ

<font color="gray">Yue Zhang, Yueping Zhang, Hongliang Fei, Dingcheng Li, Tan Yu, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**XPrompt: Exploring the Extreme of Prompt Tuning**](https://doi.org/10.48550/arXiv.2210.04457) Ôºà**2022.10.10**Ôºâ

<font color="gray">Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts**](https://doi.org/10.48550/arXiv.2210.04726) Ôºà**2022.10.10**Ôºâ

<font color="gray">C. D. Santos, Zhe Dong, Daniel Matthew Cer, John Nham, Siamak Shakeri, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization**](https://doi.org/10.48550/arXiv.2210.03029) Ôºà**2022.10.06**Ôºâ

<font color="gray">Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo, Minjoon Seo .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-12-blue)](https://github.com/seonghyeonye/rospr)

---

[**Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation**](https://doi.org/10.48550/arXiv.2210.02952) Ôºà**2022.10.06**Ôºâ

<font color="gray">Xu Guo, Boyang Albert Li, Han Yu .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-14-blue)](https://github.com/liangcici/probes-vln)

---

[**MetaPrompting: Learning to Learn Better Prompts**](https://doi.org/10.48550/arXiv.2209.11486) Ôºà**2022.09.23**Ôºâ

<font color="gray">Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li, Wanxiang Che .  - „ÄêInternational Conference on Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/dousia/metaprompting)

---

[**Prompt-based Conservation Learning for Multi-hop Question Answering**](https://doi.org/10.48550/arXiv.2209.06923) Ôºà**2022.09.14**Ôºâ

<font color="gray">Zhenyun Deng, Yonghua Zhu, Yang Chen, Qianqian Qi, M. Witbrock, etc .  - „ÄêInternational Conference on Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Prompt Tuning with Soft Context Sharing for Vision-Language Models**](https://doi.org/10.48550/arXiv.2208.13474) Ôºà**2022.08.29**Ôºâ

<font color="gray">Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Hao Zhang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning**](https://arxiv.org/abs/2208.12268) Ôºà**2022.08.25**Ôºâ

<font color="gray">Haodong Zhao, Wei Du, Fang Li, Peixuan Li, Gongshen Liu </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models - Federated Learning in Age of Foundation Model**](https://doi.org/10.48550/arXiv.2208.11625) Ôºà**2022.08.24**Ôºâ

<font color="gray">Tao Guo, Song Guo, Junxiao Wang, Wenchao Xu .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)

---

[**PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation**](https://doi.org/10.48550/arXiv.2208.10160) Ôºà**2022.08.22**Ôºâ

<font color="gray">Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-11-green)

---

[**No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence**](https://doi.org/10.1145/3540250.3549113) Ôºà**2022.07.24**Ôºâ

<font color="gray">Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, etc .  - „ÄêESEC/SIGSOFT FSE„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-25-red)  [![](https://img.shields.io/badge/Github%20Stars-28-blue)](https://github.com/adf1178/pt4code)

---

[**PTAU: Prompt Tuning for Attributing Unanswerable Questions**](https://doi.org/10.1145/3477495.3532048) Ôºà**2022.07.06**Ôºâ

<font color="gray">Jinzhi Liao, Xiang Zhao, Jianming Zheng, Xinyi Li, Fei Cai, etc .  - „ÄêProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**Learning a Better Initialization for Soft Prompts via Meta-Learning**](https://doi.org/10.48550/arXiv.2205.12471) Ôºà**2022.05.25**Ôºâ

<font color="gray">Yukun Huang, Kun Qian, Zhou Yu .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts**](https://arxiv.org/abs/2205.11961) Ôºà**2022.05.24**Ôºâ

<font color="gray">Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)  [![](https://img.shields.io/badge/Github%20Stars-58-blue)](https://github.com/akariasai/attempt)

---

[**Structured Prompt Tuning**](https://doi.org/10.48550/arXiv.2205.12309) Ôºà**2022.05.24**Ôºâ

<font color="gray">Chi-Liang Liu, Hung-yi Lee, Wen-tau Yih .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/zjukg/kgtransformer)

---

[**Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding**](https://doi.org/10.48550/arXiv.2205.11024) Ôºà**2022.05.23**Ôºâ

<font color="gray">Rishabh Bhardwaj, Amrita Saha, S. Hoi .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/declare-lab/vip)

---

[**ProQA: Structural Prompt-based Pre-training for Unified Question Answering**](https://doi.org/10.48550/arXiv.2205.04040) Ôºà**2022.05.09**Ôºâ

<font color="gray">Wanjun Zhong, Yifan Gao, Ning Ding, Yujia Qin, Zhiyuan Liu, etc .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-6-green)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/zhongwanjun/proqa)

---

[**Prompt Distribution Learning**](https://doi.org/10.1109/CVPR52688.2022.00514) Ôºà**2022.05.06**Ôºâ

<font color="gray">Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian .  - „ÄêComputer Vision and Pattern Recognition„Äë</font>

![](https://img.shields.io/badge/Citations-24-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-67-red)  [![](https://img.shields.io/badge/Github%20Stars-852-blue)](https://github.com/openai/following-instructions-human-feedback)

---

[**HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification**](https://doi.org/10.48550/arXiv.2204.13413) Ôºà**2022.04.28**Ôºâ

<font color="gray">Zihan Wang, Peiyi Wang, Tianyu Liu, Yunbo Cao, Zhifang Sui, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-34-blue)](https://github.com/wzh9969/hpt)

---

[**Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation**](https://doi.org/10.48550/arXiv.2204.13362) Ôºà**2022.04.28**Ôºâ

<font color="gray">Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)

---

[**PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization**](https://doi.org/10.48550/arXiv.2204.04413) Ôºà**2022.04.09**Ôºâ

<font color="gray">Xiaochen Liu, Yu Bai, Jiawei Li, Yinan Hu, Yang Gao .  - „ÄêInternational Conference on Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-7-green)

---

[**Learning to Compose Soft Prompts for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2204.03574) Ôºà**2022.04.07**Ôºâ

<font color="gray">Nihal V. Nayak, Peilin Yu, Stephen H. Bach .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-37-blue)](https://github.com/batsresearch/csp)

---

[**Unsupervised Prompt Learning for Vision-Language Models**](https://doi.org/10.48550/arXiv.2204.03649) Ôºà**2022.04.07**Ôºâ

<font color="gray">Hao Huang, Jack Chu, Fangyun Wei .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-17-green)  [![](https://img.shields.io/badge/Github%20Stars-67-blue)](https://github.com/tonyhuang2022/upl)

---

[**Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning**](https://doi.org/10.1145/3539597.3570398) Ôºà**2022.04.01**Ôºâ

<font color="gray">Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, etc .  - „ÄêWeb Search and Data Mining„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/alibaba/EasyNLP/tree/master/examples/fewshot_learning)

---

[**Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model**](https://doi.org/10.1109/CVPR52688.2022.01369) Ôºà**2022.03.28**Ôºâ

<font color="gray">Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, etc .  - „ÄêComputer Vision and Pattern Recognition„Äë</font>

![](https://img.shields.io/badge/Citations-31-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-131-red)  [![](https://img.shields.io/badge/Github%20Stars-115-blue)](https://github.com/dyabel/detpro)

---

[**Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning**](https://doi.org/10.48550/arXiv.2203.11720) Ôºà**2022.03.16**Ôºâ

<font color="gray">Yuhui Zuo, Wei Zhu, Guoyong Cai .  - „ÄêInternational Conference on Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning**](https://arxiv.org/abs/2203.06875) Ôºà**2022.03.14**Ôºâ

<font color="gray">Yu-Ying Jiang, Linhan Zhang, Wei Wang .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)  [![](https://img.shields.io/badge/Github%20Stars-101-blue)](https://github.com/yjiangcm/promcse)

---

[**PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks**](https://doi.org/10.18653/v1/2022.acl-long.292) Ôºà**2022.02.25**Ôºâ

<font color="gray">Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, etc .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-17-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-97-red)  [![](https://img.shields.io/badge/Github%20Stars-47-blue)](https://github.com/garyyufei/promda)

---

[**Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt**](https://arxiv.org/abs/2202.11451) Ôºà**2022.02.23**Ôºâ

<font color="gray">Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, Houfeng Wang .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-23-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/mojave-pku/uniprompt)

---

[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2202.07371) Ôºà**2022.02.15**Ôºâ

<font color="gray">Lei Li, Yongfeng Zhang, Li Chen .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-57-blue)](https://github.com/lileipisces/pepler)

---

[**Toward Digital Twin Oriented Modeling of Complex Networked Systems and Their Dynamics: A Comprehensive Survey**](https://doi.org/10.1109/ACCESS.2022.3184801) Ôºà**2022.02.15**Ôºâ

<font color="gray">Jiaqi Wen, B. Gabrys, Katarzyna Musial .  - „ÄêIEEE Access„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)

---

[**Context-Tuning: Learning Contextualized Prompts for Natural Language Generation**](https://arxiv.org/abs/2201.08670) Ôºà**2022.01.21**Ôºâ

<font color="gray">Tianyi Tang, Junyi Li, Wayne Xin Zhao .  - „ÄêInternational Conference on Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-51-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/rucaibox/context-tuning)

---

[**Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer**](https://doi.org/10.1007/978-3-031-15931-2_19) Ôºà**2022.01.14**Ôºâ

<font color="gray">Yinyi Wei, Tong Mo, Yong Jiang, Weiping Li, Wen Zhao .  - „ÄêInternational Conference on Artificial Neural Networks„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-15-blue)](https://github.com/ydongd/prototypical-prompt-verbalizer)

---

[**On Transferability of Prompt Tuning for Natural Language Processing**](https://doi.org/10.18653/v1/2022.naacl-main.290) Ôºà**2021.11.12**Ôºâ

<font color="gray">Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, etc .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-19-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-63-red)  [![](https://img.shields.io/badge/Github%20Stars-53-blue)](https://github.com/thunlp/Prompt-Transferability)

---

[**OpenPrompt: An Open-source Framework for Prompt-learning**](https://doi.org/10.18653/v1/2022.acl-demo.10) Ôºà**2021.11.03**Ôºâ

<font color="gray">Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, etc .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-59-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-139-red)  [![](https://img.shields.io/badge/Github%20Stars-2.6k-blue)](https://github.com/thunlp/OpenPrompt)

---

[**SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer**](https://doi.org/10.18653/v1/2022.acl-long.346) Ôºà**2021.10.15**Ôºâ

<font color="gray">Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, Daniel Matthew Cer .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-85-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-151-red)

---

[**P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**](https://arxiv.org/abs/2110.07602) Ôºà**2021.10.14**Ôºâ

<font color="gray">Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-148-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)  [![](https://img.shields.io/badge/Github%20Stars-606-blue)](https://github.com/thudm/p-tuning-v2)

---

[**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://doi.org/10.18653/v1/2022.acl-long.576) Ôºà**2021.09.09**Ôºâ

<font color="gray">Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-107-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-276-red)  [![](https://img.shields.io/badge/Github%20Stars-66-blue)](https://github.com/thu-coai/ppt)

---

[**Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning**](https://arxiv.org/abs/2106.09226) Ôºà**2021.06.17**Ôºâ

<font color="gray">Colin Wei, Sang Michael Xie, Tengyu Ma .  - „ÄêNeural Information Processing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-33-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-114-red)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/sangmichaelxie/pretraining_analysis)

---

[**Learning How to Ask: Querying LMs with Mixtures of Soft Prompts**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.410) Ôºà**2021.04.14**Ôºâ

<font color="gray">Guanghui Qin, J. Eisner .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-182-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-238-red)  [![](https://img.shields.io/badge/Github%20Stars-66-blue)](https://github.com/hiaoxui/soft-prompts)

---

[**Transformers as Soft Reasoners over Language**](https://doi.org/10.24963/ijcai.2020/533) Ôºà**2020.02.14**Ôºâ

<font color="gray">Peter Clark, Oyvind Tafjord, Kyle Richardson .  - „ÄêInternational Joint Conference on Artificial Intelligence„Äë</font>

![](https://img.shields.io/badge/Citations-158-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-91-red)  [![](https://img.shields.io/badge/Github%20Stars-27-blue)](https://github.com/allenai/ruletaker)

---

[**P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**](https://doi.org/10.18653/v1/2022.acl-short.8) 

<font color="gray">Xiao Liu, Kaixuan Ji, Yicheng Fu, W. Tam, Zhengxiao Du, etc .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-45-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-284-red)  [![](https://img.shields.io/badge/Github%20Stars-606-blue)](https://github.com/thudm/p-tuning-v2)

---

[**Coherent Long Text Generation by Contrastive Soft Prompt**](https://api.semanticscholar.org/4dc3683fa223d160045bca575a8b5ecf94f61604) 



![](https://img.shields.io/badge/Citations-0-green)

---

[**Continuous Prompt Tuning for Russian: How to Learn Prompts Efficiently with RuGPT3?**](https://doi.org/10.1007/978-3-031-15168-2_3) 

<font color="gray">Nikita Konodyuk, M. Tikhonova .  - „ÄêInternational Joint Conference on the Analysis of Images, Social Networks and Texts„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning**](https://api.semanticscholar.org/b5da0ffa7b60abefa036bc450e8e333087943787) 



![](https://img.shields.io/badge/Citations-0-green)

---

[**PTAU: Prompt Tuning for Attributing Unanswerable Questions**](https://api.semanticscholar.org/97d214a10afb84f3b94881a3ad8e90372ba5809c) 



![](https://img.shields.io/badge/Citations-0-green)

---

[**Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis**](https://doi.org/10.18653/v1/2022.acl-long.174) 

<font color="gray">Hui-Hsin Wu, Xiaodon Shi .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-52-red)

---

[**Changing the Narrative Perspective: From Ranking to Prompt-Based Generation of Entity Mentions**](https://api.semanticscholar.org/b70363498cb912b31c4647f92e848047f01c2754) 



![](https://img.shields.io/badge/Citations-0-green)

---

[**Enhance Performance of Ad-hoc Search via Prompt Learning**](https://doi.org/10.1007/978-3-031-24755-2_3) 

<font color="gray">Shenghao Yang, Yiqun Liu, Xiaohui Xie, M. Zhang, Shaoping Ma .  - „ÄêChina Conference on Information Retrieval„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)


</div>

# CONTINUE...

