# ğŸ“„ Soft Prompt

## Paper List

[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) ğŸ‘¨â€ğŸ“G. Zuccon,B. Koopman 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) ğŸ‘¨â€ğŸ“Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) ğŸ‘¨â€ğŸ“Koustava Goswami,Lukas Lange,J. Araki,Heike Adel 2023 ![](https://img.shields.io/badge/pub-2023--02--14-green)

[**PTAU: Prompt Tuning for Attributing Unanswerable Questions**](https://doi.org/10.1145/3477495.3532048) ğŸ‘¨â€ğŸ“Jinzhi Liao,Xiang Zhao,Jianming Zheng,Xinyi Li,Fei Cai,Jiuyang Tang etc 2022 ![](https://img.shields.io/badge/pub-2022--07--06-green)![](https://img.shields.io/badge/cite-2-red)

[**Unsupervised Prompt Learning for Vision-Language Models**](https://doi.org/10.48550/arXiv.2204.03649) ğŸ‘¨â€ğŸ“Hao Huang,Jack Chu,Fangyun Wei 2022 ![](https://img.shields.io/badge/pub-2022--04--07-green)![](https://img.shields.io/badge/cite-17-red)

[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2302.135402202.07371) ğŸ‘¨â€ğŸ“Lei Li,Yongfeng Zhang,Li Chen 2022 ![](https://img.shields.io/badge/pub-2022--02--15-green)![](https://img.shields.io/badge/cite-10-red)

[**Context-Tuning: Learning Contextualized Prompts for Natural Language Generation**](https://arxiv.org/abs/2302.135402201.08670) ğŸ‘¨â€ğŸ“Tianyi Tang,Junyi Li,Wayne Xin Zhao 2022 ![](https://img.shields.io/badge/pub-2022--01--21-green)![](https://img.shields.io/badge/cite-9-red)

[**P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**](https://arxiv.org/abs/2302.135402110.07602) ğŸ‘¨â€ğŸ“Xiao Liu,Kaixuan Ji,Yicheng Fu,Zhengxiao Du,Zhilin Yang,Jie Tang etc 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-148-red)

[**Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models**](https://arxiv.org/abs/2302.135402210.10841) ğŸ‘¨â€ğŸ“ 

[**P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**](https://doi.org/10.18653/v1/2022.acl-short.8) ğŸ‘¨â€ğŸ“Xiao Liu,Kaixuan Ji,Yicheng Fu,W. Tam,Zhengxiao Du,Zhilin Yang etc 2022 ![](https://img.shields.io/badge/cite-45-red)

[**FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning**](https://arxiv.org/abs/2302.135402208.12268) ğŸ‘¨â€ğŸ“ 

[**Instance-aware prompt learning for language understanding and generation**](https://arxiv.org/abs/2302.135402201.07126) ğŸ‘¨â€ğŸ“ 

[**Learning to Compose Soft Prompts for Compositional Zero-Shot Learning**](https://arxiv.org/abs/2302.135402204.03574) ğŸ‘¨â€ğŸ“ 

[**FPT: Improving Prompt Tuning Efficiency via Progressive Training**](https://arxiv.org/abs/2302.135402211.06840) ğŸ‘¨â€ğŸ“ 

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://arxiv.org/abs/2302.135402211.10681) ğŸ‘¨â€ğŸ“ 

[**Prompt Distribution Learning**](https://arxiv.org/abs/2302.135402205.03340) ğŸ‘¨â€ğŸ“ 

[**Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts**](https://arxiv.org/abs/2302.135402210.11292) ğŸ‘¨â€ğŸ“ 

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://arxiv.org/abs/2302.135402302.09236) ğŸ‘¨â€ğŸ“ 

[**HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification**](https://arxiv.org/abs/2302.135402204.13413) ğŸ‘¨â€ğŸ“ 

[**Structured Prompt Tuning**](https://arxiv.org/abs/2302.135402205.12309) ğŸ‘¨â€ğŸ“ 

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://arxiv.org/abs/2302.135402301.10915) ğŸ‘¨â€ğŸ“ 

[**Prompt-based Conservation Learning for Multi-hop Question Answering**](https://arxiv.org/abs/2302.135402209.06923) ğŸ‘¨â€ğŸ“ 

[**Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning**](https://doi.org/10.1145/3539597.3570398) ğŸ‘¨â€ğŸ“Menglin Jia,Luming Tang,Bor-Chun Chen,Claire Cardie,S. Belongie,Bharath Hariharan etc 2022 

[**Continuous Detection, Rapidly React: Unseen Rumors Detection based on Continual Prompt-Tuning**](https://arxiv.org/abs/2302.135402203.11720) ğŸ‘¨â€ğŸ“ 

[**PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization**](https://arxiv.org/abs/2302.135402204.04413) ğŸ‘¨â€ğŸ“ 

[**Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer**](https://doi.org/10.1007/978-3-031-15931-2_19) ğŸ‘¨â€ğŸ“ 

[**On Transferability of Prompt Tuning for Natural Language Processing**](https://doi.org/10.18653/v1/2022.naacl-main.290) ğŸ‘¨â€ğŸ“ 

[**XPrompt: Exploring the Extreme of Prompt Tuning**](https://arxiv.org/abs/2302.135402210.04457) ğŸ‘¨â€ğŸ“ 

[**PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks**](https://arxiv.org/abs/2302.135402202.12499) ğŸ‘¨â€ğŸ“ 

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“ 

[**Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding**](https://arxiv.org/abs/2302.135402205.11024) ğŸ‘¨â€ğŸ“ 

[**Continuous prompt tuning for russian: how to learn prompts efficiently with rugpt3?**](https://doi.org/10.1007/978-3-031-15168-2_3) ğŸ‘¨â€ğŸ“ 

[**No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence**](https://arxiv.org/abs/2302.135402207.11680) ğŸ‘¨â€ğŸ“ 

[**Toward Human Readable Prompt Tuning: Kubrickâ€™s The Shining is a good movie, and a good prompt too?**](https://arxiv.org/abs/2302.135402212.10539) ğŸ‘¨â€ğŸ“ 

[**Tailor: A prompt-based approach to attribute-based controlled text generation**](https://arxiv.org/abs/2302.135402204.13362) ğŸ‘¨â€ğŸ“ 

[**Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts**](https://arxiv.org/abs/2302.135402210.04726) ğŸ‘¨â€ğŸ“ 

[**Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization**](https://arxiv.org/abs/2302.135402210.03029) ğŸ‘¨â€ğŸ“ 

[**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://arxiv.org/abs/2302.135402109.04332) ğŸ‘¨â€ğŸ“ 

[**Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning(**](https://arxiv.org/abs/2302.135402106.09226) ğŸ‘¨â€ğŸ“ 

[**ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts**](https://arxiv.org/abs/2302.135402205.11961) ğŸ‘¨â€ğŸ“ 

[**Prompt Tuning with Soft Context Sharing for Vision-Language Models**](https://arxiv.org/abs/2302.135402208.13474) ğŸ‘¨â€ğŸ“ 

[**Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning**](https://arxiv.org/abs/2302.135402210.12587) ğŸ‘¨â€ğŸ“ 

[**SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer**](https://arxiv.org/abs/2302.135402110.07904) ğŸ‘¨â€ğŸ“ 

[**PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation**](https://arxiv.org/abs/2302.135402208.10160) ğŸ‘¨â€ğŸ“ 

[**Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning**](https://arxiv.org/abs/2302.135402203.06875) ğŸ‘¨â€ğŸ“ 

[**ProQA: Structural Prompt-based Pre-training for Unified Question Answering**](https://arxiv.org/abs/2302.135402205.04040) ğŸ‘¨â€ğŸ“ 

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://arxiv.org/abs/2302.135402212.10929) ğŸ‘¨â€ğŸ“ 

[**Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation**](https://arxiv.org/abs/2302.135402210.02952) ğŸ‘¨â€ğŸ“ 

[**Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis**](https://doi.org/10.18653/v1/2022.acl-long.174) ğŸ‘¨â€ğŸ“ 

[**Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning**](https://arxiv.org/abs/2302.135402202.09363) ğŸ‘¨â€ğŸ“ 

[**PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation ModePromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Mode**](https://arxiv.org/abs/2302.135402208.11625) ğŸ‘¨â€ğŸ“ 

[**OpenPrompt: An Open-source Framework for Prompt-learning**](https://arxiv.org/abs/2302.135402111.01998) ğŸ‘¨â€ğŸ“ 

[**Learning a Better Initialization for Soft Prompts via Meta-Learning**](https://arxiv.org/abs/2302.135402205.12471) ğŸ‘¨â€ğŸ“ 

[**Learning to prompt for open-vocabulary object detection with vision-language model**](https://arxiv.org/abs/2302.135402203.14940) ğŸ‘¨â€ğŸ“ 

[**Enhance Performance of Ad-hoc Search via Prompt Learning**](https://doi.org/10.1007/978-3-031-24755-2_3) ğŸ‘¨â€ğŸ“ 

[**Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt(**](https://arxiv.org/abs/2302.135402202.11451) ğŸ‘¨â€ğŸ“ 

[**MetaPrompting: Learning to Learn Better Prompts**](https://arxiv.org/abs/2302.135402209.11486) ğŸ‘¨â€ğŸ“ 

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://arxiv.org/abs/2302.135402212.02924) ğŸ‘¨â€ğŸ“ 


# CONTINUE...
