# üìÑ Chain of Thought

## Paper List

<div style="line-height:0.2em;">


[**Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models**](https://doi.org/10.48550/arXiv.2302.00618) Ôºà**2023.02.01**Ôºâ

<font color="gray">Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) Ôºà**2022.12.20**Ôºâ

<font color="gray">Namgyu Ho, Laura Schmid, Se-Young Yun .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-5-red)


---
[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) Ôºà**2022.12.16**Ôºâ

<font color="gray">Hanlin Zhang, Yi-Fan Zhang, Li Erran Li, Eric Xing .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) Ôºà**2022.11.25**Ôºâ

<font color="gray">Xi Ye, Srini Iyer, Asli Celikyilmaz, V. Stoyanov, Greg Durrett, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-5-red)


---
[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) Ôºà**2022.10.17**Ôºâ

<font color="gray">Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-27-red)


---
[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) Ôºà**2022.10.17**Ôºâ

<font color="gray">Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-9-red)


---
[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) Ôºà**2022.10.07**Ôºâ

<font color="gray">Zhuosheng Zhang, Aston Zhang, Mu Li, Alexander J. Smola .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-23-red)


---
[**Measuring and Narrowing the Compositionality Gap in Language Models**](https://doi.org/10.48550/arXiv.2210.03350) Ôºà**2022.10.07**Ôºâ

<font color="gray">Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-27-red)


---
[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) Ôºà**2022.10.06**Ôºâ

<font color="gray">Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-21-red)


---
[**Decomposed Prompting: A Modular Approach for Solving Complex Tasks**](https://doi.org/10.48550/arXiv.2210.02406) Ôºà**2022.10.05**Ôºâ

<font color="gray">Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-25-red)


---
[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) Ôºà**2022.10.03**Ôºâ

<font color="gray">Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-18-red)


---
[**Compositional Semantic Parsing with Large Language Models**](https://doi.org/10.48550/arXiv.2209.15003) Ôºà**2022.09.29**Ôºâ

<font color="gray">Andrew Drozdov, Nathanael Scharli, Ekin Akyuurek, Nathan Scales, Xinying Song, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-16-red)


---
[**Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering**](https://doi.org/10.48550/arXiv.2209.09513) Ôºà**2022.09.20**Ôºâ

<font color="gray">Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-17-red)


---
[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) Ôºà**2022.07.02**Ôºâ

<font color="gray">Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-26-red)


---
[**Emergent Abilities of Large Language Models**](https://doi.org/10.48550/arXiv.2206.07682) Ôºà**2022.06.15**Ôºâ

<font color="gray">Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-151-red)


---
[**On the Advance of Making Language Models Better Reasoners**](https://doi.org/10.48550/arXiv.2206.02336) Ôºà**2022.06.06**Ôºâ

<font color="gray">Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-40-red)


---
[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2205.11916) Ôºà**2022.05.24**Ôºâ

<font color="gray">Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-185-red)


---
[**Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**](https://doi.org/10.48550/arXiv.2205.10625) Ôºà**2022.05.21**Ôºâ

<font color="gray">Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-85-red)


---
[**Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning**](https://doi.org/10.48550/arXiv.2205.09712) Ôºà**2022.05.19**Ôºâ

<font color="gray">Antonia Creswell, M. Shanahan, I. Higgins .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-38-red)


---
[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2204.02311) Ôºà**2022.04.05**Ôºâ

<font color="gray">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-612-red)


---
[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) Ôºà**2022.04.05**Ôºâ

<font color="gray">Andrew Kyle Lampinen, I. Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/cite-61-red)


---
[**Training language models to follow instructions with human feedback**](https://doi.org/10.48550/arXiv.2203.02155) Ôºà**2022.03.04**Ôºâ

<font color="gray">Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-426-red)


---
[**Chain of Thought Prompting Elicits Reasoning in Large Language Models**](https://arxiv.org/abs/2201.11903) Ôºà**2022.01.28**Ôºâ

<font color="gray">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-384-red)


---
[**Show Your Work: Scratchpads for Intermediate Computation with Language Models**](https://arxiv.org/abs/2112.00114) Ôºà**2021.11.30**Ôºâ

<font color="gray">Maxwell Nye, Anders Andreassen, Guy Gur-Ari, H. Michalewski, Jacob Austin, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-121-red)


---
[**Few-Shot Self-Rationalization with Natural Language Prompts**](https://doi.org/10.18653/v1/2022.findings-naacl.31) Ôºà**2021.11.16**Ôºâ

<font color="gray">Ana Marasoviƒá, Iz Beltagy, Doug Downey, Matthew E. Peters .  - „ÄêNAACL-HLT„Äë</font>

![](https://img.shields.io/badge/cite-29-red)


---
[**Training Verifiers to Solve Math Word Problems**](https://arxiv.org/abs/2110.14168) Ôºà**2021.10.27**Ôºâ

<font color="gray">Karl Cobbe, V. Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-175-red)


---
[**Evaluating Large Language Models Trained on Code**](https://arxiv.org/abs/2107.03374) Ôºà**2021.07.07**Ôºâ

<font color="gray">Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-604-red)


---
[**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity**](https://doi.org/10.18653/v1/2022.acl-long.556) Ôºà**2021.04.18**Ôºâ

<font color="gray">Yao Lu, Max Bartolo, Alastair Moore, S. Riedel, Pontus Stenetorp .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-168-red)


---
[**Calibrate Before Use: Improving Few-Shot Performance of Language Models**](https://arxiv.org/abs/2102.09690) Ôºà**2021.02.19**Ôºâ

<font color="gray">Tony Zhao, Eric Wallace, Shi Feng, D. Klein, Sameer Singh .  - „ÄêInternational Conference on Machine Learning„Äë</font>

![](https://img.shields.io/badge/cite-281-red)


---
[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) Ôºà**2021.01.01**Ôºâ

<font color="gray">Tianyu Gao, Adam Fisch, Danqi Chen .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-642-red)


---
[**Active Prompting with Chain-of-Thought for Large Language Models**](https://doi.org/10.48550/arXiv.2302.12246) 



![](https://img.shields.io/badge/cite-0-red)


---
[**Multimodal Chain-of-Thought Reasoning in Language Models**]

<font color="gray">Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, etc </font>

![](https://img.shields.io/badge/cite-0-red)


---
[**Faithful Chain-of-Thought Reasoning**](https://api.semanticscholar.org/ea0688f9e7dfb0d3c2249486af65209c25809544) 



![](https://img.shields.io/badge/cite-0-red)


---
[**Language Models are Unsupervised Multitask Learners**](https://api.semanticscholar.org/9405cc0d6169988371b2755e573cc28650d14dfe) 

<font color="gray">Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, etc </font>

![](https://img.shields.io/badge/cite-8878-red)


</div>

# CONTINUE...
