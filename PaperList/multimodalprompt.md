# üìÑ Multimodal Prompt

## Paper List

<div style="line-height:0.2em;">


[**BRAVE: Broadening the visual encoding of vision-language models**](https://arxiv.org/abs/2404.07204) Ôºà**2024.04.10**Ôºâ

<font color="gray">Ouguzhan Fatih Kar, A. Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling**](https://arxiv.org/abs/2404.07031) Ôºà**2024.04.10**Ôºâ

<font color="gray">Ege Ozsoy, Chantal Pellegrini, Matthias Keicher, Nassir Navab </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation**](https://arxiv.org/abs/2404.05674) Ôºà**2024.04.08**Ôºâ

<font color="gray">Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, A. Elgammal, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)  [![](https://img.shields.io/badge/Github%20Stars-144-blue)](https://github.com/bytedance/MoMA)

---

[**Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs**](https://arxiv.org/abs/2404.05719) Ôºà**2024.04.08**Ôºâ

<font color="gray">Keen You, Haotian Zhang, E. Schoop, Floris Weers, Amanda Swearngin, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-57-red)

---

[**MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens**](https://arxiv.org/abs/2404.03413) Ôºà**2024.04.04**Ôºâ

<font color="gray">Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ViTamin: Designing Scalable Vision Models in the Vision-Language Era**](https://arxiv.org/abs/2404.02132) Ôºà**2024.04.02**Ôºâ

<font color="gray">Jienneg Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-16-red)  [![](https://img.shields.io/badge/Github%20Stars-147-blue)](https://github.com/beckschen/vitamin)

---

[**Segment Any 3D Object with Language**](https://arxiv.org/abs/2404.02157) Ôºà**2024.04.02**Ôºâ

<font color="gray">Seungjun Lee, Yuyang Zhao, Gim Hee Lee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)

---

[**Iterated Learning Improves Compositionality in Large Vision-Language Models**](https://arxiv.org/abs/2404.02145) Ôºà**2024.04.02**Ôºâ

<font color="gray">Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna </font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models**](https://doi.org/10.48550/arXiv.2403.18814) Ôºà**2024.03.27**Ôºâ

<font color="gray">Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models**](https://arxiv.org/abs/2403.16999) Ôºà**2024.03.25**Ôºâ

<font color="gray">Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning**](https://arxiv.org/abs/2403.14616) Ôºà**2024.03.21**Ôºâ

<font color="gray">Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MyVLM: Personalizing VLMs for User-Specific Queries**](https://arxiv.org/abs/2403.14599) Ôºà**2024.03.21**Ôºâ

<font color="gray">Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?**](https://arxiv.org/abs/2403.14624) Ôºà**2024.03.21**Ôºâ

<font color="gray">Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**](https://arxiv.org/abs/2403.14598) Ôºà**2024.03.21**Ôºâ

<font color="gray">Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models**](https://arxiv.org/abs/2403.13263) Ôºà**2024.03.20**Ôºâ

<font color="gray">Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?**](https://arxiv.org/abs/2403.09037) Ôºà**2024.03.14**Ôºâ

<font color="gray">Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/qinyu-allen-zhao/lvlm-lp)

---

[**3D-VLA: A 3D Vision-Language-Action Generative World Model**](https://arxiv.org/abs/2403.09631) Ôºà**2024.03.14**Ôºâ

<font color="gray">Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, etc </font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**UniCode: Learning a Unified Codebook for Multimodal Large Language Models**](https://arxiv.org/abs/2403.09072) Ôºà**2024.03.14**Ôºâ

<font color="gray">Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**DeepSeek-VL: Towards Real-World Vision-Language Understanding**](https://arxiv.org/abs/2403.05525) Ôºà**2024.03.08**Ôºâ

<font color="gray">Haoyu Lu, Wen Liu, Bo Zhang, Bing-Li Wang, Kai Dong, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-61-red)  [![](https://img.shields.io/badge/Github%20Stars-1.9k-blue)](https://github.com/deepseek-ai/deepseek-vl)

---

[**VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model**](https://arxiv.org/abs/2403.05346) Ôºà**2024.03.08**Ôºâ

<font color="gray">Junsu Kim, Yunhoe Ku, Jihyeon Kim, Junuk Cha, Seungryul Baek </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document**](https://arxiv.org/abs/2403.04473) Ôºà**2024.03.07**Ôºâ

<font color="gray">Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models**](https://arxiv.org/abs/2403.03003) Ôºà**2024.03.05**Ôºâ

<font color="gray">Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)  [![](https://img.shields.io/badge/Github%20Stars-188-blue)](https://github.com/luogen1996/llava-hr)

---

[**RegionGPT: Towards Region Understanding Vision Language Model**](https://arxiv.org/abs/2403.02330) Ôºà**2024.03.04**Ôºâ

<font color="gray">Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, etc </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Non-autoregressive Sequence-to-Sequence Vision-Language Models**](https://arxiv.org/abs/2403.02249) Ôºà**2024.03.04**Ôºâ

<font color="gray">Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, S. Soatto </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers**](https://arxiv.org/abs/2402.19479) Ôºà**2024.02.29**Ôºâ

<font color="gray">Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, etc </font>

![](https://img.shields.io/badge/Citations-3-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-32-red)  [![](https://img.shields.io/badge/Github%20Stars-435-blue)](https://github.com/snap-research/panda-70m)

---

[**Tower: An Open Multilingual Large Language Model for Translation-Related Tasks**](https://doi.org/10.48550/arXiv.2402.17733) Ôºà**2024.02.27**Ôºâ

<font color="gray">Duarte M. Alves, Jos√© P. Pombal, Nuno M. Guerreiro, Pedro H. Martins, Joao Alves, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-498-blue)](https://github.com/epfllm/megatron-llm)

---

[**ShapeLLM: Universal 3D Object Understanding for Embodied Interaction**](https://doi.org/10.48550/arXiv.2402.17766) Ôºà**2024.02.27**Ôºâ

<font color="gray">Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**VRP-SAM: SAM with Visual Reference Prompt**](https://doi.org/10.48550/arXiv.2402.17726) Ôºà**2024.02.27**Ôºâ

<font color="gray">Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**GROUNDHOG: Grounding Large Language Models to Holistic Segmentation**](https://arxiv.org/abs/2402.16846) Ôºà**2024.02.26**Ôºâ

<font color="gray">Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Genie: Generative Interactive Environments**](https://arxiv.org/abs/2402.15391) Ôºà**2024.02.23**Ôºâ

<font color="gray">Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LLMBind: A Unified Modality-Task Integration Framework**](https://doi.org/10.48550/arXiv.2402.14891) Ôºà**2024.02.22**Ôºâ

<font color="gray">Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/PKU-YuanGroup/LLMBind)

---

[**How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts**](https://arxiv.org/abs/2402.13220) Ôºà**2024.02.20**Ôºâ

<font color="gray">Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)  [![](https://img.shields.io/badge/Github%20Stars-8-blue)](https://github.com/qinyu-allen-zhao/lvlm-lp)

---

[**Video ReCap: Recursive Captioning of Hour-Long Videos**](https://arxiv.org/abs/2402.13250) Ôºà**2024.02.20**Ôºâ

<font color="gray">Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-20-red)  [![](https://img.shields.io/badge/Github%20Stars-144-blue)](https://github.com/md-mohaiminul/VideoRecap)

---

[**SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models**](https://doi.org/10.48550/arXiv.2402.05935) Ôºà**2024.02.08**Ôºâ

<font color="gray">Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-2.6k-blue)](https://github.com/alpha-vllm/llama2-accessory)

---

[**Binding Touch to Everything: Learning Unified Multimodal Tactile Representations**](https://doi.org/10.48550/arXiv.2401.18084) Ôºà**2024.01.31**Ôºâ

<font color="gray">Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**](https://doi.org/10.48550/arXiv.2401.16420) Ôºà**2024.01.29**Ôºâ

<font color="gray">Xiao-wen Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-1.9k-blue)](https://github.com/internlm/internlm-xcomposer)

---

[**Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities**](https://doi.org/10.48550/arXiv.2401.14405) Ôºà**2024.01.25**Ôºâ

<font color="gray">Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-83-blue)](https://github.com/ailab-cvc/m2pt)

---

[**MM-LLMs: Recent Advances in MultiModal Large Language Models**](https://doi.org/10.48550/arXiv.2401.13601) Ôºà**2024.01.24**Ôºâ

<font color="gray">Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models**](https://doi.org/10.48550/arXiv.2401.03105) Ôºà**2024.01.06**Ôºâ

<font color="gray">Xin He, Longhui Wei, Lingxi Xie, Qi Tian .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Learning to Prompt with Text Only Supervision for Vision-Language Models**](https://doi.org/10.48550/arXiv.2401.02418) Ôºà**2024.01.04**Ôºâ

<font color="gray">Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, L. V. Gool, F. Tombari .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Instruct-Imagen: Image Generation with Multi-modal Instruction**](https://doi.org/10.48550/arXiv.2401.01952) Ôºà**2024.01.03**Ôºâ

<font color="gray">Hexiang Hu, Kelvin C.K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model**](https://arxiv.org/abs/2312.11370) Ôºà**2023.12.18**Ôºâ

<font color="gray">Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-49-blue)](https://github.com/pipilurj/g-llava)

---

[**Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects**](https://doi.org/10.48550/arXiv.2312.05278) Ôºà**2023.12.08**Ôºâ

<font color="gray">Junyu Lu, Ruyi Gan, Di Zhang, Xiaojun Wu, Ziwei Wu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models**](https://arxiv.org/abs/2312.02949) Ôºà**2023.12.05**Ôºâ

<font color="gray">Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)  [![](https://img.shields.io/badge/Github%20Stars-90-blue)](https://github.com/ux-decoder/llava-grounding)

---

[**Sequential Modeling Enables Scalable Learning for Large Vision Models**](https://arxiv.org/abs/2312.00785) Ôºà**2023.12.01**Ôºâ

<font color="gray">Yutong Bai, Xinyang Geng, K. Mangalam, Amir Bar, Alan Yuille, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-161-red)  [![](https://img.shields.io/badge/Github%20Stars-1.4k-blue)](https://github.com/ytongbai/LVM)

---

[**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://doi.org/10.48550/arXiv.2311.17043) Ôºà**2023.11.28**Ôºâ

<font color="gray">Yanwei Li, Chengyao Wang, Jiaya Jia .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-349-blue)](https://github.com/dvlab-research/llama-vid)

---

[**MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers**](https://doi.org/10.48550/arXiv.2311.15475) Ôºà**2023.11.27**Ôºâ

<font color="gray">Yawar Siddiqui, A. Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**An Embodied Generalist Agent in 3D World**](https://doi.org/10.48550/arXiv.2311.12871) Ôºà**2023.11.18**Ôºâ

<font color="gray">Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-282-blue)](https://github.com/embodied-generalist/embodied-generalist)

---

[**Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning**](https://doi.org/10.48550/arXiv.2311.10709) Ôºà**2023.11.17**Ôºâ

<font color="gray">Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, S. Azadi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning**](https://doi.org/10.48550/arXiv.2311.10537) Ôºà**2023.11.16**Ôºâ

<font color="gray">Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](https://doi.org/10.48550/arXiv.2311.08046) Ôºà**2023.11.14**Ôºâ

<font color="gray">Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, Li Yuan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images**](https://doi.org/10.48550/arXiv.2311.06400) Ôºà**2023.11.10**Ôºâ

<font color="gray">Yinsong Xu, Jiaqi Tang, Aidong Men, Qingchao Chen .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model**](https://doi.org/10.48550/arXiv.2311.05348) Ôºà**2023.11.09**Ôºâ

<font color="gray">Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Yanchun Xie, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-120-blue)](https://github.com/OPPOMKLab/u-LLaVA)

---

[**Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges**](https://doi.org/10.48550/arXiv.2311.03287) Ôºà**2023.11.06**Ôºâ

<font color="gray">Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)

---

[**Levels of AGI: Operationalizing Progress on the Path to AGI**](https://doi.org/10.48550/arXiv.2311.02462) Ôºà**2023.11.04**Ôºâ

<font color="gray">Meredith Ringel Morris, Jascha Narain Sohl-Dickstein, Noah Fiedel, T. Warkentin, Allan Dafoe, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Woodpecker: Hallucination Correction for Multimodal Large Language Models**](https://arxiv.org/abs/2310.16045) Ôºà**2023.10.24**Ôºâ

<font color="gray">Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-47-red)  [![](https://img.shields.io/badge/Github%20Stars-572-blue)](https://github.com/bradyfu/woodpecker)

---

[**3D-GPT: Procedural 3D Modeling with Large Language Models**](https://doi.org/10.48550/arXiv.2310.12945) Ôºà**2023.10.19**Ôºâ

<font color="gray">Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation**](https://doi.org/10.48550/arXiv.2310.10586) Ôºà**2023.10.16**Ôºâ

<font color="gray">Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens**](https://doi.org/10.48550/arXiv.2310.02239) Ôºà**2023.10.03**Ôºâ

<font color="gray">Kaizhi Zheng, Xuehai He, Xin Eric Wang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Kosmos-2.5: A Multimodal Literate Model**](https://doi.org/10.48550/arXiv.2309.11419) Ôºà**2023.09.20**Ôºâ

<font color="gray">Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Investigating the Catastrophic Forgetting in Multimodal Large Language Models**](https://doi.org/10.48550/arXiv.2309.10313) Ôºà**2023.09.19**Ôºâ

<font color="gray">Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Physically Grounded Vision-Language Models for Robotic Manipulation**](https://arxiv.org/abs/2309.02561) Ôºà**2023.09.05**Ôºâ

<font color="gray">Jensen Gao, Bidipta Sarkar, F. Xia, Ted Xiao, Jiajun Wu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-39-red)

---

[**Physically Grounded Vision-Language Models for Robotic Manipulation**](https://doi.org/10.48550/arXiv.2309.02561) Ôºà**2023.09.05**Ôºâ

<font color="gray">Jensen Gao, Bidipta Sarkar, F. Xia, Ted Xiao, Jiajun Wu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**Point-Bind&Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following**](https://arxiv.org/abs/2309.00615) Ôºà**2023.09.01**Ôºâ

<font color="gray">Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-7-red)  [![](https://img.shields.io/badge/Github%20Stars-110-blue)](https://github.com/ziyuguo99/point-bind_point-llm)

---

[**PointLLM: Empowering Large Language Models to Understand Point Clouds**](https://doi.org/10.48550/arXiv.2308.16911) Ôºà**2023.08.31**Ôºâ

<font color="gray">Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-14-green)  [![](https://img.shields.io/badge/Github%20Stars-422-blue)](https://github.com/openrobotlab/pointllm)

---

[**PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation**](https://doi.org/10.48550/arXiv.2308.13746) Ôºà**2023.08.26**Ôºâ

<font color="gray">Ao Chang, Xing Tao, Xin Yang, Yuhao Huang, Xinrui Zhou, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SeamlessM4T-Massively Multilingual & Multimodal Machine Translation**](https://doi.org/10.48550/arXiv.2308.11596) Ôºà**2023.08.22**Ôºâ

<font color="gray">Seamless Communication, Lo√Øc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-10.5k-blue)](https://github.com/facebookresearch/seamless_communication)

---

[**Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes**](https://doi.org/10.48550/arXiv.2308.08769) Ôºà**2023.08.17**Ôºâ

<font color="gray">Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)

---

[**VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use**](https://doi.org/10.48550/arXiv.2308.06595) Ôºà**2023.08.12**Ôºâ

<font color="gray">Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-45-blue)](https://github.com/mlfoundations/VisIT-Bench)

---

[**3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment**](https://doi.org/10.48550/arXiv.2308.04352) Ôºà**2023.08.08**Ôºâ

<font color="gray">Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-7-green)

---

[**UniVTG: Towards Unified Video-Language Temporal Grounding**](https://doi.org/10.48550/arXiv.2307.16715) Ôºà**2023.07.31**Ôºâ

<font color="gray">Kevin Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-232-blue)](https://github.com/showlab/univtg)

---

[**Med-Flamingo: a Multimodal Medical Few-shot Learner**](https://doi.org/10.48550/arXiv.2307.15189) Ôºà**2023.07.27**Ôºâ

<font color="gray">Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, C. Zakka, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-367-blue)](https://github.com/snap-stanford/med-flamingo)

---

[**OBJECT 3DIT: Language-guided 3D-aware Image Editing**](https://doi.org/10.48550/arXiv.2307.11073) Ôºà**2023.07.20**Ôºâ

<font color="gray">Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Brain2Music: Reconstructing Music from Human Brain Activity**](https://doi.org/10.48550/arXiv.2307.11078) Ôºà**2023.07.20**Ôºâ

<font color="gray">Timo I. Denk, Yu Takagi, Takuya Matsuyama, A. Agostinelli, Tomoya Nakai, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs**](https://doi.org/10.48550/arXiv.2307.10490) Ôºà**2023.07.19**Ôºâ

<font color="gray">Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots**](https://arxiv.org/abs/2307.08715) Ôºà**2023.07.16**Ôºâ

<font color="gray">Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, etc </font>

![](https://img.shields.io/badge/Citations-13-green)

---

[**HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models**](https://arxiv.org/abs/2307.06949) Ôºà**2023.07.13**Ôºâ

<font color="gray">Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-70-red)  [![](https://img.shields.io/badge/Github%20Stars-158-blue)](https://github.com/JiauZhang/hyperdreambooth)

---

[**Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels**](https://doi.org/10.48550/arXiv.2307.01969) Ôºà**2023.07.05**Ôºâ

<font color="gray">Bang Yang, Fenglin Liu, Zheng Li, Qingyu Yin, Chenyu You, etc .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Multimodal Prompt Retrieval for Generative Visual Question Answering**](https://doi.org/10.48550/arXiv.2306.17675) Ôºà**2023.06.30**Ôºâ

<font color="gray">Timothy Ossowski, Junjie Hu .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs**](https://doi.org/10.48550/arXiv.2306.17842) Ôºà**2023.06.30**Ôºâ

<font color="gray">Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-400-blue)](https://github.com/google-research/magvit)

---

[**Multimodal Prompt Learning in Emotion Recognition Using Context and Audio Information**](https://doi.org/10.3390/math11132908) Ôºà**2023.06.28**Ôºâ

<font color="gray">Eunseo Jeong, Gyu-Min Kim, Sangwoo Kang .  - „ÄêMathematics„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023**](https://doi.org/10.48550/arXiv.2306.16545) Ôºà**2023.06.28**Ôºâ

<font color="gray">Daoji Huang, Otmar Hilliges, L. Gool, Xi Wang .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/dandoge/palm)

---

[**Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**](https://doi.org/10.48550/arXiv.2306.15195) Ôºà**2023.06.27**Ôºâ

<font color="gray">Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-710-blue)](https://github.com/shikras/shikra)

---

[**DocEdit: Language-Guided Document Editing**](https://doi.org/10.1609/aaai.v37i2.25282) Ôºà**2023.06.26**Ôºâ

<font color="gray">Puneet Mathur, R. Jain, Jiuxiang Gu, Franck Dernoncourt, Dinesh Manocha, etc .  - „ÄêAAAI Conference on Artificial Intelligence„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**PromptIR: Prompting for All-in-One Blind Image Restoration**](https://arxiv.org/abs/2306.13090) Ôºà**2023.06.22**Ôºâ

<font color="gray">Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, F. Khan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/va1shn9v/promptir)

---

[**Unleashing the AI revolution: exploring the capabilities and challenges of large language models and text‚Äêto‚Äêimage AI programs**](https://doi.org/10.1002/uog.26297) Ôºà**2023.06.17**Ôºâ

<font color="gray">A. Youssef .  - „ÄêUltrasound in Obstetrics and Gynecology„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://doi.org/10.48550/arXiv.2306.09093) Ôºà**2023.06.15**Ôºâ

<font color="gray">Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/lyuchenyang/macaw-llm)

---

[**Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models**](https://doi.org/10.48550/arXiv.2306.08641) Ôºà**2023.06.14**Ôºâ

<font color="gray">Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Kaifeng Bi, Xiaotao Gu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://doi.org/10.48550/arXiv.2306.02858) Ôºà**2023.06.05**Ôºâ

<font color="gray">Han Zhang, Xin Li, Lidong Bing .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/damo-nlp-sg/video-llama)

---

[**HeadSculpt: Crafting 3D Head Avatars with Text**](https://doi.org/10.48550/arXiv.2306.03038) Ôºà**2023.06.05**Ôºâ

<font color="gray">Xiaoping Han, Yukang Cao, K. Han, Xiatian Zhu, Jiankang Deng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)

---

[**Meta-Learning For Vision-and-Language Cross-lingual Transfer**](https://arxiv.org/abs/2305.14843) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hanxu Hu, Frank Keller </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**LayoutGPT: Compositional Visual Planning and Generation with Large Language Models**](https://arxiv.org/abs/2305.15393) Ôºà**2023.05.24**Ôºâ

<font color="gray">Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-41-red)  [![](https://img.shields.io/badge/Github%20Stars-264-blue)](https://github.com/weixi-feng/layoutgpt)

---

[**Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models**](https://arxiv.org/abs/2305.15080) Ôºà**2023.05.24**Ôºâ

<font color="gray">Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)

---

[**EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought**](https://arxiv.org/abs/2305.15021) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-80-red)

---

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-3.4k-blue)](https://github.com/microsoft/lmops)

---

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

<font color="gray">Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks**](https://arxiv.org/abs/2305.14201) Ôºà**2023.05.23**Ôºâ

<font color="gray">Tiedong Liu, Bryan Kian Hsiang Low </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-66-red)  [![](https://img.shields.io/badge/Github%20Stars-173-blue)](https://github.com/liutiedong/goat)

---

[**Masked Path Modeling for Vision-and-Language Navigation**](https://doi.org/10.48550/arXiv.2305.14268) Ôºà**2023.05.23**Ôºâ

<font color="gray">Zi-Yi Dou, Feng Gao, Nanyun Peng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**](https://arxiv.org/abs/2305.14323) Ôºà**2023.05.23**Ôºâ

<font color="gray">Z. Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-22-red)  [![](https://img.shields.io/badge/Github%20Stars-32-blue)](https://github.com/rucaibox/chatcot)

---

[**LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space**](https://arxiv.org/abs/2305.12798) Ôºà**2023.05.22**Ôºâ

<font color="gray">Chi Han, Jialiang Xu, Manling Li, Y. Fung, Chenkai Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

---

[**Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer**](https://arxiv.org/abs/2305.12761) Ôºà**2023.05.22**Ôºâ

<font color="gray">Shuang Li, Xuming Hu, Aiwei Liu, Yawen Yang, Fukun Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/thu-bpm/softmv)

---

[**A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches**](https://arxiv.org/abs/2305.12749) Ôºà**2023.05.22**Ôºâ

<font color="gray">Zihan Wang, Tianle Wang, Dheeraj Mekala, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/zihanwangki/x-tc)

---

[**Enhance Reasoning Ability of Visual-Language Models via Large Language Models**](https://arxiv.org/abs/2305.13267) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yueting Yang, Xintong Zhang, Wenjuan Han </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending**](https://doi.org/10.48550/arXiv.2305.13167) Ôºà**2023.05.22**Ôºâ

<font color="gray">Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**InstructVid2Vid: Controllable Video Editing with Natural Language Instructions**](https://arxiv.org/abs/2305.12328) Ôºà**2023.05.21**Ôºâ

<font color="gray">Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**](https://arxiv.org/abs/2305.12295) Ôºà**2023.05.20**Ôºâ

<font color="gray">Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-74-red)  [![](https://img.shields.io/badge/Github%20Stars-198-blue)](https://github.com/teacherpeterpan/logic-llm)

---

[**LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4**](https://arxiv.org/abs/2305.12147) Ôºà**2023.05.20**Ôºâ

<font color="gray">Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) Ôºà**2023.05.19**Ôºâ

<font color="gray">IokTong Lei, ZhiDong Deng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning**](https://doi.org/10.48550/arXiv.2305.11759) Ôºà**2023.05.19**Ôºâ

<font color="gray">Mustafa Safa Ozdayi, Charith S. Peris, Jack G. M. FitzGerald, Christophe Dupuy, Jimit Majmudar, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-24-blue)](https://github.com/amazon-science/controlling-llm-memorization)

---

[**RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought**](https://doi.org/10.48550/arXiv.2305.11499) Ôºà**2023.05.19**Ôºâ

<font color="gray">Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding**](https://doi.org/10.48550/arXiv.2305.11497) Ôºà**2023.05.19**Ôºâ

<font color="gray">Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, Long Chen .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Efficient Prompting via Dynamic In-Context Learning**](https://doi.org/10.48550/arXiv.2305.11170) Ôºà**2023.05.18**Ôºâ

<font color="gray">Wangchunshu Zhou, Yuchen Jiang, Ryan Cotterell, Mrinmaya Sachan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers**](https://arxiv.org/abs/2305.07185) Ôºà**2023.05.12**Ôºâ

<font color="gray">L. Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-62-red)

---

[**Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models**](https://doi.org/10.48550/arXiv.2305.04441) Ôºà**2023.05.08**Ôºâ

<font color="gray">Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting**](https://doi.org/10.48550/arXiv.2305.03902) Ôºà**2023.05.06**Ôºâ

<font color="gray">Xiaoyuan Guo, Xiang Wei, Q. Su, Hui-Huang Zhao, Shunli Zhan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Edit Everything: A Text-Guided Generative System for Images Editing**](https://arxiv.org/abs/2304.14006) Ôºà**2023.04.27**Ôºâ

<font color="gray">Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-17-blue)](https://github.com/defengxie/edit_everything)

---

[**ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**](https://arxiv.org/abs/2304.14407) Ôºà**2023.04.27**Ôºâ

<font color="gray">Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-15-red)

---

[**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178) Ôºà**2023.04.27**Ôºâ

<font color="gray">Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-34-red)  [![](https://img.shields.io/badge/Github%20Stars-875-blue)](https://github.com/x-plug/mplug-owl)

---

[**Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**](https://doi.org/10.48550/arXiv.2304.09337) Ôºà**2023.04.18**Ôºâ

<font color="gray">Stephen Brade, Bryan Wang, Maur√≠cio Sousa, Sageev Oore, Tovi Grossman .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Towards Robust Prompts on Vision-Language Models**](https://arxiv.org/abs/2304.08479) Ôºà**2023.04.17**Ôºâ

<font color="gray">Jindong Gu, A. Beirami, Xuezhi Wang, Alex Beutel, Philip H. S. Torr, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Visual Instruction Tuning**](https://arxiv.org/abs/2304.08485) Ôºà**2023.04.17**Ôºâ

<font color="gray">Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-126-red)  [![](https://img.shields.io/badge/Github%20Stars-2.8k-blue)](https://github.com/haotian-liu/LLaVA)

---

[**Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text**](https://arxiv.org/abs/2304.06939) Ôºà**2023.04.14**Ôºâ

<font color="gray">Wanrong Zhu, Jack Hessel, Anas Awadalla, S. Gadre, Jesse Dodge, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-53-red)  [![](https://img.shields.io/badge/Github%20Stars-815-blue)](https://github.com/allenai/mmc4)

---

[**Segment Everything Everywhere All at Once**](https://doi.org/10.48550/arXiv.2304.06718) Ôºà**2023.04.13**Ôºâ

<font color="gray">Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-2.7k-blue)](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once)

---

[**Efficient Multimodal Fusion via Interactive Prompting**](https://arxiv.org/abs/2304.06306) Ôºà**2023.04.13**Ôºâ

<font color="gray">Yaowei Li, Ruijie Quan, Linchao Zhu, Yezhou Yang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning**](https://arxiv.org/abs/2304.05613) Ôºà**2023.04.12**Ôºâ

<font color="gray">Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-23-red)

---

[**Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition**](https://doi.org/10.48550/arXiv.2304.04704) Ôºà**2023.04.10**Ôºâ

<font color="gray">Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-172-blue)](https://github.com/amazon-science/prompt-pretraining)

---

[**Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions**](https://doi.org/10.48550/arXiv.2304.04227) Ôºà**2023.04.09**Ôºâ

<font color="gray">Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-412-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting**](https://arxiv.org/abs/2304.03307) Ôºà**2023.04.06**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-60-red)  [![](https://img.shields.io/badge/Github%20Stars-100-blue)](https://github.com/talalwasim/vita-clip)

---

[**TagGPT: Large Language Models are Zero-shot Multimodal Taggers**](https://arxiv.org/abs/2304.03022) Ôºà**2023.04.06**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-22-blue)](https://github.com/tencentarc/taggpt)

---

[**Segment Anything**](https://doi.org/10.48550/arXiv.2304.02643) Ôºà**2023.04.05**Ôºâ

<font color="gray">A. Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-45.4k-blue)](https://github.com/facebookresearch/segment-anything)

---

[**TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs**](https://doi.org/10.48550/arXiv.2303.16434) Ôºà**2023.03.29**Ôºâ

<font color="gray">Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-8-green)

---

[**MEDIMP: Medical Images and Prompts for renal transplant representation learning**](https://arxiv.org/abs/2303.12445) Ôºà**2023.03.22**Ôºâ

<font color="gray">Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

---

[**CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition**](https://arxiv.org/abs/2303.11313) Ôºà**2023.03.20**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-41-red)  [![](https://img.shields.io/badge/Github%20Stars-209-blue)](https://github.com/deeptibhegde/clip-goes-3d)

---

[**MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://doi.org/10.48550/arXiv.2303.11381) Ôºà**2023.03.20**Ôºâ

<font color="gray">Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-674-blue)](https://github.com/microsoft/MM-REACT)

---

[**Visual Prompt Multi-Modal Tracking**](https://arxiv.org/abs/2303.10826) Ôºà**2023.03.20**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)  [![](https://img.shields.io/badge/Github%20Stars-109-blue)](https://github.com/jiawen-zhu/vipt)

---

[**Audio Visual Language Maps for Robot Navigation**](https://doi.org/10.48550/arXiv.2303.07522) Ôºà**2023.03.13**Ôºâ

<font color="gray">Chen Huang, Oier Mees, Andy Zeng, W. Burgard .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions**](https://doi.org/10.48550/arXiv.2303.06594) Ôºà**2023.03.12**Ôºâ

<font color="gray">Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-447-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Text-Visual Prompting for Efficient 2D Temporal Video Grounding**](https://doi.org/10.1109/CVPR52729.2023.01421) Ôºà**2023.03.09**Ôºâ

<font color="gray">Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding .  - „ÄêComputer Vision and Pattern Recognition„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-47-red)  [![](https://img.shields.io/badge/Github%20Stars-12-blue)](https://github.com/intel/TVP)

---

[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/abs/2303.04671) Ôºà**2023.03.08**Ôºâ

<font color="gray">Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-482-red)  [![](https://img.shields.io/badge/Github%20Stars-34.4k-blue)](https://github.com/microsoft/visual-chatgpt)

---

[**Multimodal Parameter-Efficient Few-Shot Class Incremental Learning**](https://doi.org/10.48550/arXiv.2303.04751) Ôºà**2023.03.08**Ôºâ

<font color="gray">Marco D‚ÄôAlessandro, Alberto Alonso, Enrique Calabr'es, M. Galar .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning**](https://arxiv.org/abs/2303.02861) Ôºà**2023.03.06**Ôºâ

<font color="gray">Zhen Wang, R. Panda, Leonid Karlinsky, R. Feris, Huan Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-37-red)

---

[**Multimodal Prompting with Missing Modalities for Visual Recognition**](https://doi.org/10.48550/arXiv.2303.03369) Ôºà**2023.03.06**Ôºâ

<font color="gray">Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, Chen-Yu Lee .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-59-blue)](https://github.com/yilunlee/missing_aware_prompts)

---

[**Multimodal Chain-of-Thought Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2302.00923) Ôºà**2023.02.02**Ôºâ

<font color="gray">Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-6-green)  [![](https://img.shields.io/badge/Github%20Stars-3.3k-blue)](https://github.com/amazon-science/mm-cot)

---

[**LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation**](https://doi.org/10.48550/arXiv.2210.15461) Ôºà**2022.10.19**Ôºâ

<font color="gray">Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**CoHOZ: Contrasive Multimodal prompt Tuning for Hierarchical Open-set Zero-shot Recognition**](https://doi.org/10.1145/3503161.3548021) Ôºà**2022.10.10**Ôºâ

<font color="gray">Ning Liao, Yifeng Liu, Li Xiaobo, Chenyi Lei, Guoxin Wang, etc .  - „ÄêProceedings of the 30th ACM International Conference on Multimedia„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**VIMA: General Robot Manipulation with Multimodal Prompts**](https://doi.org/10.48550/arXiv.2210.03094) Ôºà**2022.10.06**Ôºâ

<font color="gray">Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-15-green)  [![](https://img.shields.io/badge/Github%20Stars-106-blue)](https://github.com/vimalabs/VIMABench)

---

[**Learning to Prompt for Vision-Language Models**](https://doi.org/10.1007/s11263-022-01653-1) Ôºà**2022.09.01**Ôºâ

<font color="gray">Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-491-red)  [![](https://img.shields.io/badge/Github%20Stars-932-blue)](https://github.com/kaiyangzhou/coop)

---

[**Visual Prompt Tuning**](https://doi.org/10.48550/arXiv.2203.12119) Ôºà**2022.03.23**Ôºâ

<font color="gray">Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, S. Belongie, etc .  - „ÄêEuropean Conference on Computer Vision„Äë</font>

![](https://img.shields.io/badge/Citations-104-green)  [![](https://img.shields.io/badge/Github%20Stars-518-blue)](https://github.com/KMnP/vpt)

---

[**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) Ôºà**2021.06.25**Ôºâ

<font color="gray">Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. Eslami, Oriol Vinyals, etc .  - „ÄêNeural Information Processing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-173-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-531-red)

---

[**MPT: Multimodal Prompt Tuning for Event Detection**](https://api.semanticscholar.org/04b49c9356aed1be7b2b4aa5dccd0cfce31c416d) 



![](https://img.shields.io/badge/Citations-0-green)

---

[**Similarity-Aware Multimodal Prompt Learning for Fake News Detection**](https://doi.org/10.2139/ssrn.4347542) 

<font color="gray">Ye Jiang, Xiaomin Yu, Yimin Wang, Xiaoman Xu, Xingyi Song, etc .  - „ÄêSSRN Electronic Journal„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)


</div>

# CONTINUE...