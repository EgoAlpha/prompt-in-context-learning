# üìÑ Multimodal Prompt

## Paper List

<div style="line-height:0.2em;">


[**HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models**](https://arxiv.org/abs/2307.06949) Ôºà**2023.07.13**Ôºâ

<font color="gray">Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-24-red)

---

[**SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs**](https://doi.org/10.48550/arXiv.2306.17842) Ôºà**2023.06.30**Ôºâ

<font color="gray">Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-400-blue)](https://github.com/google-research/magvit)

---

[**Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**](https://doi.org/10.48550/arXiv.2306.15195) Ôºà**2023.06.27**Ôºâ

<font color="gray">Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-111-blue)](https://github.com/shikras/shikra)

---

[**PromptIR: Prompting for All-in-One Blind Image Restoration**](https://arxiv.org/abs/2306.13090) Ôºà**2023.06.22**Ôºâ

<font color="gray">Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, F. Khan </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/va1shn9v/promptir)

---

[**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://doi.org/10.48550/arXiv.2306.09093) Ôºà**2023.06.15**Ôºâ

<font color="gray">Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-979-blue)](https://github.com/lyuchenyang/macaw-llm)

---

[**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://doi.org/10.48550/arXiv.2306.02858) Ôºà**2023.06.05**Ôºâ

<font color="gray">Han Zhang, Xin Li, Lidong Bing .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/damo-nlp-sg/video-llama)

---

[**Meta-Learning For Vision-and-Language Cross-lingual Transfer**](https://arxiv.org/abs/2305.14843) Ôºà**2023.05.24**Ôºâ

<font color="gray">Hanxu Hu, Frank Keller </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LayoutGPT: Compositional Visual Planning and Generation with Large Language Models**](https://arxiv.org/abs/2305.15393) Ôºà**2023.05.24**Ôºâ

<font color="gray">Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models**](https://arxiv.org/abs/2305.15080) Ôºà**2023.05.24**Ôºâ

<font color="gray">Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/naver-ai/cream)

---

[**EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought**](https://arxiv.org/abs/2305.15021) Ôºà**2023.05.24**Ôºâ

<font color="gray">Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-19-blue)](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)

---

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

<font color="gray">Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

<font color="gray">Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks**](https://arxiv.org/abs/2305.14201) Ôºà**2023.05.23**Ôºâ

<font color="gray">Tiedong Liu, Bryan Kian Hsiang Low </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-26-red)  [![](https://img.shields.io/badge/Github%20Stars-102-blue)](https://github.com/liutiedong/goat)

---

[**ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**](https://arxiv.org/abs/2305.14323) Ôºà**2023.05.23**Ôºâ

<font color="gray">Z. Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/rucaibox/chatcot)

---

[**Enhance Reasoning Ability of Visual-Language Models via Large Language Models**](https://arxiv.org/abs/2305.13267) Ôºà**2023.05.22**Ôºâ

<font color="gray">Yueting Yang, Xintong Zhang, Wenjuan Han </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**InstructVid2Vid: Controllable Video Editing with Natural Language Instructions**](https://arxiv.org/abs/2305.12328) Ôºà**2023.05.21**Ôºâ

<font color="gray">Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-8-red)

---

[**Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**](https://arxiv.org/abs/2305.12295) Ôºà**2023.05.20**Ôºâ

<font color="gray">Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)  [![](https://img.shields.io/badge/Github%20Stars-23-blue)](https://github.com/teacherpeterpan/logic-llm)

---

[**LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4**](https://arxiv.org/abs/2305.12147) Ôºà**2023.05.20**Ôºâ

<font color="gray">Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/csitfun/logicot)

---

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) Ôºà**2023.05.19**Ôºâ

<font color="gray">IokTong Lei, ZhiDong Deng .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought**](https://doi.org/10.48550/arXiv.2305.11499) Ôºà**2023.05.19**Ôºâ

<font color="gray">Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding**](https://doi.org/10.48550/arXiv.2305.11497) Ôºà**2023.05.19**Ôºâ

<font color="gray">Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, Long Chen .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Efficient Prompting via Dynamic In-Context Learning**](https://doi.org/10.48550/arXiv.2305.11170) Ôºà**2023.05.18**Ôºâ

<font color="gray">Wangchunshu Zhou, Yuchen Jiang, Ryan Cotterell, Mrinmaya Sachan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers**](https://arxiv.org/abs/2305.07185) Ôºà**2023.05.12**Ôºâ

<font color="gray">L. Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-62-red)

---

[**Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models**](https://doi.org/10.48550/arXiv.2305.04441) Ôºà**2023.05.08**Ôºâ

<font color="gray">Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting**](https://doi.org/10.48550/arXiv.2305.03902) Ôºà**2023.05.06**Ôºâ

<font color="gray">Xiaoyuan Guo, Xiang Wei, Q. Su, Hui-Huang Zhao, Shunli Zhan .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Edit Everything: A Text-Guided Generative System for Images Editing**](https://arxiv.org/abs/2304.14006) Ôºà**2023.04.27**Ôºâ

<font color="gray">Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-73-blue)](https://github.com/defengxie/edit_everything)

---

[**ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**](https://arxiv.org/abs/2304.14407) Ôºà**2023.04.27**Ôºâ

<font color="gray">Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178) Ôºà**2023.04.27**Ôºâ

<font color="gray">Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-34-red)  [![](https://img.shields.io/badge/Github%20Stars-875-blue)](https://github.com/x-plug/mplug-owl)

---

[**Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**](https://doi.org/10.48550/arXiv.2304.09337) Ôºà**2023.04.18**Ôºâ

<font color="gray">Stephen Brade, Bryan Wang, Maur√≠cio Sousa, Sageev Oore, Tovi Grossman .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Towards Robust Prompts on Vision-Language Models**](https://arxiv.org/abs/2304.08479) Ôºà**2023.04.17**Ôºâ

<font color="gray">Jindong Gu, A. Beirami, Xuezhi Wang, Alex Beutel, Philip H. S. Torr, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Visual Instruction Tuning**](https://arxiv.org/abs/2304.08485) Ôºà**2023.04.17**Ôºâ

<font color="gray">Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-126-red)  [![](https://img.shields.io/badge/Github%20Stars-2.8k-blue)](https://github.com/haotian-liu/LLaVA)

---

[**Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text**](https://arxiv.org/abs/2304.06939) Ôºà**2023.04.14**Ôºâ

<font color="gray">Wanrong Zhu, Jack Hessel, Anas Awadalla, S. Gadre, Jesse Dodge, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-44-red)  [![](https://img.shields.io/badge/Github%20Stars-691-blue)](https://github.com/allenai/mmc4)

---

[**Segment Everything Everywhere All at Once**](https://doi.org/10.48550/arXiv.2304.06718) Ôºà**2023.04.13**Ôºâ

<font color="gray">Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-2.7k-blue)](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once)

---

[**Efficient Multimodal Fusion via Interactive Prompting**](https://arxiv.org/abs/2304.06306) Ôºà**2023.04.13**Ôºâ

<font color="gray">Yaowei Li, Ruijie Quan, Linchao Zhu, Yezhou Yang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning**](https://arxiv.org/abs/2304.05613) Ôºà**2023.04.12**Ôºâ

<font color="gray">Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-23-red)

---

[**Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition**](https://doi.org/10.48550/arXiv.2304.04704) Ôºà**2023.04.10**Ôºâ

<font color="gray">Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-172-blue)](https://github.com/amazon-science/prompt-pretraining)

---

[**Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions**](https://doi.org/10.48550/arXiv.2304.04227) Ôºà**2023.04.09**Ôºâ

<font color="gray">Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-315-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting**](https://arxiv.org/abs/2304.03307) Ôºà**2023.04.06**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-26-red)  [![](https://img.shields.io/badge/Github%20Stars-31-blue)](https://github.com/talalwasim/vita-clip)

---

[**TagGPT: Large Language Models are Zero-shot Multimodal Taggers**](https://arxiv.org/abs/2304.03022) Ôºà**2023.04.06**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-22-blue)](https://github.com/tencentarc/taggpt)

---

[**Segment Anything**](https://doi.org/10.48550/arXiv.2304.02643) Ôºà**2023.04.05**Ôºâ

<font color="gray">A. Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-33.3k-blue)](https://github.com/facebookresearch/segment-anything)

---

[**TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs**](https://doi.org/10.48550/arXiv.2303.16434) Ôºà**2023.03.29**Ôºâ

<font color="gray">Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-8-green)

---

[**MEDIMP: Medical Images and Prompts for renal transplant representation learning**](https://arxiv.org/abs/2303.12445) Ôºà**2023.03.22**Ôºâ

<font color="gray">Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition**](https://arxiv.org/abs/2303.11313) Ôºà**2023.03.20**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-27-red)  [![](https://img.shields.io/badge/Github%20Stars-144-blue)](https://github.com/deeptibhegde/clip-goes-3d)

---

[**MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://doi.org/10.48550/arXiv.2303.11381) Ôºà**2023.03.20**Ôºâ

<font color="gray">Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-674-blue)](https://github.com/microsoft/MM-REACT)

---

[**Visual Prompt Multi-Modal Tracking**](https://arxiv.org/abs/2303.10826) Ôºà**2023.03.20**Ôºâ



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)  [![](https://img.shields.io/badge/Github%20Stars-109-blue)](https://github.com/jiawen-zhu/vipt)

---

[**Audio Visual Language Maps for Robot Navigation**](https://doi.org/10.48550/arXiv.2303.07522) Ôºà**2023.03.13**Ôºâ

<font color="gray">Chen Huang, Oier Mees, Andy Zeng, W. Burgard .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions**](https://doi.org/10.48550/arXiv.2303.06594) Ôºà**2023.03.12**Ôºâ

<font color="gray">Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, etc .  - „ÄêarXiv.org„Äë</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-343-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/abs/2303.04671) Ôºà**2023.03.08**Ôºâ

<font color="gray">Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-373-red)  [![](https://img.shields.io/badge/Github%20Stars-32.9k-blue)](https://github.com/microsoft/visual-chatgpt)

---

[**Multimodal Parameter-Efficient Few-Shot Class Incremental Learning**](https://doi.org/10.48550/arXiv.2303.04751) Ôºà**2023.03.08**Ôºâ

<font color="gray">Marco D‚ÄôAlessandro, Alberto Alonso, Enrique Calabr'es, M. Galar .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning**](https://arxiv.org/abs/2303.02861) Ôºà**2023.03.06**Ôºâ

<font color="gray">Zhen Wang, R. Panda, Leonid Karlinsky, R. Feris, Huan Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-37-red)

---

[**Multimodal Prompting with Missing Modalities for Visual Recognition**](https://doi.org/10.48550/arXiv.2303.03369) Ôºà**2023.03.06**Ôºâ

<font color="gray">Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, Chen-Yu Lee .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-59-blue)](https://github.com/yilunlee/missing_aware_prompts)

---

[**Multimodal Chain-of-Thought Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2302.00923) Ôºà**2023.02.02**Ôºâ

<font color="gray">Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-6-green)  [![](https://img.shields.io/badge/Github%20Stars-3.3k-blue)](https://github.com/amazon-science/mm-cot)

---

[**LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation**](https://doi.org/10.48550/arXiv.2210.15461) Ôºà**2022.10.19**Ôºâ

<font color="gray">Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**CoHOZ: Contrasive Multimodal prompt Tuning for Hierarchical Open-set Zero-shot Recognition**](https://doi.org/10.1145/3503161.3548021) Ôºà**2022.10.10**Ôºâ

<font color="gray">Ning Liao, Yifeng Liu, Li Xiaobo, Chenyi Lei, Guoxin Wang, etc .  - „ÄêProceedings of the 30th ACM International Conference on Multimedia„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**VIMA: General Robot Manipulation with Multimodal Prompts**](https://doi.org/10.48550/arXiv.2210.03094) Ôºà**2022.10.06**Ôºâ

<font color="gray">Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-15-green)  [![](https://img.shields.io/badge/Github%20Stars-106-blue)](https://github.com/vimalabs/VIMABench)

---

[**Learning to Prompt for Vision-Language Models**](https://doi.org/10.1007/s11263-022-01653-1) Ôºà**2022.09.01**Ôºâ

<font color="gray">Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-491-red)  [![](https://img.shields.io/badge/Github%20Stars-932-blue)](https://github.com/kaiyangzhou/coop)

---

[**Visual Prompt Tuning**](https://doi.org/10.48550/arXiv.2203.12119) Ôºà**2022.03.23**Ôºâ

<font color="gray">Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, S. Belongie, etc .  - „ÄêEuropean Conference on Computer Vision„Äë</font>

![](https://img.shields.io/badge/Citations-104-green)  [![](https://img.shields.io/badge/Github%20Stars-518-blue)](https://github.com/KMnP/vpt)

---

[**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) Ôºà**2021.06.25**Ôºâ

<font color="gray">Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. Eslami, Oriol Vinyals, etc .  - „ÄêNeural Information Processing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-173-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-438-red)

---

[**Similarity-Aware Multimodal Prompt Learning for Fake News Detection**](https://doi.org/10.2139/ssrn.4347542) 

<font color="gray">Ye Jiang, Xiaomin Yu, Yimin Wang, Xiaoman Xu, Xingyi Song, etc .  - „ÄêSSRN Electronic Journal„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)


</div>

# CONTINUE...