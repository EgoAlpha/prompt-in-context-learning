# 📄 Multimodal Prompt

## Paper List

<div style="line-height:0.2em;">


[**What can generic neural networks learn from a child's visual experience?**](https://arxiv.org/abs/2305.15372) （**2023.05.24**）

<font color="gray">A. Emin Orhan, Brenden M. Lake </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**LayoutGPT: Compositional Visual Planning and Generation with Large Language Models**](https://arxiv.org/abs/2305.15393) （**2023.05.24**）

<font color="gray">Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Breaking the Curse of Quality Saturation with User-Centric Ranking**](https://arxiv.org/abs/2305.15333) （**2023.05.24**）

<font color="gray">Zhuokai Zhao, Yang Yang, Wenyu Wang, Chihuang Liu, Yu Shi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Utility-Probability Duality of Neural Networks**](https://arxiv.org/abs/2305.14859) （**2023.05.24**）

<font color="gray">Huang Bojun, Fei Yuan </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion**](https://arxiv.org/abs/2305.14849) （**2023.05.24**）

<font color="gray">Taesun Yeom, Minhyeok Lee </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Training on Thin Air: Improve Image Classification with Generated Data**](https://arxiv.org/abs/2305.15316) （**2023.05.24**）

<font color="gray">Yongchao Zhou, Hshmat Sahak, Jimmy Ba </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/yongchao97/diffusion_inversion)

---

[**Delving Deeper into Data Scaling in Masked Image Modeling**](https://arxiv.org/abs/2305.15248) （**2023.05.24**）

<font color="gray">Cheng-Ze Lu, Xiaojie Jin, Qibin Hou, Jun Hao Liew, Ming-Ming Cheng, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Neural Summarization of Electronic Health Records**](https://arxiv.org/abs/2305.15222) （**2023.05.24**）

<font color="gray">Koyena Pal, Seyed Ali Bahrainian, Laura Y. Mercurio, Carsten Eickhoff </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Topic-Guided Self-Introduction Generation for Social Media Users**](https://arxiv.org/abs/2305.15138) （**2023.05.24**）

<font color="gray">Chunpu Xu, Jing Li, Pijian Li, Min Yang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Meta-Learning For Vision-and-Language Cross-lingual Transfer**](https://arxiv.org/abs/2305.14843) （**2023.05.24**）

<font color="gray">Hanxu Hu, Frank Keller </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Thinking Twice: Clinical-Inspired Thyroid Ultrasound Lesion Detection Based on Feature Feedback**](https://arxiv.org/abs/2305.15114) （**2023.05.24**）

<font color="gray">Lingtao Wang, Jianrui Ding, Fenghe Tang, C. Ning </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/hit-wanglingtao/thinking-twice)

---

[**InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance Fields**](https://arxiv.org/abs/2305.15094) （**2023.05.24**）

<font color="gray">Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine Susstrunk </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models**](https://arxiv.org/abs/2305.15080) （**2023.05.24**）

<font color="gray">Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/naver-ai/cream)

---

[**Diffusion Models in NLP: A Survey**](https://arxiv.org/abs/2305.14671) （**2023.05.24**）

<font color="gray">Hao Zou, Zae Myung Kim, Dongyeop Kang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models**](https://arxiv.org/abs/2305.14610) （**2023.05.24**）

<font color="gray">Bryan Li, Chris Callison-Burch </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning**](https://arxiv.org/abs/2305.15065) （**2023.05.24**）

<font color="gray">Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Dior-CVAE: Diffusion Priors in Variational Dialog Generation**](https://arxiv.org/abs/2305.15025) （**2023.05.24**）

<font color="gray">Tianyu Yang, Thy Thy Tran, Iryna Gurevych </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought**](https://arxiv.org/abs/2305.15021) （**2023.05.24**）

<font color="gray">Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-19-blue)](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)

---

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) （**2023.05.24**）

<font color="gray">Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Introducing Competition to Boost the Transferability of Targeted Adversarial Examples through Clean Feature Mixup**](https://arxiv.org/abs/2305.14846) （**2023.05.24**）

<font color="gray">Junyoung Byun, Myung-Joon Kwon, Seungju Cho, Yoonji Kim, Changick Kim </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/dreamflake/cfm)

---

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) （**2023.05.24**）

<font color="gray">Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) （**2023.05.24**）

<font color="gray">Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**COMET-M: Reasoning about Multiple Events in Complex Sentences**](https://arxiv.org/abs/2305.14617) （**2023.05.24**）

<font color="gray">Sahithya Ravi, Raymond Ng, Vered Shwartz </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations**](https://arxiv.org/abs/2305.14599) （**2023.05.24**）

<font color="gray">James Y. Huang, Wenlin Yao, Kaiqiang Song, Hongming Zhang, Muhao Chen, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers**](https://arxiv.org/abs/2305.14591) （**2023.05.24**）

<font color="gray">Ke Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, Lei Li </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents**](https://arxiv.org/abs/2305.14590) （**2023.05.24**）

<font color="gray">Pritika Ramu, Sijia Wang, Lalla Mouatadid, Joy Rimchala, Lifu Huang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution**](https://arxiv.org/abs/2305.15357) （**2023.05.24**）

<font color="gray">Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Detection of Non-uniformity in Parameters for Magnetic Domain Pattern Generation by Machine Learning**](https://arxiv.org/abs/2305.14764) （**2023.05.24**）

<font color="gray">Naoya Mamada, Masaichiro Mizumaki, Ichiro Akai, Toru Aonishi </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss**](https://arxiv.org/abs/2305.14723) （**2023.05.24**）

<font color="gray">Hiroshi Sato, Ryo Masumura, Tsubasa Ochiai, Marc Delcroix, Takafumi Moriya, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**How to Solve Few-Shot Abusive Content Detection Using the Data We Actually Have**](https://arxiv.org/abs/2305.14081) （**2023.05.23**）

<font color="gray">Viktor Hangya, Alexander Fraser </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks**](https://arxiv.org/abs/2305.14201) （**2023.05.23**）

<font color="gray">Tiedong Liu, Bryan Kian Hsiang Low </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Improving Language Models via Plug-and-Play Retrieval Feedback**](https://arxiv.org/abs/2305.14002) （**2023.05.23**）

<font color="gray">Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, Ashish Sabharwal </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

---

[**Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation.**](https://doi.org/10.1109/TIP.2023.3277791) （**2023.05.23**）

<font color="gray">Chang Liu, Henghui Ding, Yulun Zhang, Xudong Jiang .  - 【IEEE Transactions on Image Processing】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Difference-Masking: Choosing What to Mask in Continued Pretraining**](https://arxiv.org/abs/2305.14577) （**2023.05.23**）

<font color="gray">Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations**](https://arxiv.org/abs/2305.14556) （**2023.05.23**）

<font color="gray">Tiziano Labruna, Sofia Brenna, Andrea Zaninello, Bernardo Magnini </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization**](https://arxiv.org/abs/2305.14551) （**2023.05.23**）

<font color="gray">Andrey Palaev, Rustam A. Lukmanov, Adil Hamid Khan </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Extracting Shopping Interest-Related Product Types from the Web**](https://arxiv.org/abs/2305.14549) （**2023.05.23**）

<font color="gray">Yinghao Li, Colin Lockard, Prashant Shiralkar, Chao Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization**](https://arxiv.org/abs/2305.14548) （**2023.05.23**）

<font color="gray">Hou Pong Chan, Qi Zeng, Heng Ji </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/kenchan0226/finegrainedfact)

---

[**ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**](https://arxiv.org/abs/2305.14323) （**2023.05.23**）

<font color="gray">Z. Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/rucaibox/chatcot)

---

[**Improved Convergence of Score-Based Diffusion Models via Prediction-Correction**](https://arxiv.org/abs/2305.14164) （**2023.05.23**）

<font color="gray">Francesco Pedrotti, J. Maas, Marco Mondelli </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**ConGraT: Self-Supervised Contrastive Pretraining for Joint Graph and Text Embeddings**](https://arxiv.org/abs/2305.14321) （**2023.05.23**）

<font color="gray">William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/wwbrannon/congrat)

---

[**mPMR: A Multilingual Pre-trained Machine Reader at Scale**](https://arxiv.org/abs/2305.13645) （**2023.05.23**）

<font color="gray">Weiwen Xu, Xin Li, Wai Lam, Lidong Bing </font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/damo-nlp-sg/pmr)

---

[**Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs**](https://arxiv.org/abs/2305.13585) （**2023.05.23**）

<font color="gray">Siyuan Wang, Zhongyu Wei, Meng Han, Zhihao Fan, Haijun Shan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models**](https://arxiv.org/abs/2305.13707) （**2023.05.23**）

<font color="gray">Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**EDIS: Entity-Driven Image Search over Multimodal Web Content**](https://arxiv.org/abs/2305.13631) （**2023.05.23**）

<font color="gray">Siqi Liu, Weixi Feng, Wenhu Chen, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models**](https://arxiv.org/abs/2305.14267) （**2023.05.23**）

<font color="gray">Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**NORM: Knowledge Distillation via N-to-One Representation Matching**](https://arxiv.org/abs/2305.13803) （**2023.05.23**）

<font color="gray">Xiaolong Liu, Lujun Li, Chao Li, Anbang Yao </font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/osvai/norm)

---

[**Online Open-set Semi-supervised Object Detection via Semi-supervised Outlier Filtering**](https://arxiv.org/abs/2305.13802) （**2023.05.23**）

<font color="gray">Zerun Wang, Ling Xiao, Liuyu Xiang, Zhaotian Weng, T. Yamasaki </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Cross-functional Analysis of Generalisation in Behavioural Learning**](https://arxiv.org/abs/2305.12951) （**2023.05.22**）

<font color="gray">Pedro Henrique Luz de Araujo, Benjamin Roth </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space**](https://arxiv.org/abs/2305.12798) （**2023.05.22**）

<font color="gray">Chi Han, Jialiang Xu, Manling Li, Y. Fung, Chenkai Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

[**Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer**](https://arxiv.org/abs/2305.12761) （**2023.05.22**）

<font color="gray">Shuang Li, Xuming Hu, Aiwei Liu, Yawen Yang, Fukun Ma, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches**](https://arxiv.org/abs/2305.12749) （**2023.05.22**）

<font color="gray">Zihan Wang, Tianle Wang, Dheeraj Mekala, Jingbo Shang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference**](https://arxiv.org/abs/2305.13484) （**2023.05.22**）

<font color="gray">Jinghan Yao, Nawras Alnaasan, Tian Chen, A. Shafi, Hari Subramoni, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/yjhmitweb/flover)

---

[**Enhance Reasoning Ability of Visual-Language Models via Large Language Models**](https://arxiv.org/abs/2305.13267) （**2023.05.22**）

<font color="gray">Yueting Yang, Xintong Zhang, Wenjuan Han </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Design a Delicious Lunchbox in Style**](https://arxiv.org/abs/2305.14522) （**2023.05.22**）

<font color="gray">Yutong Zhou </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**EnSiam: Self-Supervised Learning With Ensemble Representations**](https://arxiv.org/abs/2305.13391) （**2023.05.22**）

<font color="gray">Kyoungmin Han, Minsik Lee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Finding the Pillars of Strength for Multi-Head Attention**](https://arxiv.org/abs/2305.14380) （**2023.05.22**）

<font color="gray">Jinjie Ni, Rui Mao, Zonglin Yang, Han Lei, E. Cambria </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Text-based Person Search without Parallel Image-Text Data**](https://arxiv.org/abs/2305.12964) （**2023.05.22**）

<font color="gray">Yang Bai, Jingyao Wang, Min Cao, Chen Chen, Ziqiang Cao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Gibbs free energies via isobaric-isothermal flows**](https://doi.org/10.48550/arXiv.2305.13233) （**2023.05.22**）

<font color="gray">P. Wirnsberger, Borja Ibarz, G. Papamakarios .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-40-blue)](https://github.com/deepmind/flows_for_atomic_solids)

---

[**BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance**](https://arxiv.org/abs/2305.13395) （**2023.05.22**）

<font color="gray">Karel D'Oosterlinck, Franccois Remy, Johannes Deleu, Thomas Demeester, Chris Develder, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/kareldo/biodex)

---

[**Training Diffusion Models with Reinforcement Learning**](https://arxiv.org/abs/2305.13301) （**2023.05.22**）

<font color="gray">Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, S. Levine </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-22-red)

---

[**Latent Magic: An Investigation into Adversarial Examples Crafted in the Semantic Latent Space**](https://arxiv.org/abs/2305.12906) （**2023.05.22**）

<font color="gray">BoYang Zheng </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Spatiotemporal Attention-based Semantic Compression for Real-time Video Recognition**](https://arxiv.org/abs/2305.12796) （**2023.05.22**）

<font color="gray">Nana Li, Mehdi Bennis, Alexandros Iosifidis, Qi Zhang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**Instance-Level Semantic Maps for Vision Language Navigation**](https://arxiv.org/abs/2305.12363) （**2023.05.21**）

<font color="gray">Laksh Nanwani, Anmol Agarwal, Kanishk Jain, Raghav Prabhakar, Aaron Monis, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles**](https://arxiv.org/abs/2305.12344) （**2023.05.21**）

<font color="gray">Wahyu Pebrianto, Panca Mudjirahardjo, Sholeh Hadi Pramono, Rahmadwati, Raden Arief Setyawan </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**InstructVid2Vid: Controllable Video Editing with Natural Language Instructions**](https://arxiv.org/abs/2305.12328) （**2023.05.21**）

<font color="gray">Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)

---

[**Random Relabeling for Efficient Machine Unlearning**](https://arxiv.org/abs/2305.12320) （**2023.05.21**）

<font color="gray">Junde Li, Swaroop Ghosh </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**A Dual-level Detection Method for Video Copy Detection**](https://arxiv.org/abs/2305.12361) （**2023.05.21**）

<font color="gray">Tianyi Wang, Feipeng Ma, Zhenhua Liu, Fengyun Rao </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/feipengma6/vsc22-submission)

---

[**Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models**](https://arxiv.org/abs/2305.12356) （**2023.05.21**）

<font color="gray">Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, etc </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**IR Models and the COVID-19 Pandemic: A Comparative Study of Performance and Challenges (preprint)**](https://arxiv.org/abs/2305.12528) （**2023.05.21**）

<font color="gray">Moksh Shukla, Niti Jain, Shubham Gupta </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**DreamWaltz: Make a Scene with Complex 3D Animatable Avatars**](https://arxiv.org/abs/2305.12529) （**2023.05.21**）

<font color="gray">Yukun Huang, Jianan Wang, Ailing Zeng, Heng Cao, Xianbiao Qi, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)

---

[**Comparative Analysis of Deep Learning Models for Brand Logo Classification in Real-World Scenarios**](https://arxiv.org/abs/2305.12242) （**2023.05.20**）

<font color="gray">Qimao Yang, Huilin Chen, Qiwei Dong </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**](https://arxiv.org/abs/2305.12295) （**2023.05.20**）

<font color="gray">Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-5-red)  [![](https://img.shields.io/badge/Github%20Stars-23-blue)](https://github.com/teacherpeterpan/logic-llm)

---

[**LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4**](https://arxiv.org/abs/2305.12147) （**2023.05.20**）

<font color="gray">Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/csitfun/logicot)

---

[**Accurate Knowledge Distillation with n-best Reranking**](https://arxiv.org/abs/2305.12057) （**2023.05.20**）

<font color="gray">Hendra Setiawan </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SurgMAE: Masked Autoencoders for Long Surgical Video Analysis**](https://doi.org/10.48550/arXiv.2305.11451) （**2023.05.19**）

<font color="gray">Muhammad Abdullah Jamal, O. Mohareri .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**DUB: Discrete Unit Back-translation for Speech Translation**](https://doi.org/10.48550/arXiv.2305.11411) （**2023.05.19**）

<font color="gray">Dong Zhang, Rong Ye, Tom Ko, Mingxuan Wang, Yaqian Zhou .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/0nutation/dub)

---

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) （**2023.05.19**）

<font color="gray">IokTong Lei, ZhiDong Deng .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning**](https://doi.org/10.48550/arXiv.2305.11759) （**2023.05.19**）

<font color="gray">Mustafa Safa Ozdayi, Charith S. Peris, Jack G. M. FitzGerald, Christophe Dupuy, Jimit Majmudar, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought**](https://doi.org/10.48550/arXiv.2305.11499) （**2023.05.19**）

<font color="gray">Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning**](https://doi.org/10.48550/arXiv.2305.11701) （**2023.05.19**）

<font color="gray">Alvzbveta Manov'a, A. Durrant, G. Leontidis .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Enhancing Transformer Backbone for Egocentric Video Action Segmentation**](https://doi.org/10.48550/arXiv.2305.11365) （**2023.05.19**）

<font color="gray">Sakib Reza, Balaji Sundareshan, Mohsen Moghaddam, O. Camps .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding**](https://arxiv.org/abs/2305.12031) （**2023.05.19**）

<font color="gray">Augustin Toma, Patrick R. Lawler, Jimmy Ba, Rahul G. Krishnan, Barry B. Rubin, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)  [![](https://img.shields.io/badge/Github%20Stars-19-blue)](https://github.com/bowang-lab/clinical-camel)

---

[**Complex Claim Verification with Evidence Retrieved in the Wild**](https://doi.org/10.48550/arXiv.2305.11859) （**2023.05.19**）

<font color="gray">Jifan Chen, Grace Kim, Aniruddh Sriram, G. Durrett, Eunsol Choi .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding**](https://doi.org/10.48550/arXiv.2305.11497) （**2023.05.19**）

<font color="gray">Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, Long Chen .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation**](https://doi.org/10.48550/arXiv.2305.11761) （**2023.05.19**）

<font color="gray">Javier Garc'ia Gilabert, Carlos Escolano, M. Costa-jussà .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Attributable and Scalable Opinion Summarization**](https://doi.org/10.48550/arXiv.2305.11603) （**2023.05.19**）

<font color="gray">Tom Hosking, Hao Tang, Mirella Lapata .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/tomhosking/hercules)

---

[**Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens**](https://doi.org/10.48550/arXiv.2305.11550) （**2023.05.19**）

<font color="gray">David Stap, Vlad Niculae, C. Monz .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models**](https://doi.org/10.48550/arXiv.2305.11281) （**2023.05.18**）

<font color="gray">Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, Animesh Garg .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**In Defense of Pure 16-bit Floating-Point Neural Networks**](https://doi.org/10.48550/arXiv.2305.10947) （**2023.05.18**）

<font color="gray">Juyoung Yun, Byungkon Kang, François Rameau, Zhoulai Fu .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Student-friendly Knowledge Distillation**](https://doi.org/10.48550/arXiv.2305.10893) （**2023.05.18**）

<font color="gray">Mengyang Yuan, Bo Lang, Fengnan Quan .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Learning In-context Learning for Named Entity Recognition**](https://doi.org/10.48550/arXiv.2305.11038) （**2023.05.18**）

<font color="gray">Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Efficient Prompting via Dynamic In-Context Learning**](https://doi.org/10.48550/arXiv.2305.11170) （**2023.05.18**）

<font color="gray">Wangchunshu Zhou, Yuchen Jiang, Ryan Cotterell, Mrinmaya Sachan .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?**](https://doi.org/10.48550/arXiv.2305.09847) （**2023.05.16**）

<font color="gray">Pareesa Ameneh Golnari, Z. Yao, Yuxiong He .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Vanishing Activations: A Symptom of Deep Capsule Networks**](https://doi.org/10.48550/arXiv.2305.11178) （**2023.05.13**）

<font color="gray">Miles Everett, Mingjun Zhong, G. Leontidis .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers**](https://arxiv.org/abs/2305.07185) （**2023.05.12**）

<font color="gray">L. Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-62-red)

---

[**Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks**](https://doi.org/10.48550/arXiv.2305.10596) （**2023.05.10**）

<font color="gray">Xinrui Liu, Yajie Wang, Yu‐an Tan, Kefan Qiu, Yuan-zhang Li .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation**](https://doi.org/10.48550/arXiv.2305.11061) （**2023.05.10**）

<font color="gray">Ran Shen, Gang Sun, Hao Shen, Yiling Li, Liangfeng Jin, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models**](https://doi.org/10.48550/arXiv.2305.04441) （**2023.05.08**）

<font color="gray">Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting**](https://doi.org/10.48550/arXiv.2305.03902) （**2023.05.06**）

<font color="gray">Xiaoyuan Guo, Xiang Wei, Q. Su, Hui-Huang Zhao, Shunli Zhan .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Edit Everything: A Text-Guided Generative System for Images Editing**](https://arxiv.org/abs/2304.14006) （**2023.04.27**）

<font color="gray">Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-73-blue)](https://github.com/defengxie/edit_everything)

---

[**ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**](https://arxiv.org/abs/2304.14407) （**2023.04.27**）

<font color="gray">Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

---

[**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178) （**2023.04.27**）

<font color="gray">Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-34-red)  [![](https://img.shields.io/badge/Github%20Stars-875-blue)](https://github.com/x-plug/mplug-owl)

---

[**Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**](https://doi.org/10.48550/arXiv.2304.09337) （**2023.04.18**）

<font color="gray">Stephen Brade, Bryan Wang, Maurício Sousa, Sageev Oore, Tovi Grossman .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Towards Robust Prompts on Vision-Language Models**](https://arxiv.org/abs/2304.08479) （**2023.04.17**）

<font color="gray">Jindong Gu, A. Beirami, Xuezhi Wang, Alex Beutel, Philip H. S. Torr, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

---

[**Visual Instruction Tuning**](https://arxiv.org/abs/2304.08485) （**2023.04.17**）

<font color="gray">Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-126-red)  [![](https://img.shields.io/badge/Github%20Stars-2.8k-blue)](https://github.com/haotian-liu/LLaVA)

---

[**Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text**](https://arxiv.org/abs/2304.06939) （**2023.04.14**）

<font color="gray">Wanrong Zhu, Jack Hessel, Anas Awadalla, S. Gadre, Jesse Dodge, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-44-red)  [![](https://img.shields.io/badge/Github%20Stars-691-blue)](https://github.com/allenai/mmc4)

---

[**Segment Everything Everywhere All at Once**](https://doi.org/10.48550/arXiv.2304.06718) （**2023.04.13**）

<font color="gray">Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-2.7k-blue)](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once)

---

[**Efficient Multimodal Fusion via Interactive Prompting**](https://arxiv.org/abs/2304.06306) （**2023.04.13**）

<font color="gray">Yaowei Li, Ruijie Quan, Linchao Zhu, Yezhou Yang </font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning**](https://arxiv.org/abs/2304.05613) （**2023.04.12**）

<font color="gray">Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-23-red)

---

[**Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition**](https://doi.org/10.48550/arXiv.2304.04704) （**2023.04.10**）

<font color="gray">Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-172-blue)](https://github.com/amazon-science/prompt-pretraining)

---

[**Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions**](https://doi.org/10.48550/arXiv.2304.04227) （**2023.04.09**）

<font color="gray">Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-315-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting**](https://arxiv.org/abs/2304.03307) （**2023.04.06**）



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-26-red)  [![](https://img.shields.io/badge/Github%20Stars-31-blue)](https://github.com/talalwasim/vita-clip)

---

[**TagGPT: Large Language Models are Zero-shot Multimodal Taggers**](https://arxiv.org/abs/2304.03022) （**2023.04.06**）



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)  [![](https://img.shields.io/badge/Github%20Stars-22-blue)](https://github.com/tencentarc/taggpt)

---

[**Segment Anything**](https://doi.org/10.48550/arXiv.2304.02643) （**2023.04.05**）

<font color="gray">A. Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-33.3k-blue)](https://github.com/facebookresearch/segment-anything)

---

[**TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs**](https://doi.org/10.48550/arXiv.2303.16434) （**2023.03.29**）

<font color="gray">Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-8-green)

---

[**MEDIMP: Medical Images and Prompts for renal transplant representation learning**](https://arxiv.org/abs/2303.12445) （**2023.03.22**）

<font color="gray">Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

---

[**CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition**](https://arxiv.org/abs/2303.11313) （**2023.03.20**）



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-27-red)  [![](https://img.shields.io/badge/Github%20Stars-144-blue)](https://github.com/deeptibhegde/clip-goes-3d)

---

[**MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://doi.org/10.48550/arXiv.2303.11381) （**2023.03.20**）

<font color="gray">Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, etc .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-674-blue)](https://github.com/microsoft/MM-REACT)

---

[**Visual Prompt Multi-Modal Tracking**](https://arxiv.org/abs/2303.10826) （**2023.03.20**）



![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)  [![](https://img.shields.io/badge/Github%20Stars-109-blue)](https://github.com/jiawen-zhu/vipt)

---

[**Audio Visual Language Maps for Robot Navigation**](https://doi.org/10.48550/arXiv.2303.07522) （**2023.03.13**）

<font color="gray">Chen Huang, Oier Mees, Andy Zeng, W. Burgard .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions**](https://doi.org/10.48550/arXiv.2303.06594) （**2023.03.12**）

<font color="gray">Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, etc .  - 【arXiv.org】</font>

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-316-blue)](https://github.com/vision-cair/chatcaptioner)

---

[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/abs/2303.04671) （**2023.03.08**）

<font color="gray">Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-373-red)  [![](https://img.shields.io/badge/Github%20Stars-32.9k-blue)](https://github.com/microsoft/visual-chatgpt)

---

[**Multimodal Parameter-Efficient Few-Shot Class Incremental Learning**](https://doi.org/10.48550/arXiv.2303.04751) （**2023.03.08**）

<font color="gray">Marco D’Alessandro, Alberto Alonso, Enrique Calabr'es, M. Galar .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning**](https://arxiv.org/abs/2303.02861) （**2023.03.06**）

<font color="gray">Zhen Wang, R. Panda, Leonid Karlinsky, R. Feris, Huan Sun, etc </font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-37-red)

---

[**Multimodal Prompting with Missing Modalities for Visual Recognition**](https://doi.org/10.48550/arXiv.2303.03369) （**2023.03.06**）

<font color="gray">Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, Chen-Yu Lee .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-59-blue)](https://github.com/yilunlee/missing_aware_prompts)

---

[**Multimodal Chain-of-Thought Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2302.00923) （**2023.02.02**）

<font color="gray">Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, etc .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-6-green)  [![](https://img.shields.io/badge/Github%20Stars-3.3k-blue)](https://github.com/amazon-science/mm-cot)

---

[**LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation**](https://doi.org/10.48550/arXiv.2210.15461) （**2022.10.19**）

<font color="gray">Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, etc .  - 【Conference on Empirical Methods in Natural Language Processing】</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**CoHOZ: Contrasive Multimodal prompt Tuning for Hierarchical Open-set Zero-shot Recognition**](https://doi.org/10.1145/3503161.3548021) （**2022.10.10**）

<font color="gray">Ning Liao, Yifeng Liu, Li Xiaobo, Chenyi Lei, Guoxin Wang, etc .  - 【Proceedings of the 30th ACM International Conference on Multimedia】</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**VIMA: General Robot Manipulation with Multimodal Prompts**](https://doi.org/10.48550/arXiv.2210.03094) （**2022.10.06**）

<font color="gray">Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, etc .  - 【ArXiv】</font>

![](https://img.shields.io/badge/Citations-15-green)  [![](https://img.shields.io/badge/Github%20Stars-106-blue)](https://github.com/vimalabs/VIMABench)

---

[**Learning to Prompt for Vision-Language Models**](https://doi.org/10.1007/s11263-022-01653-1) （**2022.09.01**）

<font color="gray">Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </font>

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-491-red)  [![](https://img.shields.io/badge/Github%20Stars-932-blue)](https://github.com/kaiyangzhou/coop)

---

[**Visual Prompt Tuning**](https://doi.org/10.48550/arXiv.2203.12119) （**2022.03.23**）

<font color="gray">Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, S. Belongie, etc .  - 【European Conference on Computer Vision】</font>

![](https://img.shields.io/badge/Citations-104-green)  [![](https://img.shields.io/badge/Github%20Stars-518-blue)](https://github.com/KMnP/vpt)

---

[**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) （**2021.06.25**）

<font color="gray">Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. Eslami, Oriol Vinyals, etc .  - 【Neural Information Processing Systems】</font>

![](https://img.shields.io/badge/Citations-173-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-438-red)

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

[**Similarity-Aware Multimodal Prompt Learning for Fake News Detection**](https://doi.org/10.2139/ssrn.4347542) 

<font color="gray">Ye Jiang, Xiaomin Yu, Yimin Wang, Xiaoman Xu, Xingyi Song, etc .  - 【SSRN Electronic Journal】</font>

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---

---


</div>

# CONTINUE...