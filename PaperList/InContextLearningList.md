# üìÑ In-context Learning

## Paper List

<div style="line-height:0.2em;">


[**Larger language models do in-context learning differently**](https://doi.org/10.48550/arXiv.2303.03846) Ôºà**2023.03.07**Ôºâ

<font color="gray">Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) Ôºà**2023.03.01**Ôºâ

<font color="gray">Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) Ôºà**2023.02.23**Ôºâ

<font color="gray">Elliot Meyerson, M. Nelson, Herbie Bradley, Arash Moradi, Amy K. Hoover, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-1-green)

---

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) Ôºà**2023.02.22**Ôºâ

<font color="gray">Simeng Sun, Yang Liu, Dan Iter, Chenguang Zhu, Mohit Iyyer .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)

---

[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) Ôºà**2023.02.17**Ôºâ

<font color="gray">Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-17-blue)](https://github.com/salt-nlp/bound-cap-llm)

---

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) Ôºà**2023.01.27**Ôºâ

<font color="gray">Xinyi Wang, Wanrong Zhu, William Yang Wang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-12-blue)](https://github.com/wangxinyilinda/concept-based-demonstration-selection)

---

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) Ôºà**2023.01.27**Ôºâ

<font color="gray">Xinyi Wang, Wanrong Zhu, William Yang Wang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-12-blue)](https://github.com/wangxinyilinda/concept-based-demonstration-selection)

---

[**One Embedder, Any Task: Instruction-Finetuned Text Embeddings**](https://doi.org/10.48550/arXiv.2212.09741) Ôºà**2022.12.19**Ôºâ

<font color="gray">Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-2-green)

---

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) Ôºà**2022.11.25**Ôºâ

<font color="gray">Xi Ye, Srini Iyer, Asli Celikyilmaz, V. Stoyanov, Greg Durrett, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-5-green)

---

[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) Ôºà**2022.10.17**Ôºâ

<font color="gray">Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-48-blue)](https://github.com/noviscl/gpt3-reliability)

---

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) Ôºà**2022.10.17**Ôºâ

<font color="gray">Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-29-green)  [![](https://img.shields.io/badge/Github%20Stars-66-blue)](https://github.com/suzgunmirac/big-bench-hard)

---

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) Ôºà**2022.10.03**Ôºâ

<font color="gray">Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-18-green)

---

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) Ôºà**2022.07.02**Ôºâ

<font color="gray">Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-26-green)

---

[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2205.11916) Ôºà**2022.05.24**Ôºâ

<font color="gray">Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-187-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-309-red)  [![](https://img.shields.io/badge/Github%20Stars-150-blue)](https://github.com/kojima-takeshi188/zero_shot_cot)

---

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2204.02311) Ôºà**2022.04.05**Ôºâ

<font color="gray">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-624-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-683-red)  [![](https://img.shields.io/badge/Github%20Stars-664-blue)](https://github.com/lucidrains/CoCa-pytorch)

---

[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) Ôºà**2022.04.05**Ôºâ

<font color="gray">Andrew Kyle Lampinen, I. Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-61-green)

---

[**Chain of Thought Prompting Elicits Reasoning in Large Language Models**](https://arxiv.org/abs/2201.11903) Ôºà**2022.01.28**Ôºâ

<font color="gray">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-396-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-584-red)

---

[**Black-box Prompt Learning for Pre-trained Language Models**](https://arxiv.org/abs/2201.08531) Ôºà**2022.01.21**Ôºâ

<font color="gray">Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, Xiao Zhou, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-17-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-33-red)  [![](https://img.shields.io/badge/Github%20Stars-22-blue)](https://github.com/shizhediao/black-box-prompt-learning)

---

[**Multitask Prompted Training Enables Zero-Shot Task Generalization**](https://arxiv.org/abs/2110.08207) Ôºà**2021.10.15**Ôºâ

<font color="gray">Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, etc .  - „ÄêInternational Conference on Learning Representations„Äë</font>

![](https://img.shields.io/badge/Citations-387-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-418-red)  [![](https://img.shields.io/badge/Github%20Stars-1.4k-blue)](https://github.com/bigscience-workshop/promptsource)

---

[**Reframing Instructional Prompts to GPTk‚Äôs Language**](https://doi.org/10.18653/v1/2022.findings-acl.50) Ôºà**2021.09.16**Ôºâ

<font color="gray">Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi .  - „ÄêFindings„Äë</font>

![](https://img.shields.io/badge/Citations-59-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-66-red)

---

[**Noisy Channel Language Model Prompting for Few-Shot Text Classification**](https://doi.org/10.18653/v1/2022.acl-long.365) Ôºà**2021.08.09**Ôºâ

<font color="gray">Sewon Min, Michael Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-70-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-158-red)  [![](https://img.shields.io/badge/Github%20Stars-101-blue)](https://github.com/shmsw25/Channel-LM-Prompting)

---

[**Evaluating Large Language Models Trained on Code**](https://arxiv.org/abs/2107.03374) Ôºà**2021.07.07**Ôºâ

<font color="gray">Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-612-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-577-red)  [![](https://img.shields.io/badge/Github%20Stars-556-blue)](https://github.com/openai/human-eval)

---

[**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) Ôºà**2021.06.25**Ôºâ

<font color="gray">Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. Eslami, Oriol Vinyals, etc .  - „ÄêNeural Information Processing Systems„Äë</font>

![](https://img.shields.io/badge/Citations-179-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-405-red)

---

[**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity**](https://doi.org/10.18653/v1/2022.acl-long.556) Ôºà**2021.04.18**Ôºâ

<font color="gray">Yao Lu, Max Bartolo, Alastair Moore, S. Riedel, Pontus Stenetorp .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-168-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-183-red)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/chicagohai/active-example-selection)

---

[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) Ôºà**2021.01.01**Ôºâ

<font color="gray">Tianyu Gao, Adam Fisch, Danqi Chen .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/Citations-648-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-708-red)  [![](https://img.shields.io/badge/Github%20Stars-636-blue)](https://github.com/princeton-nlp/LM-BFF)

---

[**Few-Shot Text Generation with Pattern-Exploiting Training**](https://arxiv.org/abs/2012.11926) Ôºà**2020.12.22**Ôºâ

<font color="gray">Timo Schick, Hinrich Sch√ºtze .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/Citations-59-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-101-red)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/timoschick/pet)

---

[**Language Models as Knowledge Bases?**](https://doi.org/10.18653/v1/D19-1250) Ôºà**2019.09.01**Ôºâ

<font color="gray">Fabio Petroni, Tim Rockt√§schel, Patrick Lewis, A. Bakhtin, Yuxiang Wu, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/Citations-1038-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1.1k-red)  [![](https://img.shields.io/badge/Github%20Stars-1.0k-blue)](https://github.com/facebookresearch/LAMA)

---

[**Language Models are Unsupervised Multitask Learners**](https://api.semanticscholar.org/9405cc0d6169988371b2755e573cc28650d14dfe) 

<font color="gray">Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, etc </font>

![](https://img.shields.io/badge/Citations-8935-green)  [![](https://img.shields.io/badge/Github%20Stars-8.0k-blue)](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/gpt)


</div>

# CONTINUE...
