# üìÑ In-context Learning

## Paper List

<div style="line-height:0.2em;">


[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) Ôºà**2023.02.17**Ôºâ

<font color="gray">Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-0-red)


---
[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) Ôºà**2023.01.27**Ôºâ

<font color="gray">Xinyi Wang, Wanrong Zhu, William Yang Wang .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-1-red)


---
[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) Ôºà**2022.12.22**Ôºâ

<font color="gray">S. Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-9-red)


---
[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) Ôºà**2022.12.21**Ôºâ

<font color="gray">Hyunsoo Cho, Hyuhng Joon Kim, Junyeob Kim, Sang-Woo Lee, Sang-goo Lee, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Self-adaptive In-context Learning**](https://doi.org/10.48550/arXiv.2212.10375) Ôºà**2022.12.20**Ôºâ

<font color="gray">Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-3-red)


---
[**Is GPT-3 a Good Data Annotator?**](https://doi.org/10.48550/arXiv.2212.10450) Ôºà**2022.12.20**Ôºâ

<font color="gray">Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq R. Joty, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) Ôºà**2022.12.19**Ôºâ

<font color="gray">Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-7-red)


---
[**Structured Prompting: Scaling In-Context Learning to 1, 000 Examples**](https://doi.org/10.48550/arXiv.2212.06713) Ôºà**2022.12.13**Ôºâ

<font color="gray">Y. Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) Ôºà**2022.11.25**Ôºâ

<font color="gray">Xi Ye, Srini Iyer, Asli Celikyilmaz, V. Stoyanov, Greg Durrett, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-5-red)


---
[**Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning**](https://doi.org/10.48550/arXiv.2211.03044) Ôºà**2022.11.06**Ôºâ

<font color="gray">Yu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang, T. Abdelzaher, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-2-red)


---
[**Large Language Models Are Human-Level Prompt Engineers**](https://doi.org/10.48550/arXiv.2211.01910) Ôºà**2022.11.03**Ôºâ

<font color="gray">Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-20-red)


---
[**ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback**](https://doi.org/10.48550/arXiv.2210.12329) Ôºà**2022.10.22**Ôºâ

<font color="gray">Jiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu, Tao Yu, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/cite-4-red)


---
[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) Ôºà**2022.10.17**Ôºâ

<font color="gray">Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-9-red)


---
[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) Ôºà**2022.10.17**Ôºâ

<font color="gray">Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-27-red)


---
[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) Ôºà**2022.10.06**Ôºâ

<font color="gray">Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-21-red)


---
[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) Ôºà**2022.10.03**Ôºâ

<font color="gray">Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-18-red)


---
[**In-context Learning and Induction Heads**](https://doi.org/10.48550/arXiv.2209.11895) Ôºà**2022.09.24**Ôºâ

<font color="gray">Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-32-red)


---
[**On the Relation between Sensitivity and Accuracy in In-context Learning**](https://doi.org/10.48550/arXiv.2209.07661) Ôºà**2022.09.16**Ôºâ

<font color="gray">Yanda Chen, Chen Zhao, Zhou Yu, K. McKeown, He He .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-8-red)


---
[**What Can Transformers Learn In-Context? A Case Study of Simple Function Classes**](https://doi.org/10.48550/arXiv.2208.01066) Ôºà**2022.08.01**Ôºâ

<font color="gray">Shivam Garg, Dimitris Tsipras, Percy Liang, G. Valiant .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-25-red)


---
[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) Ôºà**2022.07.02**Ôºâ

<font color="gray">Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-26-red)


---
[**Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator**](https://doi.org/10.48550/arXiv.2206.08082) Ôºà**2022.06.16**Ôºâ

<font color="gray">Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-1-red)


---
[**Emergent Abilities of Large Language Models**](https://doi.org/10.48550/arXiv.2206.07682) Ôºà**2022.06.15**Ôºâ

<font color="gray">Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-151-red)


---
[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2205.11916) Ôºà**2022.05.24**Ôºâ

<font color="gray">Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-185-red)


---
[**Instruction Induction: From Few Examples to Natural Language Task Descriptions**](https://doi.org/10.48550/arXiv.2205.10782) Ôºà**2022.05.22**Ôºâ

<font color="gray">Or Honovich, Uri Shaham, Samuel R. Bowman, Omer Levy .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-8-red)


---
[**Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning**](https://doi.org/10.48550/arXiv.2205.05638) Ôºà**2022.05.11**Ôºâ

<font color="gray">Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-58-red)


---
[**Improving In-Context Few-Shot Learning via Self-Supervised Training**](https://doi.org/10.48550/arXiv.2205.01703) Ôºà**2022.05.03**Ôºâ

<font color="gray">Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, etc .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-6-red)


---
[**On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model**](https://doi.org/10.48550/arXiv.2204.13509) Ôºà**2022.04.28**Ôºâ

<font color="gray">Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, etc .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-13-red)


---
[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2204.02311) Ôºà**2022.04.05**Ôºâ

<font color="gray">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-612-red)


---
[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) Ôºà**2022.04.05**Ôºâ

<font color="gray">Andrew Kyle Lampinen, I. Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, etc .  - „ÄêConference on Empirical Methods in Natural Language Processing„Äë</font>

![](https://img.shields.io/badge/cite-61-red)


---
[**Chain of Thought Prompting Elicits Reasoning in Large Language Models**](https://arxiv.org/abs/2201.11903) Ôºà**2022.01.28**Ôºâ

<font color="gray">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E. Chi, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-384-red)


---
[**Black-box Prompt Learning for Pre-trained Language Models**](https://arxiv.org/abs/2201.08531) Ôºà**2022.01.21**Ôºâ

<font color="gray">Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, Xiao Zhou, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-17-red)


---
[**Learning To Retrieve Prompts for In-Context Learning**](https://doi.org/10.18653/v1/2022.naacl-main.191) Ôºà**2021.12.16**Ôºâ

<font color="gray">Ohad Rubin, Jonathan Herzig, Jonathan Berant .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-80-red)


---
[**An Explanation of In-context Learning as Implicit Bayesian Inference**](https://arxiv.org/abs/2111.02080) Ôºà**2021.11.03**Ôºâ

<font color="gray">Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma .  - „ÄêInternational Conference on Learning Representations„Äë</font>

![](https://img.shields.io/badge/cite-55-red)


---
[**MetaICL: Learning to Learn In Context**](https://doi.org/10.18653/v1/2022.naacl-main.201) Ôºà**2021.10.29**Ôºâ

<font color="gray">Sewon Min, M. Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi .  - „ÄêNorth American Chapter of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-82-red)


---
[**Meta-learning via Language Model In-context Tuning**](https://doi.org/10.18653/v1/2022.acl-long.53) Ôºà**2021.10.15**Ôºâ

<font color="gray">Yanda Chen, Ruiqi Zhong, Sheng Zha, G. Karypis, He He .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-32-red)


---
[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) Ôºà**2021.07.28**Ôºâ

<font color="gray">Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, etc .  - „ÄêACM Computing Surveys„Äë</font>

![](https://img.shields.io/badge/cite-444-red)


---
[**Evaluating Large Language Models Trained on Code**](https://arxiv.org/abs/2107.03374) Ôºà**2021.07.07**Ôºâ

<font color="gray">Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, etc .  - „ÄêArXiv„Äë</font>

![](https://img.shields.io/badge/cite-604-red)


---
[**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity**](https://doi.org/10.18653/v1/2022.acl-long.556) Ôºà**2021.04.18**Ôºâ

<font color="gray">Yao Lu, Max Bartolo, Alastair Moore, S. Riedel, Pontus Stenetorp .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-168-red)


---
[**Calibrate Before Use: Improving Few-Shot Performance of Language Models**](https://arxiv.org/abs/2102.09690) Ôºà**2021.02.19**Ôºâ

<font color="gray">Tony Zhao, Eric Wallace, Shi Feng, D. Klein, Sameer Singh .  - „ÄêInternational Conference on Machine Learning„Äë</font>

![](https://img.shields.io/badge/cite-281-red)


---
[**What Makes Good In-Context Examples for GPT-3?**](https://doi.org/10.18653/v1/2022.deelio-1.10) Ôºà**2021.01.17**Ôºâ

<font color="gray">Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, L. Carin, etc .  - „ÄêWorkshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out„Äë</font>

![](https://img.shields.io/badge/cite-169-red)


---
[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) Ôºà**2021.01.01**Ôºâ

<font color="gray">Tianyu Gao, Adam Fisch, Danqi Chen .  - „ÄêAnnual Meeting of the Association for Computational Linguistics„Äë</font>

![](https://img.shields.io/badge/cite-642-red)


---
[**Language Models are Unsupervised Multitask Learners**](https://api.semanticscholar.org/9405cc0d6169988371b2755e573cc28650d14dfe) 

<font color="gray">Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, etc </font>

![](https://img.shields.io/badge/cite-8878-red)


</div>

# CONTINUE...
