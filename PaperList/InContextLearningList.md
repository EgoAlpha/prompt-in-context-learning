# ğŸ“„ In-context Learning

## Paper List

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,Ping Yu etc 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,Kang Min Yoo etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Self-adaptive In-context Learning**](https://doi.org/10.48550/arXiv.2212.10375) ğŸ‘¨â€ğŸ“Zhiyong Wu,Yaoxiang Wang,Jiacheng Ye,Lingpeng Kong 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Is GPT-3 a Good Data Annotator?**](https://doi.org/10.48550/arXiv.2212.10450) ğŸ‘¨â€ğŸ“Bosheng Ding,Chengwei Qin,Linlin Liu,Lidong Bing,Shafiq R. Joty,Boyang Li etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) ğŸ‘¨â€ğŸ“Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng etc 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-7-red)

[**Structured Prompting: Scaling In-Context Learning to 1, 000 Examples**](https://doi.org/10.48550/arXiv.2212.06713) ğŸ‘¨â€ğŸ“Y. Hao,Yutao Sun,Li Dong,Zhixiong Han,Yuxian Gu,Furu Wei etc 2022 ![](https://img.shields.io/badge/pub-2022--12--13-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru etc 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning**](https://doi.org/10.48550/arXiv.2211.03044) ğŸ‘¨â€ğŸ“Yu Meng,Martin Michalski,Jiaxin Huang,Yu Zhang,T. Abdelzaher,Jiawei Han etc 2022 ![](https://img.shields.io/badge/pub-2022--11--06-green)![](https://img.shields.io/badge/cite-2-red)

[**Large Language Models Are Human-Level Prompt Engineers**](https://doi.org/10.48550/arXiv.2211.01910) ğŸ‘¨â€ğŸ“Yongchao Zhou,Andrei Ioan Muresanu,Ziwen Han,Keiran Paster,Silviu Pitis,Harris Chan etc 2022 ![](https://img.shields.io/badge/pub-2022--11--03-green)![](https://img.shields.io/badge/cite-19-red)

[**ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback**](https://doi.org/10.48550/arXiv.2210.12329) ğŸ‘¨â€ğŸ“Jiacheng Ye,Jiahui Gao,Jiangtao Feng,Zhiyong Wu,Tao Yu,Lingpeng Kong etc 2022 ![](https://img.shields.io/badge/pub-2022--10--22-green)![](https://img.shields.io/badge/cite-3-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) ğŸ‘¨â€ğŸ“Chenglei Si,Zhe Gan,Zhengyuan Yang,Shuohang Wang,Jianfeng Wang,Jordan L. Boyd-Graber etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-9-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-21-red)

[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) ğŸ‘¨â€ğŸ“Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,Soroush Vosoughi etc 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-19-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-17-red)

[**In-context Learning and Induction Heads**](https://doi.org/10.48550/arXiv.2209.11895) ğŸ‘¨â€ğŸ“Catherine Olsson,Nelson Elhage,Neel Nanda,Nicholas Joseph,Nova DasSarma,T. Henighan etc 2022 ![](https://img.shields.io/badge/pub-2022--09--24-green)![](https://img.shields.io/badge/cite-32-red)

[**On the Relation between Sensitivity and Accuracy in In-context Learning**](https://doi.org/10.48550/arXiv.2209.07661) ğŸ‘¨â€ğŸ“Yanda Chen,Chen Zhao,Zhou Yu,K. McKeown,He He 2022 ![](https://img.shields.io/badge/pub-2022--09--16-green)![](https://img.shields.io/badge/cite-8-red)

[**Selective Annotation Makes Language Models Better Few-Shot Learners**](https://doi.org/10.48550/arXiv.2209.01975) ğŸ‘¨â€ğŸ“Hongjin Su,Jungo Kasai,Chen Henry Wu,Weijia Shi,Tianlu Wang,Jiayi Xin etc 2022 ![](https://img.shields.io/badge/pub-2022--09--05-green)![](https://img.shields.io/badge/cite-20-red)

[**What Can Transformers Learn In-Context? A Case Study of Simple Function Classes**](https://doi.org/10.48550/arXiv.2208.01066) ğŸ‘¨â€ğŸ“Shivam Garg,Dimitris Tsipras,Percy Liang,G. Valiant 2022 ![](https://img.shields.io/badge/pub-2022--08--01-green)![](https://img.shields.io/badge/cite-24-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou etc 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-25-red)

[**Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator**](https://doi.org/10.48550/arXiv.2206.08082) ğŸ‘¨â€ğŸ“Hyuhng Joon Kim,Hyunsoo Cho,Junyeob Kim,Taeuk Kim,Kang Min Yoo,Sang-goo Lee etc 2022 ![](https://img.shields.io/badge/pub-2022--06--16-green)![](https://img.shields.io/badge/cite-1-red)

[**Emergent Abilities of Large Language Models**](https://doi.org/10.48550/arXiv.2206.07682) ğŸ‘¨â€ğŸ“Jason Wei,Yi Tay,Rishi Bommasani,Colin Raffel,Barret Zoph,Sebastian Borgeaud etc 2022 ![](https://img.shields.io/badge/pub-2022--06--15-green)![](https://img.shields.io/badge/cite-142-red)

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) ğŸ‘¨â€ğŸ“Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,Tianmin Shu etc 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-25-red)

[**Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations**](https://doi.org/10.48550/arXiv.2205.12685) ğŸ‘¨â€ğŸ“Junyeob Kim,Hyuhng Joon Kim,Hyunsoo Cho,Hwiyeol Jo,Sang-Woo Lee,Sang-goo Lee etc 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-11-red)

[**Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations**](https://doi.org/10.48550/arXiv.2205.11822) ğŸ‘¨â€ğŸ“Jaehun Jung,Lianhui Qin,S. Welleck,Faeze Brahman,Chandra Bhagavatula,Ronan Le Bras etc 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-25-red)

[**Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing**](https://doi.org/10.48550/arXiv.2205.12253) ğŸ‘¨â€ğŸ“Linlu Qiu,Peter Shaw,Panupong Pasupat,Tianze Shi,Jonathan Herzig,Emily Pitler etc 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-10-red)

[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2302.135402205.11916) ğŸ‘¨â€ğŸ“Takeshi Kojima,S. Gu,Machel Reid,Yutaka Matsuo,Yusuke Iwasawa 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-176-red)

[**Instruction Induction: From Few Examples to Natural Language Task Descriptions**](https://doi.org/10.48550/arXiv.2205.10782) ğŸ‘¨â€ğŸ“Or Honovich,Uri Shaham,Samuel R. Bowman,Omer Levy 2022 ![](https://img.shields.io/badge/pub-2022--05--22-green)![](https://img.shields.io/badge/cite-8-red)

[**Prototypical Calibration for Few-shot Learning of Language Models**](https://doi.org/10.48550/arXiv.2205.10183) ğŸ‘¨â€ğŸ“Zhixiong Han,Y. Hao,Li Dong,Yutao Sun,Furu Wei 2022 ![](https://img.shields.io/badge/pub-2022--05--20-green)![](https://img.shields.io/badge/cite-4-red)

[**Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning**](https://doi.org/10.48550/arXiv.2205.05638) ğŸ‘¨â€ğŸ“Haokun Liu,Derek Tam,Mohammed Muqeeth,Jay Mohta,Tenghao Huang,Mohit Bansal etc 2022 ![](https://img.shields.io/badge/pub-2022--05--11-green)![](https://img.shields.io/badge/cite-55-red)

[**The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning**](https://arxiv.org/abs/2302.135402205.03401) ğŸ‘¨â€ğŸ“Xi Ye,Greg Durrett 2022 ![](https://img.shields.io/badge/pub-2022--05--06-green)![](https://img.shields.io/badge/cite-9-red)

[**Improving In-Context Few-Shot Learning via Self-Supervised Training**](https://doi.org/10.48550/arXiv.2205.01703) ğŸ‘¨â€ğŸ“Mingda Chen,Jingfei Du,Ramakanth Pasunuru,Todor Mihaylov,Srini Iyer,V. Stoyanov etc 2022 ![](https://img.shields.io/badge/pub-2022--05--03-green)![](https://img.shields.io/badge/cite-5-red)

[**OPT: Open Pre-trained Transformer Language Models**](https://arxiv.org/abs/2302.135402205.01068) ğŸ‘¨â€ğŸ“Susan Zhang,Stephen Roller,Naman Goyal,Mikel Artetxe,Moya Chen,Shuohui Chen etc 2022 ![](https://img.shields.io/badge/pub-2022--05--02-green)![](https://img.shields.io/badge/cite-296-red)

[**On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model**](https://doi.org/10.48550/arXiv.2204.13509) ğŸ‘¨â€ğŸ“Seongjin Shin,Sang-Woo Lee,Hwijeen Ahn,Sungdong Kim,Hyoungseok Kim,Boseop Kim etc 2022 ![](https://img.shields.io/badge/pub-2022--04--28-green)![](https://img.shields.io/badge/cite-13-red)

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2302.135402204.02311) ğŸ‘¨â€ğŸ“Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-596-red)

[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) ğŸ‘¨â€ğŸ“Andrew Kyle Lampinen,I. Dasgupta,Stephanie C. Y. Chan,Kory Matthewson,Michael Henry Tessler,Antonia Creswell etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-60-red)

[**Prompt-free and Efficient Few-shot Learning with Language Models**](https://doi.org/10.48550/arXiv.2204.01172) ğŸ‘¨â€ğŸ“Rabeeh Karimi Mahabadi,Luke Zettlemoyer,J. Henderson,Marzieh Saeidi,Lambert Mathias,V. Stoyanov etc 2022 ![](https://img.shields.io/badge/pub-2022--04--03-green)![](https://img.shields.io/badge/cite-20-red)

[**Self-Consistency Improves Chain of Thought Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2203.11171) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou etc 2022 ![](https://img.shields.io/badge/pub-2022--03--21-green)![](https://img.shields.io/badge/cite-126-red)

[**Iteratively Prompt Pre-trained Language Models for Chain of Thought**](https://arxiv.org/abs/2302.135402203.08383) ğŸ‘¨â€ğŸ“Boshi Wang,Xiang Deng,Huan Sun 2022 ![](https://img.shields.io/badge/pub-2022--03--16-green)![](https://img.shields.io/badge/cite-5-red)

[**GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models**](https://doi.org/10.48550/arXiv.2203.07281) ğŸ‘¨â€ğŸ“Archiki Prasad,Peter Hase,Xiang Zhou,Mohit Bansal 2022 ![](https://img.shields.io/badge/pub-2022--03--14-green)![](https://img.shields.io/badge/cite-23-red)

[**Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?**](https://arxiv.org/abs/2302.135402202.12837) ğŸ‘¨â€ğŸ“Sewon Min,Xinxi Lyu,Ari Holtzman,Mikel Artetxe,M. Lewis,Hannaneh Hajishirzi etc 2022 ![](https://img.shields.io/badge/pub-2022--02--25-green)![](https://img.shields.io/badge/cite-121-red)

[**AdaPrompt: Adaptive Model Training for Prompt-based NLP**](https://arxiv.org/abs/2302.135402202.04824) ğŸ‘¨â€ğŸ“Yulong Chen,Yang Liu,Li Dong,Shuohang Wang,Chenguang Zhu,Michael Zeng etc 2022 ![](https://img.shields.io/badge/pub-2022--02--10-green)![](https://img.shields.io/badge/cite-11-red)

[**PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts**](https://doi.org/10.18653/v1/2022.acl-demo.9) ğŸ‘¨â€ğŸ“Stephen H. Bach,Victor Sanh,Zheng Xin Yong,Albert Webson,Colin Raffel,Nihal V. Nayak etc 2022 ![](https://img.shields.io/badge/pub-2022--02--02-green)![](https://img.shields.io/badge/cite-53-red)

[**Co-training Improves Prompt-based Learning for Large Language Models**](https://arxiv.org/abs/2302.135402202.00828) ğŸ‘¨â€ğŸ“Hunter Lang,Monica Agrawal,Yoon Kim,D. Sontag 2022 ![](https://img.shields.io/badge/pub-2022--02--02-green)![](https://img.shields.io/badge/cite-8-red)

[**Chain of Thought Prompting Elicits Reasoning in Large Language Models**](https://arxiv.org/abs/2302.135402201.11903) ğŸ‘¨â€ğŸ“Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,Quoc Le etc 2022 ![](https://img.shields.io/badge/pub-2022--01--28-green)![](https://img.shields.io/badge/cite-368-red)

[**Black-box Prompt Learning for Pre-trained Language Models**](https://arxiv.org/abs/2302.135402201.08531) ğŸ‘¨â€ğŸ“Shizhe Diao,Xuechun Li,Yong Lin,Zhichao Huang,Xiao Zhou,Tong Zhang etc 2022 ![](https://img.shields.io/badge/pub-2022--01--21-green)![](https://img.shields.io/badge/cite-17-red)

[**UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models**](https://arxiv.org/abs/2302.135402201.05966) ğŸ‘¨â€ğŸ“Tianbao Xie,Chen Henry Wu,Peng Shi,Ruiqi Zhong,Torsten Scholak,Michihiro Yasunaga etc 2022 ![](https://img.shields.io/badge/pub-2022--01--16-green)![](https://img.shields.io/badge/cite-96-red)

[**Learning To Retrieve Prompts for In-Context Learning**](https://doi.org/10.18653/v1/2022.naacl-main.191) ğŸ‘¨â€ğŸ“Ohad Rubin,Jonathan Herzig,Jonathan Berant 2021 ![](https://img.shields.io/badge/pub-2021--12--16-green)![](https://img.shields.io/badge/cite-80-red)

[**Few-Shot Semantic Parsing with Language Models Trained on Code**](https://doi.org/10.18653/v1/2022.naacl-main.396) ğŸ‘¨â€ğŸ“Richard Shin,Benjamin Van Durme 2021 ![](https://img.shields.io/badge/pub-2021--12--16-green)![](https://img.shields.io/badge/cite-19-red)

[**Scaling Language Models: Methods, Analysis & Insights from Training Gopher**](https://arxiv.org/abs/2302.135402112.11446) ğŸ‘¨â€ğŸ“Jack W. Rae,Sebastian Borgeaud,Trevor Cai,Katie Millican,Jordan Hoffmann,Francis Song etc 2021 ![](https://img.shields.io/badge/pub-2021--12--08-green)![](https://img.shields.io/badge/cite-300-red)

[**True Few-Shot Learning with Promptsâ€”A Real-World Perspective**](https://doi.org/10.1162/tacl_a_00485) ğŸ‘¨â€ğŸ“Timo Schick,Hinrich SchÃ¼tze 2021 ![](https://img.shields.io/badge/pub-2021--11--26-green)![](https://img.shields.io/badge/cite-14-red)

[**An Explanation of In-context Learning as Implicit Bayesian Inference**](https://arxiv.org/abs/2302.135402111.02080) ğŸ‘¨â€ğŸ“Sang Michael Xie,Aditi Raghunathan,Percy Liang,Tengyu Ma 2021 ![](https://img.shields.io/badge/pub-2021--11--03-green)![](https://img.shields.io/badge/cite-52-red)

[**MetaICL: Learning to Learn In Context**](https://doi.org/10.18653/v1/2022.naacl-main.201) ğŸ‘¨â€ğŸ“Sewon Min,M. Lewis,Luke Zettlemoyer,Hannaneh Hajishirzi 2021 ![](https://img.shields.io/badge/pub-2021--10--29-green)![](https://img.shields.io/badge/cite-80-red)

[**Meta-learning via Language Model In-context Tuning**](https://doi.org/10.18653/v1/2022.acl-long.53) ğŸ‘¨â€ğŸ“Yanda Chen,Ruiqi Zhong,Sheng Zha,G. Karypis,He He 2021 ![](https://img.shields.io/badge/pub-2021--10--15-green)![](https://img.shields.io/badge/cite-31-red)

[**Few-Shot Bot: Prompt-Based Learning for Dialogue Systems**](https://arxiv.org/abs/2302.135402110.08118) ğŸ‘¨â€ğŸ“Andrea Madotto,Zhaojiang Lin,Genta Indra Winata,Pascale Fung 2021 ![](https://img.shields.io/badge/pub-2021--10--15-green)![](https://img.shields.io/badge/cite-24-red)

[**Coherence boosting: When your pretrained language model is not paying enough attention**](https://doi.org/10.18653/v1/2022.acl-long.565) ğŸ‘¨â€ğŸ“Nikolay Malkin,Zhen Wang,N. Jojic 2021 ![](https://img.shields.io/badge/pub-2021--10--15-green)![](https://img.shields.io/badge/cite-4-red)

[**Reframing Instructional Prompts to GPTkâ€™s Language**](https://doi.org/10.18653/v1/2022.findings-acl.50) ğŸ‘¨â€ğŸ“Swaroop Mishra,Daniel Khashabi,Chitta Baral,Yejin Choi,Hannaneh Hajishirzi 2021 ![](https://img.shields.io/badge/pub-2021--09--16-green)![](https://img.shields.io/badge/cite-55-red)

[**Finetuned Language Models Are Zero-Shot Learners**](https://arxiv.org/abs/2302.135402109.01652) ğŸ‘¨â€ğŸ“Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,A. Yu,Brian Lester etc 2021 ![](https://img.shields.io/badge/pub-2021--09--03-green)![](https://img.shields.io/badge/cite-369-red)

[**Do Prompt-Based Models Really Understand the Meaning of Their Prompts?**](https://doi.org/10.18653/v1/2022.naacl-main.167) ğŸ‘¨â€ğŸ“Albert Webson,Ellie Pavlick 2021 ![](https://img.shields.io/badge/pub-2021--09--02-green)![](https://img.shields.io/badge/cite-66-red)

[**Noisy Channel Language Model Prompting for Few-Shot Text Classification**](https://doi.org/10.18653/v1/2022.acl-long.365) ğŸ‘¨â€ğŸ“Sewon Min,Michael Lewis,Hannaneh Hajishirzi,Luke Zettlemoyer 2021 ![](https://img.shields.io/badge/pub-2021--08--09-green)![](https://img.shields.io/badge/cite-69-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig etc 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**Evaluating Large Language Models Trained on Code**](https://arxiv.org/abs/2302.135402107.03374) ğŸ‘¨â€ğŸ“Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan etc 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-594-red)

[**Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models**](https://doi.org/10.18653/v1/2022.findings-acl.222) ğŸ‘¨â€ğŸ“Robert L Logan IV,Ivana Balavzevi'c,Eric Wallace,Fabio Petroni,Sameer Singh,Sebastian Riedel etc 2021 ![](https://img.shields.io/badge/pub-2021--06--24-green)![](https://img.shields.io/badge/cite-70-red)

[**True Few-Shot Learning with Language Models**](https://arxiv.org/abs/2302.135402105.11447) ğŸ‘¨â€ğŸ“Ethan Perez,Douwe Kiela,Kyunghyun Cho 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-148-red)

[**The Power of Scale for Parameter-Efficient Prompt Tuning**](https://doi.org/10.18653/v1/2021.emnlp-main.243) ğŸ‘¨â€ğŸ“Brian Lester,Rami Al-Rfou,Noah Constant 2021 ![](https://img.shields.io/badge/pub-2021--04--18-green)![](https://img.shields.io/badge/cite-675-red)

[**Constrained Language Models Yield Few-Shot Semantic Parsers**](https://doi.org/10.18653/v1/2021.emnlp-main.608) ğŸ‘¨â€ğŸ“Richard Shin,C. H. Lin,Sam Thomson,Charles C. Chen,Subhro Roy,Emmanouil Antonios Platanios etc 2021 ![](https://img.shields.io/badge/pub-2021--04--18-green)![](https://img.shields.io/badge/cite-79-red)

[**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity**](https://doi.org/10.18653/v1/2022.acl-long.556) ğŸ‘¨â€ğŸ“Yao Lu,Max Bartolo,Alastair Moore,S. Riedel,Pontus Stenetorp 2021 ![](https://img.shields.io/badge/pub-2021--04--18-green)![](https://img.shields.io/badge/cite-165-red)

[**Surface Form Competition: Why the Highest Probability Answer Isnâ€™t Always Right**](https://doi.org/10.18653/v1/2021.emnlp-main.564) ğŸ‘¨â€ğŸ“Ari Holtzman,Peter West,Vered Schwartz,Yejin Choi,Luke Zettlemoyer 2021 ![](https://img.shields.io/badge/pub-2021--04--16-green)![](https://img.shields.io/badge/cite-73-red)

[**GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow**](https://doi.org/10.5281/ZENODO.5297715) ğŸ‘¨â€ğŸ“Sid Black,Leo Gao,Phil Wang,Connor Leahy,Stella Rose Biderman 2021 ![](https://img.shields.io/badge/pub-2021--03--21-green)![](https://img.shields.io/badge/cite-198-red)

[**Calibrate Before Use: Improving Few-Shot Performance of Language Models**](https://arxiv.org/abs/2302.135402102.09690) ğŸ‘¨â€ğŸ“Tony Zhao,Eric Wallace,Shi Feng,D. Klein,Sameer Singh 2021 ![](https://img.shields.io/badge/pub-2021--02--19-green)![](https://img.shields.io/badge/cite-277-red)

[**What Makes Good In-Context Examples for GPT-3?**](https://doi.org/10.18653/v1/2022.deelio-1.10) ğŸ‘¨â€ğŸ“Jiachang Liu,Dinghan Shen,Yizhe Zhang,Bill Dolan,L. Carin,Weizhu Chen etc 2021 ![](https://img.shields.io/badge/pub-2021--01--17-green)![](https://img.shields.io/badge/cite-163-red)

[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) ğŸ‘¨â€ğŸ“Tianyu Gao,Adam Fisch,Danqi Chen 2021 ![](https://img.shields.io/badge/pub-2021--01--01-green)![](https://img.shields.io/badge/cite-635-red)

[**The Pile: An 800GB Dataset of Diverse Text for Language Modeling**](https://arxiv.org/abs/2302.135402101.00027) ğŸ‘¨â€ğŸ“Leo Gao,Stella Rose Biderman,Sid Black,Laurence Golding,Travis Hoppe,Charles Foster etc 2020 ![](https://img.shields.io/badge/pub-2020--12--31-green)![](https://img.shields.io/badge/cite-328-red)

[**Language Models are Few-Shot Learners**](https://arxiv.org/abs/2302.135402005.14165) ğŸ‘¨â€ğŸ“Tom B. Brown,Benjamin Mann,Nick Ryder,Melanie Subbiah,J. Kaplan,Prafulla Dhariwal etc 2020 ![](https://img.shields.io/badge/pub-2020--05--28-green)![](https://img.shields.io/badge/cite-8351-red)

[**ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**](https://arxiv.org/abs/2302.135402003.10555) ğŸ‘¨â€ğŸ“Kevin Clark,Minh-Thang Luong,Quoc V. Le,Christopher D. Manning 2020 ![](https://img.shields.io/badge/pub-2020--03--23-green)![](https://img.shields.io/badge/cite-1846-red)

[**Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference**](https://doi.org/10.18653/v1/2021.eacl-main.20) ğŸ‘¨â€ğŸ“Timo Schick,Hinrich SchÃ¼tze 2020 ![](https://img.shields.io/badge/pub-2020--01--21-green)![](https://img.shields.io/badge/cite-579-red)

[**Momentum Contrast for Unsupervised Visual Representation Learning**](https://doi.org/10.1109/cvpr42600.2020.00975) ğŸ‘¨â€ğŸ“Kaiming He,Haoqi Fan,Yuxin Wu,Saining Xie,Ross B. Girshick 2019 ![](https://img.shields.io/badge/pub-2019--11--13-green)![](https://img.shields.io/badge/cite-5429-red)

[**BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**](https://doi.org/10.18653/v1/2020.acl-main.703) ğŸ‘¨â€ğŸ“M. Lewis,Yinhan Liu,Naman Goyal,Marjan Ghazvininejad,Abdelrahman Mohamed,Omer Levy etc 2019 ![](https://img.shields.io/badge/pub-2019--10--29-green)![](https://img.shields.io/badge/cite-4052-red)

[**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**](https://arxiv.org/abs/2302.135401910.10683) ğŸ‘¨â€ğŸ“Colin Raffel,Noam M. Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena etc 2019 ![](https://img.shields.io/badge/pub-2019--10--23-green)![](https://img.shields.io/badge/cite-6286-red)

[**HuggingFace's Transformers: State-of-the-art Natural Language Processing**](https://arxiv.org/abs/2302.135401910.03771) ğŸ‘¨â€ğŸ“Thomas Wolf,Lysandre Debut,Victor Sanh,Julien Chaumond,Clement Delangue,Anthony Moi etc 2019 ![](https://img.shields.io/badge/pub-2019--10--09-green)![](https://img.shields.io/badge/cite-3513-red)

[**Transformers: State-of-the-Art Natural Language Processing**](https://doi.org/10.18653/v1/2020.emnlp-demos.6) ğŸ‘¨â€ğŸ“Thomas Wolf,Lysandre Debut,Victor Sanh,Julien Chaumond,Clement Delangue,Anthony Moi etc 2019 ![](https://img.shields.io/badge/pub-2019--10--09-green)![](https://img.shields.io/badge/cite-3941-red)

[**ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**](https://arxiv.org/abs/2302.135401909.11942) ğŸ‘¨â€ğŸ“Zhenzhong Lan,Mingda Chen,Sebastian Goodman,Kevin Gimpel,Piyush Sharma,Radu Soricut etc 2019 ![](https://img.shields.io/badge/pub-2019--09--26-green)![](https://img.shields.io/badge/cite-3711-red)

[**RoBERTa: A Robustly Optimized BERT Pretraining Approach**](https://arxiv.org/abs/2302.135401907.11692) ğŸ‘¨â€ğŸ“Yinhan Liu,Myle Ott,Naman Goyal,Jingfei Du,Mandar Joshi,Danqi Chen etc 2019 ![](https://img.shields.io/badge/pub-2019--07--26-green)![](https://img.shields.io/badge/cite-10930-red)

[**BERTScore: Evaluating Text Generation with BERT**](https://arxiv.org/abs/2302.135401904.09675) ğŸ‘¨â€ğŸ“Tianyi Zhang,Varsha Kishore,Felix Wu,Kilian Q. Weinberger,Yoav Artzi 2019 ![](https://img.shields.io/badge/pub-2019--04--21-green)![](https://img.shields.io/badge/cite-1634-red)

[**BioBERT: a pre-trained biomedical language representation model for biomedical text mining**](https://doi.org/10.1093/bioinformatics/btz682) ğŸ‘¨â€ğŸ“Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So etc 2019 ![](https://img.shields.io/badge/pub-2019--01--25-green)![](https://img.shields.io/badge/cite-2743-red)

[**Representation Learning with Contrastive Predictive Coding**](https://arxiv.org/abs/2302.135401807.03748) ğŸ‘¨â€ğŸ“AÃ¤ron van den Oord,Yazhe Li,Oriol Vinyals 2018 ![](https://img.shields.io/badge/pub-2018--07--10-green)![](https://img.shields.io/badge/cite-4597-red)

[**Know What You Donâ€™t Know: Unanswerable Questions for SQuAD**](https://doi.org/10.18653/v1/P18-2124) ğŸ‘¨â€ğŸ“Pranav Rajpurkar,Robin Jia,Percy Liang 2018 ![](https://img.shields.io/badge/pub-2018--06--11-green)![](https://img.shields.io/badge/cite-1718-red)

[**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding**](https://doi.org/10.18653/v1/W18-5446) ğŸ‘¨â€ğŸ“Alex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman etc 2018 ![](https://img.shields.io/badge/pub-2018--04--20-green)![](https://img.shields.io/badge/cite-3499-red)

[**UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**](https://arxiv.org/abs/2302.135401802.03426) ğŸ‘¨â€ğŸ“Leland McInnes,John Healy 2018 ![](https://img.shields.io/badge/pub-2018--02--09-green)![](https://img.shields.io/badge/cite-5118-red)

[**Decoupled Weight Decay Regularization**](https://api.semanticscholar.org/d07284a6811f1b2745d91bdb06b040b57f226882) ğŸ‘¨â€ğŸ“I. Loshchilov,F. Hutter 2017 ![](https://img.shields.io/badge/pub-2017--11--14-green)![](https://img.shields.io/badge/cite-6264-red)

[**On Calibration of Modern Neural Networks**](https://arxiv.org/abs/2302.135401706.04599) ğŸ‘¨â€ğŸ“Chuan Guo,Geoff Pleiss,Yu Sun,Kilian Q. Weinberger 2017 ![](https://img.shields.io/badge/pub-2017--06--14-green)![](https://img.shields.io/badge/cite-3066-red)

[**Attention is All you Need**](https://arxiv.org/abs/2302.135401706.03762) ğŸ‘¨â€ğŸ“Ashish Vaswani,Noam M. Shazeer,Niki Parmar,Jakob Uszkoreit,Llion Jones,Aidan N. Gomez etc 2017 ![](https://img.shields.io/badge/pub-2017--06--12-green)![](https://img.shields.io/badge/cite-51971-red)

[**Curiosity-Driven Exploration by Self-Supervised Prediction**](https://doi.org/10.1109/CVPRW.2017.70) ğŸ‘¨â€ğŸ“Deepak Pathak,Pulkit Agrawal,Alexei A. Efros,Trevor Darrell 2017 ![](https://img.shields.io/badge/pub-2017--05--15-green)![](https://img.shields.io/badge/cite-1672-red)

[**A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference**](https://doi.org/10.18653/v1/N18-1101) ğŸ‘¨â€ğŸ“Adina Williams,Nikita Nangia,Samuel R. Bowman 2017 ![](https://img.shields.io/badge/pub-2017--04--18-green)![](https://img.shields.io/badge/cite-2610-red)

[**Understanding Black-box Predictions via Influence Functions**](https://arxiv.org/abs/2302.135401703.04730) ğŸ‘¨â€ğŸ“Pang Wei Koh,Percy Liang 2017 ![](https://img.shields.io/badge/pub-2017--03--14-green)![](https://img.shields.io/badge/cite-1811-red)

[**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**](https://arxiv.org/abs/2302.135401703.03400) ğŸ‘¨â€ğŸ“Chelsea Finn,P. Abbeel,S. Levine 2017 ![](https://img.shields.io/badge/pub-2017--03--09-green)![](https://img.shields.io/badge/cite-7087-red)

[**Axiomatic Attribution for Deep Networks**](https://arxiv.org/abs/2302.135401703.01365) ğŸ‘¨â€ğŸ“Mukund Sundararajan,Ankur Taly,Qiqi Yan 2017 ![](https://img.shields.io/badge/pub-2017--03--04-green)![](https://img.shields.io/badge/cite-3082-red)

[**Billion-Scale Similarity Search with GPUs**](https://doi.org/10.1109/tbdata.2019.2921572) ğŸ‘¨â€ğŸ“Jeff Johnson,Matthijs Douze,H. JÃ©gou 2017 ![](https://img.shields.io/badge/pub-2017--02--28-green)![](https://img.shields.io/badge/cite-1725-red)

[**Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles**](https://arxiv.org/abs/2302.135401612.01474) ğŸ‘¨â€ğŸ“Balaji Lakshminarayanan,A. Pritzel,C. Blundell 2016 ![](https://img.shields.io/badge/pub-2016--12--05-green)![](https://img.shields.io/badge/cite-3175-red)

[**Understanding deep learning requires rethinking generalization**](https://arxiv.org/abs/2302.135401611.03530) ğŸ‘¨â€ğŸ“Chiyuan Zhang,Samy Bengio,Moritz Hardt,B. Recht,Oriol Vinyals 2016 ![](https://img.shields.io/badge/pub-2016--11--04-green)![](https://img.shields.io/badge/cite-3936-red)

[**Optimization as a Model for Few-Shot Learning**](https://api.semanticscholar.org/29c887794eed2ca9462638ff853e6fe1ab91d5d8) ğŸ‘¨â€ğŸ“S. Ravi,H. Larochelle 2016 ![](https://img.shields.io/badge/pub-2016--11--04-green)![](https://img.shields.io/badge/cite-2567-red)

[**A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks**](https://arxiv.org/abs/2302.135401610.02136) ğŸ‘¨â€ğŸ“Dan Hendrycks,Kevin Gimpel 2016 ![](https://img.shields.io/badge/pub-2016--10--07-green)![](https://img.shields.io/badge/cite-1788-red)

[**SQuAD: 100,000+ Questions for Machine Comprehension of Text**](https://doi.org/10.18653/v1/D16-1264) ğŸ‘¨â€ğŸ“Pranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang 2016 ![](https://img.shields.io/badge/pub-2016--06--16-green)![](https://img.shields.io/badge/cite-5197-red)

[**Matching Networks for One Shot Learning**](https://arxiv.org/abs/2302.135401606.04080) ğŸ‘¨â€ğŸ“Oriol Vinyals,C. Blundell,T. Lillicrap,K. Kavukcuoglu,Daan Wierstra 2016 ![](https://img.shields.io/badge/pub-2016--06--13-green)![](https://img.shields.io/badge/cite-4816-red)

[**â€œWhy Should I Trust You?â€: Explaining the Predictions of Any Classifier**](https://doi.org/10.1145/2939672.2939778) ğŸ‘¨â€ğŸ“Marco Tulio Ribeiro,Sameer Singh,Carlos Guestrin 2016 ![](https://img.shields.io/badge/pub-2016--02--16-green)![](https://img.shields.io/badge/cite-9632-red)

[**Deep Residual Learning for Image Recognition**](https://doi.org/10.1109/cvpr.2016.90) ğŸ‘¨â€ğŸ“Kaiming He,X. Zhang,Shaoqing Ren,Jian Sun 2015 ![](https://img.shields.io/badge/pub-2015--12--10-green)![](https://img.shields.io/badge/cite-122183-red)

[**Character-level Convolutional Networks for Text Classification**](https://arxiv.org/abs/2302.135401509.01626) ğŸ‘¨â€ğŸ“Xiang Zhang,J. Zhao,Yann LeCun 2015 ![](https://img.shields.io/badge/pub-2015--09--04-green)![](https://img.shields.io/badge/cite-4160-red)

[**A large annotated corpus for learning natural language inference**](https://doi.org/10.18653/v1/D15-1075) ğŸ‘¨â€ğŸ“Samuel R. Bowman,Gabor Angeli,Christopher Potts,Christopher D. Manning 2015 ![](https://img.shields.io/badge/pub-2015--08--21-green)![](https://img.shields.io/badge/cite-2999-red)

[**ActivityNet: A large-scale video benchmark for human activity understanding**](https://doi.org/10.1109/CVPR.2015.7298698) ğŸ‘¨â€ğŸ“Fabian Caba Heilbron,Victor Escorcia,Bernard Ghanem,Juan Carlos Niebles 2015 ![](https://img.shields.io/badge/pub-2015--06--07-green)![](https://img.shields.io/badge/cite-1612-red)

[**Adam: A Method for Stochastic Optimization**](https://arxiv.org/abs/2302.135401412.6980) ğŸ‘¨â€ğŸ“Diederik P. Kingma,Jimmy Ba 2014 ![](https://img.shields.io/badge/pub-2014--12--22-green)![](https://img.shields.io/badge/cite-110816-red)

[**Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps**](https://arxiv.org/abs/2302.135401312.6034) ğŸ‘¨â€ğŸ“K. Simonyan,A. Vedaldi,Andrew Zisserman 2013 ![](https://img.shields.io/badge/pub-2013--12--20-green)![](https://img.shields.io/badge/cite-5229-red)

[**Playing Atari with Deep Reinforcement Learning**](https://arxiv.org/abs/2302.135401312.5602) ğŸ‘¨â€ğŸ“Volodymyr Mnih,K. Kavukcuoglu,David Silver,A. Graves,Ioannis Antonoglou,Daan Wierstra etc 2013 ![](https://img.shields.io/badge/pub-2013--12--19-green)![](https://img.shields.io/badge/cite-8414-red)

[**Hidden factors and hidden topics: understanding rating dimensions with review text**](https://doi.org/10.1145/2507157.2507163) ğŸ‘¨â€ğŸ“Julian McAuley,J. Leskovec 2013 ![](https://img.shields.io/badge/pub-2013--10--12-green)![](https://img.shields.io/badge/cite-1517-red)

[**Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank**](https://api.semanticscholar.org/687bac2d3320083eb4530bf18bb8f8f721477600) ğŸ‘¨â€ğŸ“R. Socher,Alex Perelygin,Jean Wu,Jason Chuang,Christopher D. Manning,A. Ng etc 2013 ![](https://img.shields.io/badge/pub-2013--10--01-green)![](https://img.shields.io/badge/cite-6124-red)

[**Efficient Estimation of Word Representations in Vector Space**](https://arxiv.org/abs/2302.135401301.3781) ğŸ‘¨â€ğŸ“Tomas Mikolov,Kai Chen,G. Corrado,J. Dean 2013 ![](https://img.shields.io/badge/pub-2013--01--16-green)![](https://img.shields.io/badge/cite-25157-red)

[**The NumPy Array: A Structure for Efficient Numerical Computation**](https://doi.org/10.1109/MCSE.2011.37) ğŸ‘¨â€ğŸ“S. Walt,S. Colbert,G. Varoquaux 2011 ![](https://img.shields.io/badge/pub-2011--02--07-green)![](https://img.shields.io/badge/cite-6594-red)

[**Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit**](https://doi.org/10.1007/s10579-010-9124-x) ğŸ‘¨â€ğŸ“Wiebke Wagner 2010 ![](https://img.shields.io/badge/pub-2010--12--01-green)![](https://img.shields.io/badge/cite-1724-red)

[**Why Does Unsupervised Pre-training Help Deep Learning?**](https://doi.org/10.5555/1756006.1756025) ğŸ‘¨â€ğŸ“D. Erhan,Aaron C. Courville,Yoshua Bengio,Pascal Vincent 2010 ![](https://img.shields.io/badge/pub-2010--03--01-green)![](https://img.shields.io/badge/cite-2072-red)

[**The Probabilistic Relevance Framework: BM25 and Beyond**](https://doi.org/10.1561/1500000019) ğŸ‘¨â€ğŸ“S. Robertson,H. Zaragoza 2009 ![](https://img.shields.io/badge/pub-2009--04--01-green)![](https://img.shields.io/badge/cite-1854-red)

[**Learning to rank for information retrieval**](https://doi.org/10.1007/978-3-642-14267-3) ğŸ‘¨â€ğŸ“Tie-Yan Liu 2009 ![](https://img.shields.io/badge/pub-2009--03--01-green)![](https://img.shields.io/badge/cite-2730-red)

[**The PASCAL Recognising Textual Entailment Challenge**](https://doi.org/10.1007/11736790_9) ğŸ‘¨â€ğŸ“Ido Dagan,Oren Glickman,B. Magnini 2007 ![](https://img.shields.io/badge/pub-2007--06--28-green)![](https://img.shields.io/badge/cite-1948-red)

[**Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales**](https://doi.org/10.3115/1219840.1219855) ğŸ‘¨â€ğŸ“B. Pang,Lillian Lee 2005 ![](https://img.shields.io/badge/pub-2005--06--17-green)![](https://img.shields.io/badge/cite-2509-red)

[**A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts**](https://doi.org/10.3115/1218955.1218990) ğŸ‘¨â€ğŸ“B. Pang,Lillian Lee 2004 ![](https://img.shields.io/badge/pub-2004--07--21-green)![](https://img.shields.io/badge/cite-3765-red)

[**Latent Dirichlet Allocation**](https://doi.org/10.1016/B978-0-12-411519-4.00006-9) ğŸ‘¨â€ğŸ“D. Blei,A. Ng,Michael I. Jordan 2001 ![](https://img.shields.io/badge/pub-2001--01--03-green)![](https://img.shields.io/badge/cite-33642-red)

[**Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping**](https://api.semanticscholar.org/94066dc12fe31e96af7557838159bde598cb4f10) ğŸ‘¨â€ğŸ“A. Ng,D. Harada,Stuart J. Russell 1999 ![](https://img.shields.io/badge/pub-1999--06--27-green)![](https://img.shields.io/badge/cite-1797-red)

[**OPTICS: ordering points to identify the clustering structure**](https://doi.org/10.1145/304182.304187) ğŸ‘¨â€ğŸ“M. Ankerst,M. Breunig,H. Kriegel,J. Sander 1999 ![](https://img.shields.io/badge/pub-1999--06--01-green)![](https://img.shields.io/badge/cite-3993-red)

[**An Introduction to Variational Methods for Graphical Models**](https://doi.org/10.1023/A:1007665907178) ğŸ‘¨â€ğŸ“Michael I. Jordan,Zoubin Ghahramani,T. Jaakkola,L. Saul 1999 ![](https://img.shields.io/badge/pub-1999--02--01-green)![](https://img.shields.io/badge/cite-4039-red)

[**Long Short-Term Memory**](https://doi.org/10.1162/neco.1997.9.8.1735) ğŸ‘¨â€ğŸ“S. Hochreiter,J. Schmidhuber 1997 ![](https://img.shields.io/badge/pub-1997--11--01-green)![](https://img.shields.io/badge/cite-61441-red)

[**Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.**](https://doi.org/10.1037/0033-295X.102.3.419) ğŸ‘¨â€ğŸ“James L. McClelland,B. McNaughton,R. Oâ€™Reilly 1995 ![](https://img.shields.io/badge/pub-1995--07--01-green)![](https://img.shields.io/badge/cite-4614-red)

[**Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans.**](https://doi.org/10.1037/0033-295X.99.2.195) ğŸ‘¨â€ğŸ“L. Squire 1992 ![](https://img.shields.io/badge/pub-1992--03--30-green)![](https://img.shields.io/badge/cite-5284-red)

[**Learning internal representations by error propagation**](https://doi.org/10.1016/B978-1-4832-1446-7.50035-2) ğŸ‘¨â€ğŸ“D. Rumelhart,Geoffrey E. Hinton,Ronald J. Williams 1986 ![](https://img.shields.io/badge/pub-1986--01--03-green)![](https://img.shields.io/badge/cite-20072-red)

[**Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper**](https://doi.org/10.1111/J.2517-6161.1977.TB01600.X) ğŸ‘¨â€ğŸ“A. Dempster,N. Laird,D. Rubin 1977 ![](https://img.shields.io/badge/pub-1977--09--01-green)![](https://img.shields.io/badge/cite-50347-red)

[**Monte Carlo Sampling Methods Using Markov Chains and Their Applications**](https://doi.org/10.1093/BIOMET/57.1.97) ğŸ‘¨â€ğŸ“W. K. Hastings 1970 ![](https://img.shields.io/badge/pub-1970--04--01-green)![](https://img.shields.io/badge/cite-14415-red)

[**Statistical Inference for Probabilistic Functions of Finite State Markov Chains**](https://doi.org/10.1214/AOMS/1177699147) ğŸ‘¨â€ğŸ“L. Baum,T. Petrie 1966 ![](https://img.shields.io/badge/pub-1966--12--01-green)![](https://img.shields.io/badge/cite-2906-red)

[**A Markovian Decision Process**](https://doi.org/10.1512/IUMJ.1957.6.56038) ğŸ‘¨â€ğŸ“R. Bellman 1957 ![](https://img.shields.io/badge/pub-1957--04--18-green)![](https://img.shields.io/badge/cite-2103-red)

[**Equation of state calculations by fast computing machines**](https://doi.org/10.1063/1.1699114) ğŸ‘¨â€ğŸ“N. Metropolis,A. W. Rosenbluth,M. Rosenbluth,A. H. Teller,E. Teller 1953 ![](https://img.shields.io/badge/pub-1953--06--01-green)![](https://img.shields.io/badge/cite-33689-red)

[**DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia**](https://doi.org/10.3233/SW-140134) ğŸ‘¨â€ğŸ“Jens Lehmann,Robert Isele,Max Jakob,Anja Jentzsch,D. Kontokostas,Pablo N. Mendes etc 2015 ![](https://img.shields.io/badge/cite-2645-red)

[**Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods**](https://api.semanticscholar.org/384bb3944abe9441dcd2cede5e7cd7353e9ee5f7) ğŸ‘¨â€ğŸ“J. Platt 1999 ![](https://img.shields.io/badge/cite-5501-red)

[**Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition**](https://api.semanticscholar.org/b54bcfca3fddc26b8889739a247a25e445818149) ğŸ‘¨â€ğŸ“Dan Jurafsky,James H. Martin 2000 ![](https://img.shields.io/badge/cite-3977-red)

[**Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond**](https://doi.org/10.1109/tnn.2005.848998) ğŸ‘¨â€ğŸ“A. Atiya 2005 ![](https://img.shields.io/badge/cite-8566-red)

[**Stochastic Neighbor Embedding**](https://api.semanticscholar.org/14d46c6396837986bb4b9a14024cb64797b8c6c0) ğŸ‘¨â€ğŸ“Geoffrey E. Hinton,S. Roweis 2002 ![](https://img.shields.io/badge/cite-1601-red)

[**SMOTE: Synthetic Minority Over-sampling Technique**](https://doi.org/10.1613/jair.953) ğŸ‘¨â€ğŸ“N. Chawla,K. Bowyer,L. Hall,W. Kegelmeyer 2002 ![](https://img.shields.io/badge/cite-17180-red)

[**Improving Language Understanding by Generative Pre-Training**](https://api.semanticscholar.org/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035) ğŸ‘¨â€ğŸ“Alec Radford,Karthik Narasimhan 2018 ![](https://img.shields.io/badge/cite-4775-red)

[**The use of MMR, diversity-based reranking for reordering documents and producing summaries**](https://doi.org/10.1145/290941.291025) ğŸ‘¨â€ğŸ“J. Carbonell,Jade Goldstein-Stewart 1998 ![](https://img.shields.io/badge/cite-2508-red)

[**Human behavior and the principle of least effort**](https://doi.org/10.1037/h0052803) ğŸ‘¨â€ğŸ“G. Zipf 1949 ![](https://img.shields.io/badge/cite-7346-red)

[**Language Models are Unsupervised Multitask Learners**](https://api.semanticscholar.org/9405cc0d6169988371b2755e573cc28650d14dfe) ğŸ‘¨â€ğŸ“Alec Radford,Jeff Wu,Rewon Child,D. Luan,Dario Amodei,Ilya Sutskever etc 2019 ![](https://img.shields.io/badge/cite-8849-red)

[**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**](https://doi.org/10.18653/v1/N19-1423) ğŸ‘¨â€ğŸ“Jacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova 2019 ![](https://img.shields.io/badge/cite-47696-red)

[**Active Learning Literature Survey**](https://api.semanticscholar.org/818826f356444f3daa3447755bf63f171f39ec47) ğŸ‘¨â€ğŸ“Burr Settles 2009 ![](https://img.shields.io/badge/cite-5213-red)

[**of the Association for Computational Linguistics:**](https://doi.org/10.1016/b0-08-044854-2/05234-2) ğŸ‘¨â€ğŸ“Vladimir Meza Ruiz,Rashmi Gangadharaiah,Maria Leonor Pacheco,Danqi Chen,Ryan Cotterell 2001 ![](https://img.shields.io/badge/cite-4067-red)

# CONTINUE...
