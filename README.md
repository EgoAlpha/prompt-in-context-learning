<div align="center">

<img src="./figures/Prompt-EgoAlpha_white.svg" width="600px">

 <div align="center">
    <a href="https://blog.sunguoqi.com/">
      <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&width=435&lines=%22Hello%2C%20World%22;Welcome to our project!&center=true&size=27" alt="Typing SVG" />
    </a>
  </div>

**Awesome resources for in-context learning and prompt engineering: Mastery of the latest LLMs such as ChatGPT, GPT-3, and FlanT5.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./README_zh.md">ç®€ä½“ä¸­æ–‡</a>
    <p>
</h4>
<p align="center">
  <a href="#papersğŸ“œ">ğŸ“ Papers</a> |
  <a href="./Playground.md">âš¡ï¸  Playground</a> |
  <a href="./PromptEngineering.md">ğŸ›  Prompt Engineering</a> |
  <a href="./chatgptprompt.md">ğŸŒ ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **â­ï¸ Shining â­ï¸:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, letâ€™s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*ğŸ‰[Papers](#papersğŸ“œ)ğŸ‰*:  The latest papers about in-context learning or prompt engineering. 

*ğŸ‰[Playground](./Playground.md)ğŸ‰*:  Large language models that enable prompt experimentation. 

*ğŸ‰[Prompt Engineering](./PromptEngineering.md)ğŸ‰*: Prompt techniques for leveraging large language models. 

*ğŸ‰[ChatGPT Prompt](./chatgptprompt.md)ğŸ‰*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
ğŸ’EgoAlpha: Hello! humanğŸ‘¤, are you ready?
```  

# ğŸ“¢ News

- **[2023.3.4]** We establish this project that is organised by professor Yu Liu from EgoAlpha Lab.

<img width="200%" src="./figures/hr.gif" />

# PapersğŸ“œ

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
<!-- - [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph) -->

---

## Prompt Engineering

### ğŸ“Œ Prompt Design

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) ğŸ‘¨â€ğŸ“Xuanting Chen,Junjie Ye,Can Zu,Nuo Xu,Rui Zheng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks**](https://doi.org/10.48550/arXiv.2303.00733) ğŸ‘¨â€ğŸ“Kai-Wei Chang,Yu-Kai Wang,Hua Shen,Iu-thing Kang,W. Tseng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis**](https://doi.org/10.48550/arXiv.2303.00815) ğŸ‘¨â€ğŸ“Jingli Shi,Weihua Li,Quan-wei Bai,Yi Yang,Jianhua Jiang 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**EvoPrompting: Language Models for Code-Level Neural Architecture Search**](https://doi.org/10.48550/arXiv.2302.14838) ğŸ‘¨â€ğŸ“Angelica Chen,David Dohan,David R. So 2023 ![](https://img.shields.io/badge/pub-2023--02--28-green)

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) ğŸ‘¨â€ğŸ“Elliot Meyerson,M. Nelson,Herbie Bradley,Arash Moradi,Amy K. Hoover,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models**](https://doi.org/10.48550/arXiv.2302.12173) ğŸ‘¨â€ğŸ“Kai Greshake,Sahar Abdelnabi,Shailesh Mishra,C. Endres,Thorsten Holz,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) ğŸ‘¨â€ğŸ“Jules White,Quchen Fu,Sam Hays,M. Sandborn,Carlos Olea,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales**](https://doi.org/10.48550/arXiv.2302.08961) ğŸ‘¨â€ğŸ“M. Ruskov 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) ğŸ‘¨â€ğŸ“Albert Lu,Hongxin Zhang,Yanzhe Zhang,Xuezhi Wang,Diyi Yang 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)



ğŸ‘‰[Complete paper list ğŸ”— for prompt design](./PaperList/PromptDesignList.md)ğŸ‘ˆ

### ğŸ“Œ Automatic Prompt 

[**Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data**](https://doi.org/10.48550/arXiv.2302.12822) ğŸ‘¨â€ğŸ“Kashun Shum,Shizhe Diao,Tong Zhang 2023 ![](https://img.shields.io/badge/pub-2023--02--24-green)

[**Guiding Large Language Models via Directional Stimulus Prompting**](https://doi.org/10.48550/arXiv.2302.11520) ğŸ‘¨â€ğŸ“Zekun Li,Baolin Peng,Pengcheng He,Michel Galley,Jianfeng Gao,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) ğŸ‘¨â€ğŸ“Yoichi Ishibashi,D. Bollegala,Katsuhito Sudoh,Satoshi Nakamura 2023 ![](https://img.shields.io/badge/pub-2023--02--11-green)

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--07-green)![](https://img.shields.io/badge/cite-2-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language**](https://doi.org/10.1145/3545945.3569823) ğŸ‘¨â€ğŸ“Paul Denny,Viraj Kumar,Nasser Giacaman 2022 ![](https://img.shields.io/badge/pub-2022--10--27-green)![](https://img.shields.io/badge/cite-5-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-29-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning**](https://doi.org/10.48550/arXiv.2209.14610) ğŸ‘¨â€ğŸ“Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,etc 2022 ![](https://img.shields.io/badge/pub-2022--09--29-green)![](https://img.shields.io/badge/cite-11-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Chain of Thought

[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) ğŸ‘¨â€ğŸ“Namgyu Ho,Laura Schmid,Se-Young Yun 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-5-red)

[**Teaching Small Language Models to Reason**](https://doi.org/10.48550/arXiv.2212.08410) ğŸ‘¨â€ğŸ“Lucie Charlotte Magister,Jonathan Mallinson,Jakub Adamek,Eric Malmi,Aliaksei Severyn 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-7-red)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) ğŸ‘¨â€ğŸ“Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**PAL: Program-aided Language Models**](https://doi.org/10.48550/arXiv.2211.10435) ğŸ‘¨â€ğŸ“Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--18-green)![](https://img.shields.io/badge/cite-20-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-29-red)

[**Scaling Instruction-Finetuned Language Models**](https://doi.org/10.48550/arXiv.2210.11416) ğŸ‘¨â€ğŸ“Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-54-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) ğŸ‘¨â€ğŸ“Chenglei Si,Zhe Gan,Zhengyuan Yang,Shuohang Wang,Jianfeng Wang,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-9-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Chain of Thought"](./PaperList/ChainofThoughtList.md)ğŸ‘ˆ

### ğŸ“Œ Evaluation & Reliability

[**Controlling for Stereotypes in Multimodal Language Model Evaluation**](https://doi.org/10.48550/arXiv.2302.01582) ğŸ‘¨â€ğŸ“Manuj Malik,Richard Johansson 2023 ![](https://img.shields.io/badge/pub-2023--02--03-green)

[**Large Language Models Can Be Easily Distracted by Irrelevant Context**](https://doi.org/10.48550/arXiv.2302.00093) ğŸ‘¨â€ğŸ“Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--31-green)![](https://img.shields.io/badge/cite-3-red)

[**Emergent Analogical Reasoning in Large Language Models**](https://doi.org/10.48550/arXiv.2212.09196) ğŸ‘¨â€ğŸ“Taylor W. Webb,K. Holyoak,Hongjing Lu 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-5-red)

[**Discovering Language Model Behaviors with Model-Written Evaluations**](https://doi.org/10.48550/arXiv.2212.09251) ğŸ‘¨â€ğŸ“Ethan Perez,Sam Ringer,KamilÄ— LukoÅ¡iÅ«tÄ—,Karina Nguyen,Edwin Chen,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-6-red)

[**Constitutional AI: Harmlessness from AI Feedback**](https://doi.org/10.48550/arXiv.2212.08073) ğŸ‘¨â€ğŸ“Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell,John Kernion,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--15-green)![](https://img.shields.io/badge/cite-15-red)

[**On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08061) ğŸ‘¨â€ğŸ“Omar Shaikh,Hongxin Zhang,William B. Held,Michael Bernstein,Diyi Yang 2022 ![](https://img.shields.io/badge/pub-2022--12--15-green)![](https://img.shields.io/badge/cite-1-red)

[**Refining Semantic Similarity of Paraphasias Using a Contextual Language Model.**](https://doi.org/10.1044/2022_jslhr-22-00277) ğŸ‘¨â€ğŸ“Alexandra C. Salem,Robert C. Gale,Marianne Casilio,Mikala Fleegle,Gerasimos Fergadiotis,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--09-green)

[**Solving math word problems with process- and outcome-based feedback**](https://doi.org/10.48550/arXiv.2211.14275) ğŸ‘¨â€ğŸ“J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-3-red)

[**Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**](https://doi.org/10.48550/arXiv.2211.12588) ğŸ‘¨â€ğŸ“Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen 2022 ![](https://img.shields.io/badge/pub-2022--11--22-green)![](https://img.shields.io/badge/cite-21-red)

[**Holistic Evaluation of Language Models**](https://doi.org/10.48550/arXiv.2211.09110) ğŸ‘¨â€ğŸ“Percy Liang,Rishi Bommasani,Tony Lee,Dimitris Tsipras,Dilara Soylu,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--16-green)![](https://img.shields.io/badge/cite-33-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)ğŸ‘ˆ

## In-context Learning

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Self-adaptive In-context Learning**](https://doi.org/10.48550/arXiv.2212.10375) ğŸ‘¨â€ğŸ“Zhiyong Wu,Yaoxiang Wang,Jiacheng Ye,Lingpeng Kong 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-3-red)

[**Is GPT-3 a Good Data Annotator?**](https://doi.org/10.48550/arXiv.2212.10450) ğŸ‘¨â€ğŸ“Bosheng Ding,Chengwei Qin,Linlin Liu,Lidong Bing,Shafiq R. Joty,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) ğŸ‘¨â€ğŸ“Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-7-red)

[**Structured Prompting: Scaling In-Context Learning to 1, 000 Examples**](https://doi.org/10.48550/arXiv.2212.06713) ğŸ‘¨â€ğŸ“Y. Hao,Yutao Sun,Li Dong,Zhixiong Han,Yuxian Gu,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--13-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning**](https://doi.org/10.48550/arXiv.2211.03044) ğŸ‘¨â€ğŸ“Yu Meng,Martin Michalski,Jiaxin Huang,Yu Zhang,T. Abdelzaher,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--06-green)![](https://img.shields.io/badge/cite-2-red)



ğŸ‘‰[Complete paper list ğŸ”— for "In-context Learning"](./PaperList/InContextLearningList.md)ğŸ‘ˆ

## Multimodal Prompt

### ğŸ“Œ Hard Prompt/ Discrete Prompt

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--07-green)![](https://img.shields.io/badge/cite-2-red)

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) ğŸ‘¨â€ğŸ“Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,etc 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-25-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Hard Prompt"](./PaperList/HardPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Soft Prompt/ Continuous Prompt

[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) ğŸ‘¨â€ğŸ“G. Zuccon,B. Koopman 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://doi.org/10.48550/arXiv.2302.09236) ğŸ‘¨â€ğŸ“Yuhang Zhou,Suraj Maharjan,Bei Liu 2023 ![](https://img.shields.io/badge/pub-2023--02--18-green)

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) ğŸ‘¨â€ğŸ“Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) ğŸ‘¨â€ğŸ“Koustava Goswami,Lukas Lange,J. Araki,Heike Adel 2023 ![](https://img.shields.io/badge/pub-2023--02--14-green)

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) ğŸ‘¨â€ğŸ“Mingyu Derek Ma,Jiun-Yu Kao,Shuyang Gao,Arpit Gupta,Di Jin,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--26-green)

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) ğŸ‘¨â€ğŸ“M Saiful Bari,Aston Zhang,Shuai Zheng,Xingjian Shi,Yi Zhu,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) ğŸ‘¨â€ğŸ“Weijia Shi,Xiaochuang Han,Hila Gonen,Ari Holtzman,Yulia Tsvetkov,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-1-red)

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) ğŸ‘¨â€ğŸ“Damith Chamalke Senadeera,Julia Ive 2022 ![](https://img.shields.io/badge/pub-2022--12--06-green)

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2211.10681) ğŸ‘¨â€ğŸ“Xiaocheng Lu,Ziming Liu,Song Guo,Jingcai Guo 2022 ![](https://img.shields.io/badge/pub-2022--11--19-green)



ğŸ‘‰[Complete paper list ğŸ”— for "Soft Prompt"](./PaperList/SoftPromptList.md)ğŸ‘ˆ

<!-- ## Knowledge Augmented Prompts

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) ğŸ‘¨â€ğŸ“Xuanting Chen,Junjie Ye,Can Zu,Nuo Xu,Rui Zheng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks**](https://doi.org/10.48550/arXiv.2303.00733) ğŸ‘¨â€ğŸ“Kai-Wei Chang,Yu-Kai Wang,Hua Shen,Iu-thing Kang,W. Tseng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis**](https://doi.org/10.48550/arXiv.2303.00815) ğŸ‘¨â€ğŸ“Jingli Shi,Weihua Li,Quan-wei Bai,Yi Yang,Jianhua Jiang 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**EvoPrompting: Language Models for Code-Level Neural Architecture Search**](https://doi.org/10.48550/arXiv.2302.14838) ğŸ‘¨â€ğŸ“Angelica Chen,David Dohan,David R. So 2023 ![](https://img.shields.io/badge/pub-2023--02--28-green)

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) ğŸ‘¨â€ğŸ“Elliot Meyerson,M. Nelson,Herbie Bradley,Arash Moradi,Amy K. Hoover,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models**](https://doi.org/10.48550/arXiv.2302.12173) ğŸ‘¨â€ğŸ“Kai Greshake,Sahar Abdelnabi,Shailesh Mishra,C. Endres,Thorsten Holz,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) ğŸ‘¨â€ğŸ“Jules White,Quchen Fu,Sam Hays,M. Sandborn,Carlos Olea,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales**](https://doi.org/10.48550/arXiv.2302.08961) ğŸ‘¨â€ğŸ“M. Ruskov 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) ğŸ‘¨â€ğŸ“Albert Lu,Hongxin Zhang,Yanzhe Zhang,Xuezhi Wang,Diyi Yang 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)



ğŸ‘‰[Complete paper list ğŸ”— for "Knowledge Augmented Prompts"](./PaperList/KnowledgeAugmentedPromptList.md)ğŸ‘ˆ

## Prompt for Knowledge Graph

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) ğŸ‘¨â€ğŸ“Xuanting Chen,Junjie Ye,Can Zu,Nuo Xu,Rui Zheng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks**](https://doi.org/10.48550/arXiv.2303.00733) ğŸ‘¨â€ğŸ“Kai-Wei Chang,Yu-Kai Wang,Hua Shen,Iu-thing Kang,W. Tseng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis**](https://doi.org/10.48550/arXiv.2303.00815) ğŸ‘¨â€ğŸ“Jingli Shi,Weihua Li,Quan-wei Bai,Yi Yang,Jianhua Jiang 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**EvoPrompting: Language Models for Code-Level Neural Architecture Search**](https://doi.org/10.48550/arXiv.2302.14838) ğŸ‘¨â€ğŸ“Angelica Chen,David Dohan,David R. So 2023 ![](https://img.shields.io/badge/pub-2023--02--28-green)

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) ğŸ‘¨â€ğŸ“Elliot Meyerson,M. Nelson,Herbie Bradley,Arash Moradi,Amy K. Hoover,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models**](https://doi.org/10.48550/arXiv.2302.12173) ğŸ‘¨â€ğŸ“Kai Greshake,Sahar Abdelnabi,Shailesh Mishra,C. Endres,Thorsten Holz,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) ğŸ‘¨â€ğŸ“Jules White,Quchen Fu,Sam Hays,M. Sandborn,Carlos Olea,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales**](https://doi.org/10.48550/arXiv.2302.08961) ğŸ‘¨â€ğŸ“M. Ruskov 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) ğŸ‘¨â€ğŸ“Albert Lu,Hongxin Zhang,Yanzhe Zhang,Xuezhi Wang,Diyi Yang 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)



ğŸ‘‰[Complete paper list ğŸ”— for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)ğŸ‘ˆ -->

<img width="200%" src="./figures/hr.gif" />

# ğŸ“ Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
ç»¼è¿°è®ºæ–‡å¯ä»¥æ”¾åœ¨è¿™ä¸ªä½ç½®
```
<img width="200%" src="./figures/hr.gif" />

# âœ‰ï¸ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `cyfedu1024@gmail.com` or `cyfedu1024@163.com`.

We are willing to communicate with your research team or confirm in variety of fields.

<img width="200%" src="./figures/hr.gif" />

# ğŸ™ Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. More researchers are welcome to join us and make more contributions to the community.

<img width="200%" src="./figures/hr.gif" />

# ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]()


<!-- ## Guest Contributors
* [No] -->

<img width="200%" src="./figures/hr.gif" />

# ğŸ“” License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div>
