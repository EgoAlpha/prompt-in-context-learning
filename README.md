<div align="center">

<img src="./figures/egoalpha_logo_compress.jpeg" width="600px">

 <div align="center">
    <a href="https://blog.sunguoqi.com/">
      <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&width=435&lines=%22Hello%2C%20World%22;Welcome to our project!&center=true&size=27" alt="Typing SVG" />
    </a>
  </div>

**An Open-Source Enfineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./README_zh.md">简体中文</a>
    <p>
</h4>
<p align="center">
  <a href="#papers📜">📝 Papers</a> |
  <a href="./Playground.md">⚡️  Playground</a> |
  <a href="./Promptzoo.md">🛠 Prompt Zoo</a> |
  <a href="./chatgptprompt.md">🌍 ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **⭐️ Shining ⭐️:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, let’s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*🎉[Papers](#papers📜)🎉*:  The latest papers about in-context learning or prompt engineering. 

*🎉[Playground](./Playground.md)🎉*:  Large language models that enable prompt experimentation. 

*🎉[Prompt Zoo](./Promptzoo.md)🎉*: Prompt techniques for leveraging large language models. 

*🎉[ChatGPT Prompt](./chatgptprompt.md)🎉*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
💎EgoAlpha: Hello! human👤, are you ready?
```  


<!-- These models can be applied on:

* 📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.
* 🖼️ Images, for tasks like image classification, object detection, and segmentation.
* 🗣️ Audio, for tasks like speech recognition and audio classification.（这里需要重新整理） -->

# 📢 News

- **[2023.3.4]** We establish this project that is organised by professor Yu Liu from EgoAlpha Lab.

<!-- [![](https://github-readme-juejin-recent-article-flywith24.vercel.app/juejin?id=219558054476792&limit=2)](https://juejin.cn/user/219558054476792/posts) -->

<img width="200%" src="./figures/hr.gif" />

# Papers📜

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
- [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph)

---
## Prompt Engineering

### 📌 Prompt Design

- *2021*
  - paper1([Paper]()/[Code]())
  - paper2([Paper]()/[Code]())
### 📌 Automatic Prompt 
- *2021*
  - Paper1([Paper]()/[Code]())
  - Paper2([Paper]()/[Code]())

### 📌 Chain of Thought

- *2021*
  - Paper1([Paper]()/[Code]())
  - Paper2([Paper]()/[Code]())

### 📌 Evaluation & Reliability

- *2021*
  - Paper1([Paper]()/[Code]())
  - Paper2([Paper]()/[Code]())

## In-context Learning

- *2021*
  - Paper1([Paper]()/[Code]())
  - Paper2([Paper]()/[Code]())

## Multimodal Prompt

### 📌 Hard Prompt/ Discrete Prompt

- *2023*
  - Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery([Paper](https://arxiv.org/abs/2302.03668)/[Code](https://github.com/YuxinWenRick/hard-prompts-made-easy))

- *2022*
  - RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning([Paper](https://arxiv.org/pdf/2205.12548.pdf)/[Code](https://github.com/mingkaid/rl-prompt))【EMNLP2022】

### 📌 Soft Prompt/ Continuous Prompt

- *2023*
  - Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts([Paper](https://arxiv.org/abs/2302.08958)/[Code](https://github.com/zhjohnchan/PTUnifier))
  - SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains([Paper](https://arxiv.org/pdf/2302.06868.pdf)/[Code](https://github.com/boschresearch/switchprompt))【EACL2023】
  - How Does In-Context Learning Help Prompt Tuning?([Paper](https://arxiv.org/pdf/2302.11521.pdf))
  - Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning([Paper](https://arxiv.org/pdf/2210.12587.pdf))【ICLR2023】
  - Scalable Prompt Generation for Semi-supervised Learning with Language Models([Paper](https://arxiv.org/abs/2302.09236)/[Code]())【EACL2023】
  - FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning([Paper](https://arxiv.org/pdf/2208.12268.pdf))
  - ([Paper]()/[Code]())
  - Paper([Paper]()/[Code]())【】
  - Paper([Paper]()/[Code]())【】
  - Paper([Paper]()/[Code]())【】
  - Paper([Paper]()/[Code]())【】

- *2022*
  - SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer([Paper](https://arxiv.org/abs/2110.07904)/[Code](https://github.com/google-research/prompt-tuning/tree/main/prompt_tuning/spot))【ACL2022】
  - ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts([Paper](https://arxiv.org/abs/2205.11961)/ [Code](https://github.com/AkariAsai/ATTEMPT))【EMNLP2022】
  - Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis([Paper](https://aclanthology.org/2022.acl-long.174.pdf))【ACL2022】
  - Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization([Paper](https://arxiv.org/abs/2210.03029)/ [Code](https://github.com/seonghyeonye/RoSPr))
  - PPT: Pre-trained Prompt Tuning for Few-shot Learning([Paper](https://arxiv.org/abs/2109.04332)/ [Code](https://github.com/thu-coai/PPT))【ACL2022】
  - ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts([Paper](https://aclanthology.org/2022.emnlp-main.446.pdf)/ [Code](https://github.com/AkariAsai/ATTEMPT))【EMNLP2022】
  - PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization([Paper](https://arxiv.org/pdf/2204.04413.pdf))【COLING2022】
  - Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning([Paper](https://arxiv.org/pdf/2211.10681.pdf)/ [Code](https://github.com/Forest-art/DFSP))
  - Prompt Tuning with Soft Context Sharing for Vision-Language Models([Paper](https://arxiv.org/pdf/2208.13474.pdf)/ [Code]())
  - Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding([Paper](https://arxiv.org/abs/2205.11024)/ [Code](https://github.com/declare-lab/VIP))【EMNLP2022】
  - Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts([Paper](https://arxiv.org/pdf/2210.04726.pdf))
  - Learning a Better Initialization for Soft Prompts via Meta-Learning([Paper](https://arxiv.org/pdf/2205.12471.pdf))
  - PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks([Paper](https://arxiv.org/pdf/2202.12499.pdf)/ [Code](https://github.com/GaryYufei/PromDA))【ACL2022】
  - Co-training Improves Prompt-based Learning for Large Language Models([Paper](https://proceedings.mlr.press/v162/lang22a/lang22a.pdf)/ [Code](https://github.com/clinicalml/cotrain-prompting))【ICML2022】
  - MetaPrompting: Learning to Learn Better Prompts([Paper](https://arxiv.org/pdf/2209.11486.pdf)/ [Code](https://github.com/Dousia/MetaPrompting))【COLING2022】
  - Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI([Paper](https://arxiv.org/pdf/2212.02924.pdf)/ [Code](https://github.com/damith92/T5_encoder_decoder_prompt_tuning_for_text_generation))
  - Coherent Long Text Generation by Contrastive Soft Prompt([Paper](https://aclanthology.org/2022.gem-1.42.pdf))
  - On Transferability of Prompt Tuning for Natural Language Processing([Paper](https://arxiv.org/pdf/2111.06719.pdf)/ [Code](https://github.com/thunlp/Prompt-Transferability))
  - Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts([Paper](https://arxiv.org/pdf/2210.11292.pdf)/ [Code](https://github.com/xyltt/LPT))【EMNLP2022】
  - Structured Prompt Tuning([Paper](https://arxiv.org/pdf/2205.12309.pdf)/ [Code](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md))
  - Learning to Compose Soft Prompts for Compositional Zero-Shot Learning([Paper](https://arxiv.org/abs/2204.03574)/ [Code](https://github.com/BatsResearch/csp))
  - Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation([Paper](https://arxiv.org/pdf/2210.02952.pdf)/ [Code](https://github.com/guoxuxu/soft-prompt-transfer/tree/main/optima))【EMNLP2022】
  - Prompt Distribution Learning([Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf)/ [Code]())【CVPR2022】
  - XPrompt: Exploring the Extreme of Prompt Tuning([Paper](https://arxiv.org/pdf/2210.04457.pdf)/ [Code]())【EMNLP2022】
  - PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation([Paper](https://arxiv.org/pdf/2208.10160.pdf)/ [Code](https://github.com/THUDM/P-tuning-v2))
  - PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning([Paper](https://www.ijcai.org/proceedings/2022/0096.pdf))【IJCAI2022】
  - FPT: Improving Prompt Tuning Efficiency via Progressive Training([Paper](https://arxiv.org/pdf/2211.06840.pdf)/ [Code](https://github.com/thunlp/FastPromptTuning))【EMNLP2022】
  - PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Model([Paper](https://arxiv.org/pdf/2208.11625.pdf))
  - No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence([Paper](https://dl.acm.org/doi/abs/10.1145/3540250.3549113))【ESEC/FSE2022】
  - ProQA: Structural Prompt-based Pre-training for Unified Question Answering([Paper](https://arxiv.org/pdf/2205.04040.pdf)/ [Code](https://github.com/zhongwanjun/ProQA))【NAACL2022】
  - Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts([Paper](https://arxiv.org/pdf/2210.04726.pdf))
  - Continuous Prompt Tuning for Russian: How to Learn Prompts Efficiently with RuGPT3?([Paper](https://link.springer.com/chapter/10.1007/978-3-031-15168-2_3)/ [Code](https://github.com/sberbank-ai/ru-prompts.))
  - Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer([Paper](https://link.springer.com/chapter/10.1007/978-3-031-15931-2_19)/ [Code](https://github.com/Ydongd/prototypical-prompt-verbalizer))【ICANN2022】
  - Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models([Paper](https://arxiv.org/pdf/2210.10841.pdf))
  - Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning([Paper](https://dl.acm.org/doi/abs/10.1145/3534678.3539382)/ [Code](https://github.com/RUCAIBox/UniCRS))【KDD2022】
  - Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning([Paper](https://aclanthology.org/2022.findings-emnlp.220.pdf)/ [Code](https://github.com/YJiangcm/PromCSE))【EMNLP2022】
  - HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification([Paper](https://arxiv.org/pdf/2204.13413.pdf)/ [Code](https://github.com/wzh9969/HPT))【EMNLP2022】
  - Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning([Paper](https://arxiv.org/pdf/2301.10915.pdf))【NeurIPS2022】
  - Changing the Narrative Perspective: From Ranking to Prompt-Based Generation of Entity Mentions([Paper](https://ceur-ws.org/Vol-3117/paper4.pdf)/[Code](https://github.com/chenmike1986/change_pov))
  - Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI.([Paper](https://arxiv.org/pdf/2212.02924.pdf)/ [Code](https://github.com/damith92/T5_encoder_decoder_prompt_tuning_for_text_generation))
  - Toward Human Readable Prompt Tuning: Kubrick’s The Shining is a good movie, and a good prompt too?([Paper](https://arxiv.org/pdf/2212.10539.pdf))
  - SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning([Paper](https://arxiv.org/pdf/2212.10929.pdf))
  - Prompt-based Conservation Learning for Multi-hop Question Answering([Paper](https://arxiv.org/pdf/2209.06923.pdf))【COLING2022】
  - Enhance Performance of Ad-hoc Search via Prompt Learning([Paper](https://link.springer.com/chapter/10.1007/978-3-031-24755-2_3))
  - Continuous Detection, Rapidly React: Unseen Rumors Detection based on Continual Prompt-Tuning([Paper](https://arxiv.org/pdf/2203.11720.pdf))【COLING2022】

- *2021*
  - The Power of Scale for Parameter-Efficient Prompt Tuning([Paper](https://arxiv.org/abs/2104.08691)/ [Code](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md##t511))【EMNLP2021】
  - Learning How to Ask: Querying LMs with Mixtures of Soft Prompts([Paper](https://arxiv.org/pdf/2104.06599.pdf)/[Code](https://github.com/hiaoxui/soft-prompts))【NAACL-HLT 2021】
  - Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning([Paper](https://proceedings.neurips.cc/paper/2021/file/86b3e165b8154656a71ffe8a327ded7d-Paper.pdf))【NeurIPS2021】
    
### 📌 Mixer Prompt

- *2022*  
  - OpenPrompt: An Open-source Framework for Prompt-learning([Paper](https://arxiv.org/pdf/2111.01998.pdf)/ [Code](https://github.com/thunlp/OpenPrompt))
  - Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt([Paper](https://arxiv.org/pdf/2202.11451.pdf)/[Code](https://github.com/mojave-pku/UniPrompt))

- *2021*
  - Paper3([Paper]()/ [Code]())【】
  - 

## 🌕 Knowledge Augmented Prompts

- *2021*
  - paper1([Paper]()/[Code]())
  - paper2([Paper]()/[Code]())

## 🌖 Prompt for Knowledge Graph

- *2021*
  - paper1([Paper]()/[Code]())
  - paper2([Paper]()/[Code]())
<!-- In Natural Language Processing:
- [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)
- [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

In Computer Vision:
- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)
- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)
- [Semantic Segmentation with SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [Panoptic Segmentation with MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)
- [Depth Estimation with DPT](https://huggingface.co/docs/transformers/model_doc/dpt)
- [Video Classification with VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)
- [Universal Segmentation with OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)

In Audio:
- [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

In Multimodal tasks:
- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [Zero-shot Image Classification with CLIP](https://huggingface.co/openai/clip-vit-large-patch14)
- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip) -->

<!-- ##  示例Development Setup
示例
```console
  npm install
  npm run dev
``` -->

<!-- ## ⭐️ 示例Embedding Into Your Website
示例

A [`Dockerfile`](Dockerfile) is provided in the root of the repository.
If you want to run JSON Crack locally:

* Build a Docker image with `docker build -t jsoncrack .`
* Run locally with `docker run -p 8888:8080 jsoncrack`
* Go to http://localhost:8888 -->

<!-- ## ⚡️ 示例Key Features
示例
- Search Nodes
- Share links & Create Embed Widgets
- Download/Clipboard as image
- Upload JSON locally or fetch from URL
- User-friendly Interface
- Light/Dark Mode -->

<img width="200%" src="./figures/hr.gif" />

# 🎓 Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
综述论文可以放在这个位置
```
<img width="200%" src="./figures/hr.gif" />

# ✉️ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `cyfedu1024@gmail.com`.

We are willing to communicate with your research team or confirm in variety of fields.

<img width="200%" src="./figures/hr.gif" />

# 🙏 Acknowledgements

【Thanks to [EgoAlpha Lab](https://github.com/EgoAlpha) for the help with XXX and [who](https://github.com/xxx) for the help with A工作 and B 工作.】/【Thanks to the XXX modeule.】/ [We use the implemention of XXX from https://xxx.xxx]

<img width="200%" src="./figures/hr.gif" />

# 👨‍👩‍👧‍👦 Contributors

## Main Contributors
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]()
* [Yifei Cao](https://github.com/cyfedu1024)

## Guest Contributors
* [No]

<img width="200%" src="./figures/hr.gif" />

# 📔 License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div>

 [**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) 👨‍🎓Namgyu Ho,Laura Schmid,Se-Young Yun 2022![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-4-red)
