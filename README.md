<div align="center">

<img src="./figures/egoalpha_logo_compress.jpeg" width="600px">

 <div align="center">
    <a href="https://blog.sunguoqi.com/">
      <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&width=435&lines=%22Hello%2C%20World%22;Welcome to our project!&center=true&size=27" alt="Typing SVG" />
    </a>
  </div>

**An Open-Source Enfineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./README_zh.md">ç®€ä½“ä¸­æ–‡</a>
    <p>
</h4>
<p align="center">
  <a href="#papersğŸ“œ">ğŸ“ Papers</a> |
  <a href="./Playground.md">âš¡ï¸  Playground</a> |
  <a href="./Promptzoo.md">ğŸ›  Prompt Zoo</a> |
  <a href="./chatgptprompt.md">ğŸŒ ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **â­ï¸ Shining â­ï¸:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, letâ€™s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*ğŸ‰[Papers](#papersğŸ“œ)ğŸ‰*:  The latest papers about in-context learning or prompt engineering. 

*ğŸ‰[Playground](./Playground.md)ğŸ‰*:  Large language models that enable prompt experimentation. 

*ğŸ‰[Prompt Zoo](./Promptzoo.md)ğŸ‰*: Prompt techniques for leveraging large language models. 

*ğŸ‰[ChatGPT Prompt](./chatgptprompt.md)ğŸ‰*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
ğŸ’EgoAlpha: Hello! humanğŸ‘¤, are you ready?
```  

# ğŸ“¢ News

- **[2023.3.4]** We establish this project that is organised by professor Yu Liu from EgoAlpha Lab.

<img width="200%" src="./figures/hr.gif" />

# PapersğŸ“œ

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
- [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph)

---

## Prompt Engineering

### ğŸ“Œ Prompt Design

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



### ğŸ“Œ Automatic Prompt 

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,Hongkun Yu,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-28-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-21-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-17-red)

[**Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning**](https://doi.org/10.48550/arXiv.2209.14610) ğŸ‘¨â€ğŸ“Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan 2022 ![](https://img.shields.io/badge/pub-2022--09--29-green)![](https://img.shields.io/badge/cite-11-red)

[**Selective Annotation Makes Language Models Better Few-Shot Learners**](https://doi.org/10.48550/arXiv.2209.01975) ğŸ‘¨â€ğŸ“Hongjin Su,Jungo Kasai,Chen Henry Wu,Weijia Shi,Tianlu Wang,Jiayi Xin,Rui Zhang,Mari Ostendorf,Luke Zettlemoyer,Noah A. Smith,Tao Yu 2022 ![](https://img.shields.io/badge/pub-2022--09--05-green)![](https://img.shields.io/badge/cite-20-red)

[**Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models**](https://doi.org/10.1109/TVCG.2022.3209479) ğŸ‘¨â€ğŸ“Hendrik Strobelt,Albert Webson,Victor Sanh,Benjamin Hoover,J. Beyer,H. Pfister,Alexander M. Rush 2022 ![](https://img.shields.io/badge/pub-2022--08--16-green)![](https://img.shields.io/badge/cite-9-red)

[**Exploring CLIP for Assessing the Look and Feel of Images**](https://doi.org/10.48550/arXiv.2207.12396) ğŸ‘¨â€ğŸ“Jianyi Wang,Kelvin C. K. Chan,Chen Change Loy 2022 ![](https://img.shields.io/badge/pub-2022--07--25-green)![](https://img.shields.io/badge/cite-1-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-25-red)

[**Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal AI Generated Art**](https://doi.org/10.1145/3527927.3532792) ğŸ‘¨â€ğŸ“Han Qiao,Vivian Liu,Lydia B. Chilton 2022 ![](https://img.shields.io/badge/pub-2022--06--20-green)![](https://img.shields.io/badge/cite-5-red)



### ğŸ“Œ Chain of Thought

[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) ğŸ‘¨â€ğŸ“Namgyu Ho,Laura Schmid,Se-Young Yun 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-4-red)

[**Teaching Small Language Models to Reason**](https://doi.org/10.48550/arXiv.2212.08410) ğŸ‘¨â€ğŸ“Lucie Charlotte Magister,Jonathan Mallinson,Jakub Adamek,Eric Malmi,Aliaksei Severyn 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-7-red)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) ğŸ‘¨â€ğŸ“Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**PAL: Program-aided Language Models**](https://doi.org/10.48550/arXiv.2211.10435) ğŸ‘¨â€ğŸ“Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig 2022 ![](https://img.shields.io/badge/pub-2022--11--18-green)![](https://img.shields.io/badge/cite-18-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,Hongkun Yu,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-28-red)

[**Scaling Instruction-Finetuned Language Models**](https://doi.org/10.48550/arXiv.2210.11416) ğŸ‘¨â€ğŸ“Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc V. Le,Jason Wei 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-53-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung,Aakanksha Chowdhery,Quoc V. Le,E. Chi,Denny Zhou,Jason Wei 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) ğŸ‘¨â€ğŸ“Chenglei Si,Zhe Gan,Zhengyuan Yang,Shuohang Wang,Jianfeng Wang,Jordan L. Boyd-Graber,Lijuan Wang 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-9-red)



### ğŸ“Œ Evaluation & Reliability



## In-context Learning

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,Ping Yu,Kurt Shuster,Tianlu Wang,Qing Liu,Punit Singh Koura,Xian Li,Brian O'Horo,Gabriel Pereyra,Jeff Wang,Christopher Dewan,Asli Celikyilmaz,Luke Zettlemoyer,Veselin Stoyanov 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,Kang Min Yoo,Taeuk Kim 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Self-adaptive In-context Learning**](https://doi.org/10.48550/arXiv.2212.10375) ğŸ‘¨â€ğŸ“Zhiyong Wu,Yaoxiang Wang,Jiacheng Ye,Lingpeng Kong 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Is GPT-3 a Good Data Annotator?**](https://doi.org/10.48550/arXiv.2212.10450) ğŸ‘¨â€ğŸ“Bosheng Ding,Chengwei Qin,Linlin Liu,Lidong Bing,Shafiq R. Joty,Boyang Li 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) ğŸ‘¨â€ğŸ“Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-7-red)

[**Structured Prompting: Scaling In-Context Learning to 1, 000 Examples**](https://doi.org/10.48550/arXiv.2212.06713) ğŸ‘¨â€ğŸ“Y. Hao,Yutao Sun,Li Dong,Zhixiong Han,Yuxian Gu,Furu Wei 2022 ![](https://img.shields.io/badge/pub-2022--12--13-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning**](https://doi.org/10.48550/arXiv.2211.03044) ğŸ‘¨â€ğŸ“Yu Meng,Martin Michalski,Jiaxin Huang,Yu Zhang,T. Abdelzaher,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--11--06-green)![](https://img.shields.io/badge/cite-2-red)



## Multimodal Prompt

### ğŸ“Œ Hard Prompt/ Discrete Prompt



### ğŸ“Œ Soft Prompt/ Continuous Prompt



## Knowledge Augmented Prompts

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



## Prompt for Knowledge Graph

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



<img width="200%" src="./figures/hr.gif" />

# ğŸ“ Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
ç»¼è¿°è®ºæ–‡å¯ä»¥æ”¾åœ¨è¿™ä¸ªä½ç½®
```
<img width="200%" src="./figures/hr.gif" />

# âœ‰ï¸ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `cyfedu1024@gmail.com` or `cyfedu1024@163.com`.

We are willing to communicate with your research team or confirm in variety of fields.

<img width="200%" src="./figures/hr.gif" />

# ğŸ™ Acknowledgements

ã€Thanks to [EgoAlpha Lab](https://github.com/EgoAlpha) for the help with XXX and [who](https://github.com/xxx) for the help with Aå·¥ä½œ and B å·¥ä½œ.ã€‘/ã€Thanks to the XXX modeule.ã€‘/ [We use the implemention of XXX from https://xxx.xxx]

<img width="200%" src="./figures/hr.gif" />

# ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors

## Main Contributors
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]()
* [Yifei Cao](https://github.com/cyfedu1024)

## Guest Contributors
* [No]

<img width="200%" src="./figures/hr.gif" />

# ğŸ“” License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div>
