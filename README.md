<div align="center">


<img src="./figures/Prompt-EgoAlpha_white.svg" width="600px">

 <div align="center">

 [![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=30&duration=2500&pause=500&color=8D589A&background=FCFCFF00&center=true&vCenter=true&width=500&lines=Hello!+Human%2C+Are+You+Ready%3F;Welcome+to+my+world!)]()
 
 </div>

**An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">

    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>

</h3> -->

<!-- <h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./chatgptprompt_zh.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
    <p>
</h4> -->

<p align="center">

  <a href="#üìú-papers">üìù Papers</a> |
  <a href="./Playground.md">‚ö°Ô∏è  Playground</a> |
  <a href="./PromptEngineering.md">üõ† Prompt Engineering</a> |
  <a href="./chatgptprompt.md">üåç ChatGPT Prompt</a> ÔΩú
  <a href="./langchain_guide/LangChainTutorial.ipynb">‚õ≥ LLMs Usage Guide</a> 

</p>

</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->

![version](https://img.shields.io/badge/version-v2.0.0-yellow)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)

<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->

</div>

> **‚≠êÔ∏è Shining ‚≠êÔ∏è:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, let‚Äôs take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*üéâ[Papers](#üìú-papers)üéâ*:  The latest papers about in-context learning or prompt engineering. 

*üéâ[Playground](./Playground.md)üéâ*:  Large language models that enable prompt experimentation. 

*üéâ[Prompt Engineering](./PromptEngineering.md)üéâ*: Prompt techniques for leveraging large language models. 

*üéâ[ChatGPT Prompt](./chatgptprompt.md)üéâ*: Prompt examples that can be applied in our work and daily lives. 

*üéâ[LLMs Usage Guide](./chatgptprompt.md)üéâ*: The method for quickly getting started with large language models by using LangChain.

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): 
- Those who enhance their abilities through the use of AI; 
- Those whose jobs are replaced by AI automation.

```

üíéEgoAlpha: Hello! humanüë§, are you ready?

```  

# üì¢ News
<!-- üî•üî•üî• -->
‚òÑÔ∏è **EgoAlpha releases the TrustGPT focuses on reasoning. Trust the GPT with the strongest reasoning abilities for authentic and reliable answers. You can click [here](https://trustgpt.co) or visit the [Playgrounds](./Playground.md) directly to experience it„ÄÇ**

- **[2023.5.26]**
    - Paper: [Iterative Forward Tuning Boosts In-context Learning in Language Models](https://arxiv.org/pdf/2305.13016.pdf)
    
- **[2023.5.25]**
    - Paper: [Diversity-Aware Meta Visual Prompting](https://arxiv.org/abs/2303.08138)
    
- **[2023.5.24]**
    - Paper:[VideoLLM: Modeling Video Sequence with Large Language Models](https://arxiv.org/pdf/2305.13292.pdf)
    
- **[2023.5.23]**
    - Paper:[Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)
    - Paper: [PointGPT: Auto-regressively GenerativePre-training from Point Clouds](https://arxiv.org/pdf/2305.11487.pdf)
- **[2023.5.22]**
    - Paper: [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://arxiv.org/abs/2305.10266)
    
- **[2023.5.21]**
    - Paper:[Language Models Meet World Models: Embodied Experiences Enhance Language Models](https://arxiv.org/abs/2305.10626)

- **[2023.5.20]**
    - Paper:[AttentionViz: A Global View of Transformer Attention](https://arxiv.org/pdf/2305.03210.pdf)
    
- **[2023.5.19]**
    - [OpenAI introducing the ChatGPT APP for IOS](https://apps.apple.com/app/openai-chatgpt/id6448311069)
    - Paper: [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)
    
- **[2023.5.18]**
    - Paper: [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)
    
- **[2023.5.17]**
    - Paper: [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)
    
- **[2023.5.16]**
    - Paper:[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185.pdf)

- **[2023.5.15]** 
    - Paper: [ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4](https://arxiv.org/pdf/2305.07490.pdf)
    
- **[2023.5.14]**
    - Paper: [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301v1.pdf)

- **[2023.5.13]**
    - Paper: [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/pdf/2305.01278.pdf)

- **[2023.5.12]**
    - Paper: [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)

[üëâ Complete history news üëà](./historynews.md)

<img width="200%" src="./figures/hr.gif" />

# üìú Papers

> You can directly click on the title to jump to the corresponding PDF link location

- [Survey](#Survey)

- [Prompt Engineering](#prompt-engineering)

    - [Prompt Design](#prompt-design)
    - [Automatic Prompt](#automatic-prompt)
    - [Chain of Thought](#chain-of-thought)
    - [Knowledge Augmented Prompt](#knowledge-augmented-prompt)
    - [Evaluation & Reliability](#evaluation--reliability)

- [In-context learning](#in-context-learning)

- [Multimodal Prompt](#multimodal-prompt)

- [Prompt Application](#prompt-application)

- [Foundation Models](#foundation-models)

---

## Survey

<div style="line-height:0.2em;">



[**Prompt Engineering for Healthcare: Methodologies and Applications**](https://doi.org/10.48550/arXiv.2304.14670) Ôºà**2023.04.28**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond**](https://arxiv.org/abs/2304.13712) Ôºà**2023.04.26**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-149-red)  [![](https://img.shields.io/badge/Github%20Stars-3.7k-blue)](https://github.com/mooler0410/llmspracticalguide)

[**A Survey of Large Language Models**](https://arxiv.org/abs/2303.18223) Ôºà**2023.03.31**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-438-red)  [![](https://img.shields.io/badge/Github%20Stars-1.9k-blue)](https://github.com/rucaibox/llmsurvey)

[**Augmented Language Models: a Survey**](https://doi.org/10.48550/arXiv.2302.07842) Ôºà**2023.02.15**Ôºâ

![](https://img.shields.io/badge/Citations-4-green)

[**A Survey for In-context Learning**](https://doi.org/10.48550/arXiv.2301.00234) Ôºà**2022.12.31**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Towards Reasoning in Large Language Models: A Survey**](https://doi.org/10.48550/arXiv.2212.10403) Ôºà**2022.12.20**Ôºâ

![](https://img.shields.io/badge/Citations-5-green)  [![](https://img.shields.io/badge/Github%20Stars-285-blue)](https://github.com/jeffhj/lm-reasoning)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) Ôºà**2022.12.19**Ôºâ

![](https://img.shields.io/badge/Citations-7-green)  [![](https://img.shields.io/badge/Github%20Stars-352-blue)](https://github.com/zjunlp/Prompt4ReasoningPapers)

[**Emergent Abilities of Large Language Models**](https://doi.org/10.48550/arXiv.2206.07682) Ôºà**2022.06.15**Ôºâ

![](https://img.shields.io/badge/Citations-156-green)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) Ôºà**2021.07.28**Ôºâ

![](https://img.shields.io/badge/Citations-462-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1.5k-red)  [![](https://img.shields.io/badge/Github%20Stars-194-blue)](https://github.com/mingkaid/rl-prompt)


</div>

üëâ[Complete paper list üîó for "Survey"](./PaperList/survey.md)üëà

## Prompt Engineering

### Prompt Design

<div style="line-height:0.2em;">



[**WizardLM: Empowering Large Language Models to Follow Complex Instructions**](https://arxiv.org/abs/2304.12244) Ôºà**2023.04.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-25-red)  [![](https://img.shields.io/badge/Github%20Stars-1.6k-blue)](https://github.com/nlpxucan/wizardlm)

[**LLM+P: Empowering Large Language Models with Optimal Planning Proficiency**](https://arxiv.org/abs/2304.11477) Ôºà**2023.04.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-19-red)  [![](https://img.shields.io/badge/Github%20Stars-144-blue)](https://github.com/Cranial-XIX/llm-pddl)

[**Progressive-Hint Prompting Improves Reasoning in Large Language Models**](https://arxiv.org/abs/2304.09797) Ôºà**2023.04.19**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-25-red)  [![](https://img.shields.io/badge/Github%20Stars-95-blue)](https://github.com/chuanyang-Zheng/Progressive-Hint)

[**Boosted Prompt Ensembles for Large Language Models**](https://doi.org/10.48550/arXiv.2304.05970) Ôºà**2023.04.12**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-16-blue)](https://github.com/awwang10/llmpromptboosting)

[**Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition**](https://arxiv.org/abs/2304.04704) Ôºà**2023.04.10**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-20-red)  [![](https://img.shields.io/badge/Github%20Stars-170-blue)](https://github.com/amazon-science/prompt-pretraining)

[**REFINER: Reasoning Feedback on Intermediate Representations**](https://arxiv.org/abs/2304.01904) Ôºà**2023.04.04**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)  [![](https://img.shields.io/badge/Github%20Stars-30-blue)](https://github.com/debjitpaul/refiner)

[**Context-faithful Prompting for Large Language Models**](https://doi.org/10.48550/arXiv.2303.11315) Ôºà**2023.03.20**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-17-blue)](https://github.com/wzhouad/context-faithful-llm)

[**Reflexion: an autonomous agent with dynamic memory and self-reflection**](https://doi.org/10.48550/arXiv.2303.11366) Ôºà**2023.03.20**Ôºâ

![](https://img.shields.io/badge/Citations-4-green)  [![](https://img.shields.io/badge/Github%20Stars-345-blue)](https://github.com/noahshinn024/reflexion)

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) Ôºà**2023.02.21**Ôºâ

![](https://img.shields.io/badge/Citations-3-green)

[**GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks**](https://doi.org/10.48550/arXiv.2302.08043) Ôºà**2023.02.16**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Prompt Design"](./PaperList/PromptDesignList.md)üëà

### Automatic Prompt 

<div style="line-height:0.2em;">



[**Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data**](https://doi.org/10.48550/arXiv.2302.12822) Ôºà**2023.02.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Guiding Large Language Models via Directional Stimulus Prompting**](https://doi.org/10.48550/arXiv.2302.11520) Ôºà**2023.02.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/leezekun/directional-stimulus-prompting)

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) Ôºà**2023.02.11**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/livnlp/prompt-robustness)

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) Ôºà**2023.02.07**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-405-blue)](https://github.com/YuxinWenRick/hard-prompts-made-easy)

[**Ask Me Anything: A simple strategy for prompting language models**](https://doi.org/10.48550/arXiv.2210.02441) Ôºà**2022.10.05**Ôºâ

![](https://img.shields.io/badge/Citations-14-green)  [![](https://img.shields.io/badge/Github%20Stars-451-blue)](https://github.com/hazyresearch/ama_prompting)

[**STaR: Bootstrapping Reasoning With Reasoning**](https://doi.org/10.48550/arXiv.2203.14465) Ôºà**2022.03.28**Ôºâ

![](https://img.shields.io/badge/Citations-56-green)  [![](https://img.shields.io/badge/Github%20Stars-15-blue)](https://github.com/ezelikman/STaR)

[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) Ôºà**2021.01.01**Ôºâ

![](https://img.shields.io/badge/Citations-648-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-733-red)  [![](https://img.shields.io/badge/Github%20Stars-654-blue)](https://github.com/princeton-nlp/LM-BFF)

[**Eliciting Knowledge from Language Models Using Automatically Generated Prompts**](https://doi.org/10.18653/v1/2020.emnlp-main.346) Ôºà**2020.10.29**Ôºâ

![](https://img.shields.io/badge/Citations-137-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-506-red)

[**Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification**](https://doi.org/10.5282/UBM/EPUB.74034) Ôºà**2020.10.26**Ôºâ

![](https://img.shields.io/badge/Citations-85-green)  [![](https://img.shields.io/badge/Github%20Stars-1.5k-blue)](https://github.com/timoschick/pet)


</div>

üëâ[Complete paper list üîó for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)üëà

### Chain of Thought

<div style="line-height:0.2em;">



[**Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting**](https://doi.org/10.48550/arXiv.2305.04388) Ôºà**2023.05.07**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/milesaturpin/cot-unfaithfulness)

[**Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models**](https://doi.org/10.48550/arXiv.2305.04091) Ôºà**2023.05.06**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-119-blue)](https://github.com/agi-edgerunners/plan-and-solve-prompting)

[**Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework**](https://doi.org/10.48550/arXiv.2305.03268) Ôºà**2023.05.05**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/ruochenzhao/verify-and-edit)

[**Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings**](https://doi.org/10.48550/arXiv.2305.02317) Ôºà**2023.05.03**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**SCOTT: Self-Consistent Chain-of-Thought Distillation**](https://doi.org/10.48550/arXiv.2305.01879) Ôºà**2023.05.03**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models**](https://arxiv.org/abs/2304.11657) Ôºà**2023.04.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-17-red)  [![](https://img.shields.io/badge/Github%20Stars-56-blue)](https://github.com/gasolsun36/iter-cot)

[**Chain of Thought Prompt Tuning in Vision Language Models**](https://arxiv.org/abs/2304.07919) Ôºà**2023.04.16**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

[**Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media**](https://doi.org/10.48550/arXiv.2304.03087) Ôºà**2023.04.06**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data**](https://doi.org/10.48550/arXiv.2302.12822) Ôºà**2023.02.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Active Prompting with Chain-of-Thought for Large Language Models**](https://doi.org/10.48550/arXiv.2302.12246) Ôºà**2023.02.23**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-100-blue)](https://github.com/shizhediao/active-cot)


</div>

üëâ[Complete paper list üîó for "Chain of Thought"](./PaperList/ChainofThoughtList.md)üëà

### Knowledge Augmented Prompt

<div style="line-height:0.2em;">



[**LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model**](https://arxiv.org/abs/2304.06248) Ôºà**2023.04.13**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-21-blue)](https://github.com/chocowu/lasuie)

[**Commonsense-Aware Prompting for Controllable Empathetic Dialogue Generation**](https://doi.org/10.48550/arXiv.2302.01441) Ôºà**2023.02.02**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**REPLUG: Retrieval-Augmented Black-Box Language Models**](https://doi.org/10.48550/arXiv.2301.12652) Ôºà**2023.01.30**Ôºâ

![](https://img.shields.io/badge/Citations-3-green)

[**Self-Instruct: Aligning Language Model with Self Generated Instructions**](https://doi.org/10.48550/arXiv.2212.10560) Ôºà**2022.12.20**Ôºâ

![](https://img.shields.io/badge/Citations-9-green)  [![](https://img.shields.io/badge/Github%20Stars-2.1k-blue)](https://github.com/yizhongw/self-instruct)

[**One Embedder, Any Task: Instruction-Finetuned Text Embeddings**](https://doi.org/10.48550/arXiv.2212.09741) Ôºà**2022.12.19**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-472-blue)](https://github.com/HKUNLP/instructor-embedding)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) Ôºà**2022.12.16**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/hlzhang109/lmlp)

[**Don‚Äôt Prompt, Search! Mining-based Zero-Shot Learning with Language Models**](https://doi.org/10.48550/arXiv.2210.14803) Ôºà**2022.10.26**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding**](https://doi.org/10.48550/arXiv.2210.08536) Ôºà**2022.10.16**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/wjn1996/kp-plm)

[**Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding**](https://doi.org/10.48550/arXiv.2210.03304) Ôºà**2022.10.07**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-27-blue)](https://github.com/whaleloops/KEPT)

[**Promptagator: Few-shot Dense Retrieval From 8 Examples**](https://doi.org/10.48550/arXiv.2209.11755) Ôºà**2022.09.23**Ôºâ

![](https://img.shields.io/badge/Citations-16-green)


</div>

üëâ[Complete paper list üîó for "Knowledge Augmented Prompt"](./PaperList/KnowledgeAugmentedPromptList.md)üëà


### Evaluation & Reliability

<div style="line-height:0.2em;">



[**AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models**](https://arxiv.org/abs/2304.06364) Ôºà**2023.04.13**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-28-red)  [![](https://img.shields.io/badge/Github%20Stars-225-blue)](https://github.com/microsoft/agieval)

[**GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment**](https://arxiv.org/abs/2303.16634) Ôºà**2023.03.29**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-42-red)

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) Ôºà**2023.03.01**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints**](https://doi.org/10.48550/arXiv.2302.09185) Ôºà**2023.02.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/salt-nlp/bound-cap-llm)

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) Ôºà**2023.02.11**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/livnlp/prompt-robustness)

[**Controlling for Stereotypes in Multimodal Language Model Evaluation**](https://doi.org/10.48550/arXiv.2302.01582) Ôºà**2023.02.03**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Large Language Models Can Be Easily Distracted by Irrelevant Context**](https://doi.org/10.48550/arXiv.2302.00093) Ôºà**2023.01.31**Ôºâ

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-17-blue)](https://github.com/google-research-datasets/gsm-ic)

[**Emergent Analogical Reasoning in Large Language Models**](https://doi.org/10.48550/arXiv.2212.09196) Ôºà**2022.12.19**Ôºâ

![](https://img.shields.io/badge/Citations-5-green)  [![](https://img.shields.io/badge/Github%20Stars-14-blue)](https://github.com/taylorwwebb/emergent_analogies_llm)

[**Discovering Language Model Behaviors with Model-Written Evaluations**](https://doi.org/10.48550/arXiv.2212.09251) Ôºà**2022.12.19**Ôºâ

![](https://img.shields.io/badge/Citations-8-green)  [![](https://img.shields.io/badge/Github%20Stars-136-blue)](https://github.com/anthropics/evals)

[**Constitutional AI: Harmlessness from AI Feedback**](https://doi.org/10.48550/arXiv.2212.08073) Ôºà**2022.12.15**Ôºâ

![](https://img.shields.io/badge/Citations-21-green)  [![](https://img.shields.io/badge/Github%20Stars-106-blue)](https://github.com/anthropics/constitutionalharmlessnesspaper)


</div>

üëâ[Complete paper list üîó for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)üëà

## In-context Learning

<div style="line-height:0.2em;">



[**Self-Refine: Iterative Refinement with Self-Feedback**](https://arxiv.org/abs/2303.17651) Ôºà**2023.03.30**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-57-red)  [![](https://img.shields.io/badge/Github%20Stars-215-blue)](https://github.com/madaan/self-refine)

[**Larger language models do in-context learning differently**](https://doi.org/10.48550/arXiv.2303.03846) Ôºà**2023.03.07**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) Ôºà**2023.02.23**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) Ôºà**2023.02.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PLACES: Prompting Language Models for Social Conversation Synthesis**](https://doi.org/10.48550/arXiv.2302.03269) Ôºà**2023.02.07**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/alexa/places)

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) Ôºà**2023.01.27**Ôºâ

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/wangxinyilinda/concept-based-demonstration-selection)

[**Transformers as Algorithms: Generalization and Stability in In-context Learning**](https://arxiv.org/abs/2301.07067) Ôºà**2023.01.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-20-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) Ôºà**2022.12.22**Ôºâ

![](https://img.shields.io/badge/Citations-11-green)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) Ôºà**2022.12.21**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)

[**In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models**](https://doi.org/10.48550/arXiv.2212.10670) Ôºà**2022.12.20**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "In-context Learning"](./PaperList/InContextLearningList.md)üëà

## Multimodal Prompt

<div style="line-height:0.2em;">



[**MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers**](https://arxiv.org/abs/2305.07185) Ôºà**2023.05.12**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models**](https://doi.org/10.48550/arXiv.2305.04441) Ôºà**2023.05.08**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting**](https://doi.org/10.48550/arXiv.2305.03902) Ôºà**2023.05.06**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Edit Everything: A Text-Guided Generative System for Images Editing**](https://arxiv.org/abs/2304.14006) Ôºà**2023.04.27**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-67-blue)](https://github.com/defengxie/edit_everything)

[**ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**](https://arxiv.org/abs/2304.14407) Ôºà**2023.04.27**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-10-red)

[**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178) Ôºà**2023.04.27**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-9-red)  [![](https://img.shields.io/badge/Github%20Stars-578-blue)](https://github.com/x-plug/mplug-owl)

[**Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**](https://doi.org/10.48550/arXiv.2304.09337) Ôºà**2023.04.18**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Towards Robust Prompts on Vision-Language Models**](https://arxiv.org/abs/2304.08479) Ôºà**2023.04.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)

[**Visual Instruction Tuning**](https://arxiv.org/abs/2304.08485) Ôºà**2023.04.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-91-red)  [![](https://img.shields.io/badge/Github%20Stars-2.3k-blue)](https://github.com/haotian-liu/LLaVA)

[**Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text**](https://arxiv.org/abs/2304.06939) Ôºà**2023.04.14**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-43-red)  [![](https://img.shields.io/badge/Github%20Stars-682-blue)](https://github.com/allenai/mmc4)


</div>

üëâ[Complete paper list üîó for "Multimodal Prompt"](./PaperList/multimodalprompt.md)üëà

## Prompt Application

<div style="line-height:0.2em;">



[**Emergent and Predictable Memorization in Large Language Models**](https://arxiv.org/abs/2304.11158) Ôºà**2023.04.21**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-11-red)

[**SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks**](https://doi.org/10.48550/arXiv.2303.00733) Ôºà**2023.03.01**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis**](https://doi.org/10.48550/arXiv.2303.00815) Ôºà**2023.03.01**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**EvoPrompting: Language Models for Code-Level Neural Architecture Search**](https://doi.org/10.48550/arXiv.2302.14838) Ôºà**2023.02.28**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models**](https://doi.org/10.48550/arXiv.2302.12173) Ôºà**2023.02.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales**](https://doi.org/10.48550/arXiv.2302.08961) Ôºà**2023.02.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LabelPrompt: Effective Prompt-based Learning for Relation Classification**](https://doi.org/10.48550/arXiv.2302.08068) Ôºà**2023.02.16**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition**](https://doi.org/10.48550/arXiv.2302.08102) Ôºà**2023.02.16**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Prompting for Multimodal Hateful Meme Classification**](https://doi.org/10.48550/arXiv.2302.04156) Ôºà**2023.02.08**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**Toxicity Detection with Generative Prompt-based Inference**](https://doi.org/10.48550/arXiv.2205.12390) Ôºà**2022.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)


</div>

üëâ[Complete paper list üîó for "Prompt Application"](./PaperList/promptapplication.md)üëà

## Foundation Models

<div style="line-height:0.2em;">



[**X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages**](https://doi.org/10.48550/arXiv.2305.04160) Ôºà**2023.05.07**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision**](https://doi.org/10.48550/arXiv.2305.03047) Ôºà**2023.05.04**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**AutoML-GPT: Automatic Machine Learning with GPT**](https://doi.org/10.48550/arXiv.2305.02499) Ôºà**2023.05.04**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**](https://doi.org/10.48550/arXiv.2305.02301) Ôºà**2023.05.03**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**Unlimiformer: Long-Range Transformers with Unlimited Length Input**](https://doi.org/10.48550/arXiv.2305.01625) Ôºà**2023.05.02**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-668-blue)](https://github.com/abertsch72/unlimiformer)

[**Transfer Visual Prompt Generator across LLMs**](https://doi.org/10.48550/arXiv.2305.01278) Ôºà**2023.05.02**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback**](https://arxiv.org/abs/2304.10750) Ôºà**2023.04.21**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)

[**Segment Anything Model for Medical Image Analysis: an Experimental Study**](https://arxiv.org/abs/2304.10517) Ôºà**2023.04.20**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-24-red)  [![](https://img.shields.io/badge/Github%20Stars-14-blue)](https://github.com/mazurowski-lab/segment-anything-medical)

[**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**](https://arxiv.org/abs/2304.09842) Ôºà**2023.04.19**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-55-red)  [![](https://img.shields.io/badge/Github%20Stars-656-blue)](https://github.com/lupantech/chameleon-llm)

[**Accuracy of Segment-Anything Model (SAM) in medical image segmentation tasks**](https://arxiv.org/abs/2304.09324) Ôºà**2023.04.18**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-12-red)


</div>

üëâ[Complete paper list üîó for "Foundation Models"](./PaperList/foundationmodels.md)üëà

<!-- ### üìå Hard Prompt/ Discrete Prompt

<div style="line-height:0.2em;">



[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) Ôºà**2023.02.07**Ôºâ

![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-405-blue)](https://github.com/YuxinWenRick/hard-prompts-made-easy)

[**Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?**](https://doi.org/10.48550/arXiv.2301.00184) Ôºà**2022.12.31**Ôºâ

![](https://img.shields.io/badge/Citations-4-green)

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) Ôºà**2022.12.21**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**ADEPT: A DEbiasing PrompT Framework**](https://doi.org/10.48550/arXiv.2211.05414) Ôºà**2022.11.10**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/EmpathYang/ADEPT)

[**PromptAttack: Prompt-based Attack for Language Models via Gradient Search**](https://doi.org/10.48550/arXiv.2209.01882) Ôºà**2022.09.05**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) Ôºà**2022.05.25**Ôºâ

![](https://img.shields.io/badge/Citations-25-green)  [![](https://img.shields.io/badge/Github%20Stars-192-blue)](https://github.com/mingkaid/rl-prompt)

[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2202.07371) Ôºà**2022.02.15**Ôºâ

![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-18-red)  [![](https://img.shields.io/badge/Github%20Stars-65-blue)](https://github.com/lileipisces/pepler)

[**Instance-aware Prompt Learning for Language Understanding and Generation**](https://arxiv.org/abs/2201.07126) Ôºà**2022.01.18**Ôºâ

![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/jinfeihu-stan/ipl)


</div>

üëâ[Complete paper list üîó for "Hard Prompt"](./PaperList/HardPromptList.md)üëà

### üìå Soft Prompt/ Continuous Prompt

<div style="line-height:0.2em;">




</div>

üëâ[Complete paper list üîó for "Soft Prompt"](./PaperList/SoftPromptList.md)üëà -->

<!-- ## Prompt for Knowledge Graph

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected',type:'lite'}

üëâ[Complete paper list üîó for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)üëà --> 

<img width="200%" src="./figures/hr.gif" />

<!-- # üéì Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```

ÁªºËø∞ËÆ∫ÊñáÂèØ‰ª•ÊîæÂú®Ëøô‰∏™‰ΩçÁΩÆ

``` -->

<!-- <img width="200%" src="./figures/hr.gif" /> -->

# üë®‚Äçüíª LLM Usage
Large language models (LLMs) are becoming a revolutionary technology that is shaping the development of our era. Developers can create applications that were previously only possible in our imaginations by building LLMs. However, using these LLMs often comes with certain technical barriers, and even at the introductory stage, people may be intimidated by cutting-edge technology: Do you have any questions like the following?

- ‚ùì *How can LLM be built using programming?* 
- ‚ùì *How can it be used and deployed in your own programs?* 

üí° If there was a tutorial that could be accessible to all audiences, not just computer science professionals, it would provide detailed and comprehensive guidance to quickly get started and operate in a short amount of time, ultimately achieving the goal of being able to use LLMs flexibly and creatively to build the programs they envision. And now, just for you: the most detailed and comprehensive Langchain beginner's guide, sourced from the official langchain website but with further adjustments to the content, accompanied by the most detailed and annotated code examples, teaching code lines by line and sentence by sentence to all audiences.

**Click üëâ[here](./langchain_guide/LangChainTutorial.ipynb)üëà to take a quick tour of getting started with LLM.**

<img width="200%" src="./figures/hr.gif" />

# ‚úâÔ∏è Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `helloegoalpha@gmail.com`.

We are willing to engage in discussions with friends from the academic and industrial communities, and explore the latest developments in prompt engineering and in-context learning together.

<img width="200%" src="./figures/hr.gif" />

# üôè Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. We also would like to express our sincere gratitude to the authors of the relevant resources. Your efforts have broadened our horizons and enabled us to perceive a more wonderful world.


<!-- <img width="200%" src="./figures/hr.gif" /> -->

<!-- # üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]() -->


<!-- ## Guest Contributors
* [No] -->

<!-- <img width="200%" src="./figures/hr.gif" />

# üìî License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div> -->