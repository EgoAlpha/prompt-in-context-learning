<div align="center">


<img src="./figures/Prompt-EgoAlpha_white.svg" width="600px">

 <div align="center">

 [![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=30&duration=2500&pause=500&color=8D589A&background=FCFCFF00&center=true&vCenter=true&width=500&lines=Hello!+Human%2C+Are+You+Ready%3F;Welcome+to+my+world!)]()
 
 </div>

**An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">

    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>

</h3> -->

<!-- <h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./chatgptprompt_zh.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
    <p>
</h4> -->

<p align="center">

  <a href="#üìú-papers">üìù Papers</a> |
  <a href="./Playground.md">‚ö°Ô∏è  Playground</a> |
  <a href="./PromptEngineering.md">üõ† Prompt Engineering</a> |
  <a href="./chatgptprompt.md">üåç ChatGPT Prompt</a> ÔΩú
  <a href="./langchain_guide/LangChainTutorial.ipynb">‚õ≥ LLMs Usage Guide</a> 

</p>

</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->

![version](https://img.shields.io/badge/version-v2.0.0-yellow)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)

<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->

</div>

> **‚≠êÔ∏è Shining ‚≠êÔ∏è:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, let‚Äôs take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*üéâ[Papers](#üìú-papers)üéâ*:  The latest papers about in-context learning or prompt engineering. 

*üéâ[Playground](./Playground.md)üéâ*:  Large language models that enable prompt experimentation. 

*üéâ[Prompt Engineering](./promptengineering.md)üéâ*: Prompt techniques for leveraging large language models. 

*üéâ[ChatGPT Prompt](./chatgptprompt.md)üéâ*: Prompt examples that can be applied in our work and daily lives. 

*üéâ[LLMs Usage Guide](./chatgptprompt.md)üéâ*: The method for quickly getting started with large language models by using LangChain.

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): 
- Those who enhance their abilities through the use of AI; 
- Those whose jobs are replaced by AI automation.

```

üíéEgoAlpha: Hello! humanüë§, are you ready?

```  

# üì¢ News
<!-- üî•üî•üî• -->
‚òÑÔ∏è **EgoAlpha releases the TrustGPT focuses on reasoning. Trust the GPT with the strongest reasoning abilities for authentic and reliable answers. You can click [here](https://trustgpt.co) or visit the [Playgrounds](./Playground.md) directly to experience it„ÄÇ**

- **[2023.6.24]**
    - Paper: [PromptIR: Prompting for All-in-One Blind Image Restoration](https://www.researchgate.net/publication/371786106_PromptIR_Prompting_for_All-in-One_Blind_Image_Restoration)

- **[2023.6.23]**
    - Paper: [OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue](https://arxiv.org/format/2306.12174)
    
- **[2023.6.22]**
    - [Stanford has released an automatic evaluation system for LLM called AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)
    - [Ocean-1: the world's first contact center foundation model.](https://cresta.com/blog/introducing-ocean-1-worlds-first-contact-center-foundation-model/)

- **[2023.6.21]**
    - [GPT-Engineering, Who generates an entire codebase based on a prompt.](https://github.com/AntonOsika/gpt-engineer)

- **[2023.6.20]**
    - Paper: [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://scontent-fra3-1.xx.fbcdn.net/v/t39.8562-6/354636794_599417672291955_3799385851435258804_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=PW3or0UVoKoAX-2_D0q&_nc_ht=scontent-fra3-1.xx&oh=00_AfB2SW8Rp55YKG0AEJcpC9tUECbXdl_m83yk9cxX7jie1A&oe=64967631)
    
- **[2023.6.19]**
    - Technical Report: [AIGC industry overview article from SEALAND SECURITIES](https://pdf.dfcfw.com/pdf/H3_AP202303201584404280_1.pdf?1679327615000.pdf)

- **[2023.6.18]**
    - Paper: [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory](https://arxiv.org/abs/2306.03901)
    
- **[2023.6.17]**
    - Paper: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://www.researchgate.net/publication/371540959_Macaw-LLM_Multi-Modal_Language_Modeling_with_Image_Audio_Video_and_Text_Integration)

- **[2023.6.16]**
    - Financial [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) model open source, benchmarked against BloombergGPT, training parameters can be reduced from 6.17 billion to 3.67 million, can predict stock prices.
    ([Paper](https://arxiv.org/abs/2306.06031)/[Code](https://github.com/AI4Finance-Foundation/FinGPT))

- **[2023.6.15]**
    - Paper:[MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://arxiv.org/abs/2306.01311 )
    
- **[2023.6.14]**
    - Paper:[XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models](https://arxiv.org/abs/2306.07971)
    
- **[2023.6.13]**
    - Paper:[Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
 
- **[2023.6.12]**
    - Paper: [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425)
    
- **[2023.6.11]**
    - Paper: [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/abs/2306.05179)

- **[2023.6.10]**
    - [Aquila, language model series, including the Aquila Basic Model (7B and 33B), AquilaChat dialogue model, and AquilaCode text-to-code generation model.](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)
    
- **[2023.6.9]**
    - Paper: [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)

[üëâ Complete history news üëà](./historynews.md)

<img width="200%" src="./figures/hr.gif" />

# üìú Papers

> You can directly click on the title to jump to the corresponding PDF link location

- [üì¢ News](#-news)
- [üìú Papers](#-papers)
  - [Survey](#survey)
  - [Prompt Engineering](#prompt-engineering)
    - [Prompt Design](#prompt-design)
    - [Automatic Prompt](#automatic-prompt)
    - [Chain of Thought](#chain-of-thought)
    - [Knowledge Augmented Prompt](#knowledge-augmented-prompt)
    - [Evaluation \& Reliability](#evaluation--reliability)
  - [In-context Learning](#in-context-learning)
  - [Multimodal Prompt](#multimodal-prompt)
  - [Prompt Application](#prompt-application)
  - [Foundation Models](#foundation-models)
- [üë®‚Äçüíª LLM Usage](#-llm-usage)
- [‚úâÔ∏è Contact](#Ô∏è-contact)
- [üôè Acknowledgements](#-acknowledgements)

---

## Survey

<div style="line-height:0.2em;">



[**Prompt Engineering for Healthcare: Methodologies and Applications**](https://doi.org/10.48550/arXiv.2304.14670) Ôºà**2023.04.28**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond**](https://arxiv.org/abs/2304.13712) Ôºà**2023.04.26**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-186-red)  [![](https://img.shields.io/badge/Github%20Stars-4.5k-blue)](https://github.com/mooler0410/llmspracticalguide)

[**A Survey of Large Language Models**](https://arxiv.org/abs/2303.18223) Ôºà**2023.03.31**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-480-red)  [![](https://img.shields.io/badge/Github%20Stars-2.0k-blue)](https://github.com/rucaibox/llmsurvey)

[**Augmented Language Models: a Survey**](https://doi.org/10.48550/arXiv.2302.07842) Ôºà**2023.02.15**Ôºâ

![](https://img.shields.io/badge/Citations-4-green)

[**A Survey for In-context Learning**](https://doi.org/10.48550/arXiv.2301.00234) Ôºà**2022.12.31**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Towards Reasoning in Large Language Models: A Survey**](https://doi.org/10.48550/arXiv.2212.10403) Ôºà**2022.12.20**Ôºâ

![](https://img.shields.io/badge/Citations-5-green)  [![](https://img.shields.io/badge/Github%20Stars-296-blue)](https://github.com/jeffhj/lm-reasoning)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) Ôºà**2022.12.19**Ôºâ

![](https://img.shields.io/badge/Citations-7-green)  [![](https://img.shields.io/badge/Github%20Stars-375-blue)](https://github.com/zjunlp/Prompt4ReasoningPapers)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) Ôºà**2021.07.28**Ôºâ

![](https://img.shields.io/badge/Citations-462-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1.6k-red)  [![](https://img.shields.io/badge/Github%20Stars-201-blue)](https://github.com/mingkaid/rl-prompt)


</div>

üëâ[Complete paper list üîó for "Survey"](./PaperList/survey.md)üëà

## Prompt Engineering

### Prompt Design

<div style="line-height:0.2em;">



[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Impersonation Reveals Large Language Models' Strengths and Biases**](https://arxiv.org/abs/2305.14930) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

[**Universal Self-adaptive Prompting**](https://arxiv.org/abs/2305.14926) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions**](https://arxiv.org/abs/2305.14908) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering**](https://arxiv.org/abs/2305.14901) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

[**Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis**](https://arxiv.org/abs/2305.14877) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions**](https://arxiv.org/abs/2305.14795) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Adapting Language Models to Compress Contexts**](https://arxiv.org/abs/2305.14788) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)


</div>

üëâ[Complete paper list üîó for "Prompt Design"](./PaperList/PromptDesignList.md)üëà

### Automatic Prompt 

<div style="line-height:0.2em;">



[**Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker**](https://arxiv.org/abs/2305.13729) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-2-red)

[**Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement**](https://arxiv.org/abs/2305.14497) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes**](https://arxiv.org/abs/2305.13499) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Automated Few-shot Classification with Instruction-Finetuned Language Models**](https://arxiv.org/abs/2305.12576) Ôºà**2023.05.21**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**AutoTrial: Prompting Language Models for Clinical Trial Design**](https://doi.org/10.48550/arXiv.2305.11366) Ôºà**2023.05.19**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency**](https://doi.org/10.48550/arXiv.2305.10713) Ôºà**2023.05.18**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/shadowkiller33/flatness)

[**Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt**](https://doi.org/10.48550/arXiv.2305.11186) Ôºà**2023.05.17**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data**](https://doi.org/10.48550/arXiv.2302.12822) Ôºà**2023.02.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Guiding Large Language Models via Directional Stimulus Prompting**](https://doi.org/10.48550/arXiv.2302.11520) Ôºà**2023.02.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-11-blue)](https://github.com/leezekun/directional-stimulus-prompting)

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) Ôºà**2023.02.11**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/livnlp/prompt-robustness)


</div>

üëâ[Complete paper list üîó for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)üëà

### Chain of Thought

<div style="line-height:0.2em;">



[**Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration**](https://arxiv.org/abs/2305.15262) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations**](https://arxiv.org/abs/2305.14556) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

[**Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization**](https://arxiv.org/abs/2305.14548) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/kenchan0226/finegrainedfact)

[**ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**](https://arxiv.org/abs/2305.14323) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/rucaibox/chatcot)

[**mPMR: A Multilingual Pre-trained Machine Reader at Scale**](https://arxiv.org/abs/2305.13645) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-3-green)  [![](https://img.shields.io/badge/Github%20Stars-4-blue)](https://github.com/damo-nlp-sg/pmr)

[**Cross-functional Analysis of Generalisation in Behavioural Learning**](https://arxiv.org/abs/2305.12951) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space**](https://arxiv.org/abs/2305.12798) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)


</div>

üëâ[Complete paper list üîó for "Chain of Thought"](./PaperList/ChainofThoughtList.md)üëà

### Knowledge Augmented Prompt

<div style="line-height:0.2em;">



[**Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?**](https://arxiv.org/abs/2305.15183) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Referral Augmentation for Zero-Shot Information Retrieval**](https://arxiv.org/abs/2305.15098) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Decomposing Complex Queries for Tip-of-the-tongue Retrieval**](https://arxiv.org/abs/2305.15053) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LLMDet: A Large Language Models Detection Tool**](https://arxiv.org/abs/2305.15004) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Machine Reading Comprehension using Case-based Reasoning**](https://arxiv.org/abs/2305.14815) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization**](https://arxiv.org/abs/2305.14760) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Knowledge Augmented Prompt"](./PaperList/KnowledgeAugmentedPromptList.md)üëà


### Evaluation & Reliability

<div style="line-height:0.2em;">



[**SETI: Systematicity Evaluation of Textual Inference**](https://arxiv.org/abs/2305.15045) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Is GPT-4 a Good Data Analyst?**](https://arxiv.org/abs/2305.15038) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions**](https://arxiv.org/abs/2305.14874) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

[**Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples**](https://arxiv.org/abs/2305.15269) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

[**EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models**](https://arxiv.org/abs/2305.15268) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**HuatuoGPT, towards Taming Language Model to Be a Doctor**](https://arxiv.org/abs/2305.15075) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-31-blue)](https://github.com/freedomintelligence/huatuogpt)

[**Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models**](https://arxiv.org/abs/2305.15074) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-6-red)  [![](https://img.shields.io/badge/Github%20Stars-20-blue)](https://github.com/hgaurav2k/jeebench)

[**ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories**](https://arxiv.org/abs/2305.15028) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-2-blue)](https://github.com/hemingkx/imagenetvc)

[**Sentiment Analysis in the Era of Large Language Models: A Reality Check**](https://arxiv.org/abs/2305.15005) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**A RelEntLess Benchmark for Modelling Graded Relations between Named Entities**](https://arxiv.org/abs/2305.15002) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)üëà

## In-context Learning

<div style="line-height:0.2em;">



[**Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering**](https://arxiv.org/abs/2305.15387) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**READ: Recurrent Adaptation of Large Transformers**](https://arxiv.org/abs/2305.15348) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

[**Learning Answer Generation using Supervision from Automatic Question Answering Evaluators**](https://arxiv.org/abs/2305.15344) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**SummIt: Iterative Text Summarization via ChatGPT**](https://arxiv.org/abs/2305.14835) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations**](https://arxiv.org/abs/2305.14728) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing**](https://arxiv.org/abs/2305.15338) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

[**Self-Evolution Learning for Discriminative Language Model Pretraining**](https://arxiv.org/abs/2305.15275) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-1-green)

[**PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology**](https://arxiv.org/abs/2305.15072) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/superjamessyx/generative-foundation-ai-assistant-for-pathology)

[**Not All Metrics Are Guilty: Improving NLG Evaluation with LLM Paraphrasing**](https://arxiv.org/abs/2305.15067) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-1-blue)](https://github.com/rucaibox/para-ref)

[**Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization**](https://arxiv.org/abs/2305.15008) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "In-context Learning"](./PaperList/InContextLearningList.md)üëà

## Multimodal Prompt

<div style="line-height:0.2em;">



[**Meta-Learning For Vision-and-Language Cross-lingual Transfer**](https://arxiv.org/abs/2305.14843) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LayoutGPT: Compositional Visual Planning and Generation with Large Language Models**](https://arxiv.org/abs/2305.15393) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)

[**Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models**](https://arxiv.org/abs/2305.15080) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-4-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/naver-ai/cream)

[**EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought**](https://arxiv.org/abs/2305.15021) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-3-red)  [![](https://img.shields.io/badge/Github%20Stars-19-blue)](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks**](https://arxiv.org/abs/2305.14201) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-14-red)

[**Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations**](https://arxiv.org/abs/2305.14556) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-1-red)

[**Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization**](https://arxiv.org/abs/2305.14548) Ôºà**2023.05.23**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/kenchan0226/finegrainedfact)


</div>

üëâ[Complete paper list üîó for "Multimodal Prompt"](./PaperList/multimodalprompt.md)üëà

## Prompt Application

<div style="line-height:0.2em;">



[**Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering**](https://arxiv.org/abs/2305.15387) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LLMDet: A Large Language Models Detection Tool**](https://arxiv.org/abs/2305.15004) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Machine Reading Comprehension using Case-based Reasoning**](https://arxiv.org/abs/2305.14815) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions**](https://arxiv.org/abs/2305.14795) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**A Causal View of Entity Bias in (Large) Language Models**](https://arxiv.org/abs/2305.14695) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Prompt Application"](./PaperList/promptapplication.md)üëà

## Foundation Models

<div style="line-height:0.2em;">



[**Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering**](https://arxiv.org/abs/2305.15387) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LLMDet: A Large Language Models Detection Tool**](https://arxiv.org/abs/2305.15004) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Frugal Prompting for Dialog Models**](https://arxiv.org/abs/2305.14919) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Machine Reading Comprehension using Case-based Reasoning**](https://arxiv.org/abs/2305.14815) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions**](https://arxiv.org/abs/2305.14795) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**A Causal View of Entity Bias in (Large) Language Models**](https://arxiv.org/abs/2305.14695) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations**](https://arxiv.org/abs/2305.14618) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Foundation Models"](./PaperList/foundationmodels.md)üëà

<!-- ### üìå Hard Prompt/ Discrete Prompt

<div style="line-height:0.2em;">



[**LLMDet: A Large Language Models Detection Tool**](https://arxiv.org/abs/2305.15004) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach**](https://arxiv.org/abs/2305.14973) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification**](https://arxiv.org/abs/2305.14963) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**In-Context Demonstration Selection with Cross Entropy Difference**](https://arxiv.org/abs/2305.14726) Ôºà**2023.05.24**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Cross-functional Analysis of Generalisation in Behavioural Learning**](https://arxiv.org/abs/2305.12951) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer**](https://arxiv.org/abs/2305.12761) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches**](https://arxiv.org/abs/2305.12749) Ôºà**2023.05.22**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4**](https://arxiv.org/abs/2305.12147) Ôºà**2023.05.20**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-10-blue)](https://github.com/csitfun/logicot)

[**SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs**](https://doi.org/10.48550/arXiv.2305.11461) Ôºà**2023.05.19**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)

[**Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning**](https://doi.org/10.48550/arXiv.2305.11759) Ôºà**2023.05.19**Ôºâ

![](https://img.shields.io/badge/Citations-0-green)


</div>

üëâ[Complete paper list üîó for "Hard Prompt"](./PaperList/HardPromptList.md)üëà

### üìå Soft Prompt/ Continuous Prompt

<div style="line-height:0.2em;">




</div>

üëâ[Complete paper list üîó for "Soft Prompt"](./PaperList/SoftPromptList.md)üëà -->

<!-- ## Prompt for Knowledge Graph

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected',type:'lite'}

üëâ[Complete paper list üîó for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)üëà --> 

<img width="200%" src="./figures/hr.gif" />

<!-- # üéì Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```

ÁªºËø∞ËÆ∫ÊñáÂèØ‰ª•ÊîæÂú®Ëøô‰∏™‰ΩçÁΩÆ

``` -->

<!-- <img width="200%" src="./figures/hr.gif" /> -->

# üë®‚Äçüíª LLM Usage
Large language models (LLMs) are becoming a revolutionary technology that is shaping the development of our era. Developers can create applications that were previously only possible in our imaginations by building LLMs. However, using these LLMs often comes with certain technical barriers, and even at the introductory stage, people may be intimidated by cutting-edge technology: Do you have any questions like the following?

- ‚ùì *How can LLM be built using programming?* 
- ‚ùì *How can it be used and deployed in your own programs?* 

üí° If there was a tutorial that could be accessible to all audiences, not just computer science professionals, it would provide detailed and comprehensive guidance to quickly get started and operate in a short amount of time, ultimately achieving the goal of being able to use LLMs flexibly and creatively to build the programs they envision. And now, just for you: the most detailed and comprehensive Langchain beginner's guide, sourced from the official langchain website but with further adjustments to the content, accompanied by the most detailed and annotated code examples, teaching code lines by line and sentence by sentence to all audiences.

**Click üëâ[here](./langchain_guide/LangChainTutorial.ipynb)üëà to take a quick tour of getting started with LLM.**

<img width="200%" src="./figures/hr.gif" />

# ‚úâÔ∏è Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `helloegoalpha@gmail.com`.

We are willing to engage in discussions with friends from the academic and industrial communities, and explore the latest developments in prompt engineering and in-context learning together.

<img width="200%" src="./figures/hr.gif" />

# üôè Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. We also would like to express our sincere gratitude to the authors of the relevant resources. Your efforts have broadened our horizons and enabled us to perceive a more wonderful world.


<!-- <img width="200%" src="./figures/hr.gif" /> -->

<!-- # üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]() -->


<!-- ## Guest Contributors
* [No] -->

<!-- <img width="200%" src="./figures/hr.gif" />

# üìî License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div> -->