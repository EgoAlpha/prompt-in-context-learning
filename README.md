<div align="center">

<img src="./figures/Prompt-EgoAlpha_white.svg" width="600px">

 <div align="center">
     
 [![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=30&duration=2500&pause=500&color=8D589A&background=FCFCFF00&center=true&vCenter=true&width=500&lines=Hello!+Human%2C+Are+You+Ready%3F;Welcome+to+my+world!)]()
     
 </div>



**An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./chatgptprompt_zh.md">ç®€ä½“ä¸­æ–‡</a>
    <p>
</h4>

<p align="center">
  <a href="#ğŸ“œ-papers">ğŸ“ Papers</a> |
  <a href="./Playground.md">âš¡ï¸  Playground</a> |
  <a href="./PromptEngineering.md">ğŸ›  Prompt Engineering</a> |
  <a href="./chatgptprompt.md">ğŸŒ ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **â­ï¸ Shining â­ï¸:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, letâ€™s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*ğŸ‰[Papers](#ğŸ“œ-papers)ğŸ‰*:  The latest papers about in-context learning or prompt engineering. 

*ğŸ‰[Playground](./Playground.md)ğŸ‰*:  Large language models that enable prompt experimentation. 

*ğŸ‰[Prompt Engineering](./PromptEngineering.md)ğŸ‰*: Prompt techniques for leveraging large language models. 

*ğŸ‰[ChatGPT Prompt](./chatgptprompt.md)ğŸ‰*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
ğŸ’EgoAlpha: Hello! humanğŸ‘¤, are you ready?
```  

# ğŸ“¢ News

- **[2023.3.9]** GPT-4 is coming next week and it will be multimodal,announced by OpenAI.
- **[2023.3.8]** [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)
- **[2023.3.7]** [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)
- **[2023.3.6]** [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)


<img width="200%" src="./figures/hr.gif" />

# ğŸ“œ Papers

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
<!-- - [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph) -->

---

## Prompt Engineering

### ğŸ“Œ Prompt Design

[**How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks**](https://doi.org/10.48550/arXiv.2303.00293) ğŸ‘¨â€ğŸ“Xuanting Chen,Junjie Ye,Can Zu,Nuo Xu,Rui Zheng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks**](https://doi.org/10.48550/arXiv.2303.00733) ğŸ‘¨â€ğŸ“Kai-Wei Chang,Yu-Kai Wang,Hua Shen,Iu-thing Kang,W. Tseng,etc 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis**](https://doi.org/10.48550/arXiv.2303.00815) ğŸ‘¨â€ğŸ“Jingli Shi,Weihua Li,Quan-wei Bai,Yi Yang,Jianhua Jiang 2023 ![](https://img.shields.io/badge/pub-2023--03--01-green)

[**EvoPrompting: Language Models for Code-Level Neural Architecture Search**](https://doi.org/10.48550/arXiv.2302.14838) ğŸ‘¨â€ğŸ“Angelica Chen,David Dohan,David R. So 2023 ![](https://img.shields.io/badge/pub-2023--02--28-green)

[**Language Model Crossover: Variation through Few-Shot Prompting**](https://doi.org/10.48550/arXiv.2302.12170) ğŸ‘¨â€ğŸ“Elliot Meyerson,M. Nelson,Herbie Bradley,Arash Moradi,Amy K. Hoover,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models**](https://doi.org/10.48550/arXiv.2302.12173) ğŸ‘¨â€ğŸ“Kai Greshake,Sahar Abdelnabi,Shailesh Mishra,C. Endres,Thorsten Holz,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT**](https://doi.org/10.48550/arXiv.2302.11382) ğŸ‘¨â€ğŸ“Jules White,Quchen Fu,Sam Hays,M. Sandborn,Carlos Olea,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales**](https://doi.org/10.48550/arXiv.2302.08961) ğŸ‘¨â€ğŸ“M. Ruskov 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks**](https://doi.org/10.48550/arXiv.2302.08043) ğŸ‘¨â€ğŸ“Zemin Liu,Xingtong Yu,Yuan Fang,Xinming Zhang 2023 ![](https://img.shields.io/badge/pub-2023--02--16-green)



ğŸ‘‰[Complete paper list ğŸ”— for prompt design](./PaperList/PromptDesignList.md)ğŸ‘ˆ

### ğŸ“Œ Automatic Prompt 

[**Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data**](https://doi.org/10.48550/arXiv.2302.12822) ğŸ‘¨â€ğŸ“Kashun Shum,Shizhe Diao,Tong Zhang 2023 ![](https://img.shields.io/badge/pub-2023--02--24-green)

[**Guiding Large Language Models via Directional Stimulus Prompting**](https://doi.org/10.48550/arXiv.2302.11520) ğŸ‘¨â€ğŸ“Zekun Li,Baolin Peng,Pengcheng He,Michel Galley,Jianfeng Gao,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) ğŸ‘¨â€ğŸ“Yoichi Ishibashi,D. Bollegala,Katsuhito Sudoh,Satoshi Nakamura 2023 ![](https://img.shields.io/badge/pub-2023--02--11-green)

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--07-green)![](https://img.shields.io/badge/cite-2-red)

[**Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language**](https://doi.org/10.1145/3545945.3569823) ğŸ‘¨â€ğŸ“Paul Denny,Viraj Kumar,Nasser Giacaman 2022 ![](https://img.shields.io/badge/pub-2022--10--27-green)![](https://img.shields.io/badge/cite-5-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Making Pre-trained Language Models Better Few-shot Learners**](https://doi.org/10.18653/v1/2021.acl-long.295) ğŸ‘¨â€ğŸ“Tianyu Gao,Adam Fisch,Danqi Chen 2021 ![](https://img.shields.io/badge/pub-2021--01--01-green)![](https://img.shields.io/badge/cite-642-red)

[**Eliciting Knowledge from Language Models Using Automatically Generated Prompts**](https://doi.org/10.18653/v1/2020.emnlp-main.346) ğŸ‘¨â€ğŸ“Taylor Shin,Yasaman Razeghi,Robert L Logan IV,Eric Wallace,Sameer Singh 2020 ![](https://img.shields.io/badge/pub-2020--10--29-green)![](https://img.shields.io/badge/cite-136-red)

[**Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification**](https://doi.org/10.5282/UBM/EPUB.74034) ğŸ‘¨â€ğŸ“Timo Schick,Helmut Schmid,Hinrich SchÃ¼tze 2020 ![](https://img.shields.io/badge/pub-2020--10--26-green)![](https://img.shields.io/badge/cite-84-red)

[**How Can We Know What Language Models Know?**](https://doi.org/10.1162/tacl_a_00324) ğŸ‘¨â€ğŸ“Zhengbao Jiang,Frank F. Xu,J. Araki,Graham Neubig 2019 ![](https://img.shields.io/badge/pub-2019--11--28-green)![](https://img.shields.io/badge/cite-423-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Chain of Thought

[**Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models**](https://doi.org/10.48550/arXiv.2302.00618) ğŸ‘¨â€ğŸ“Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--01-green)![](https://img.shields.io/badge/cite-2-red)

[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) ğŸ‘¨â€ğŸ“Namgyu Ho,Laura Schmid,Se-Young Yun 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-5-red)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) ğŸ‘¨â€ğŸ“Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-2-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Measuring and Narrowing the Compositionality Gap in Language Models**](https://doi.org/10.48550/arXiv.2210.03350) ğŸ‘¨â€ğŸ“Ofir Press,Muru Zhang,Sewon Min,Ludwig Schmidt,Noah A. Smith,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-27-red)

[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) ğŸ‘¨â€ğŸ“Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-21-red)

[**Decomposed Prompting: A Modular Approach for Solving Complex Tasks**](https://doi.org/10.48550/arXiv.2210.02406) ğŸ‘¨â€ğŸ“Tushar Khot,H. Trivedi,Matthew Finlayson,Yao Fu,Kyle Richardson,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--05-green)![](https://img.shields.io/badge/cite-25-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Compositional Semantic Parsing with Large Language Models**](https://doi.org/10.48550/arXiv.2209.15003) ğŸ‘¨â€ğŸ“Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,etc 2022 ![](https://img.shields.io/badge/pub-2022--09--29-green)![](https://img.shields.io/badge/cite-16-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Chain of Thought"](./PaperList/ChainofThoughtList.md)ğŸ‘ˆ

### ğŸ“Œ Evaluation & Reliability

[**Evaluating the Robustness of Discrete Prompts**](https://doi.org/10.48550/arXiv.2302.05619) ğŸ‘¨â€ğŸ“Yoichi Ishibashi,D. Bollegala,Katsuhito Sudoh,Satoshi Nakamura 2023 ![](https://img.shields.io/badge/pub-2023--02--11-green)

[**Controlling for Stereotypes in Multimodal Language Model Evaluation**](https://doi.org/10.48550/arXiv.2302.01582) ğŸ‘¨â€ğŸ“Manuj Malik,Richard Johansson 2023 ![](https://img.shields.io/badge/pub-2023--02--03-green)

[**Large Language Models Can Be Easily Distracted by Irrelevant Context**](https://doi.org/10.48550/arXiv.2302.00093) ğŸ‘¨â€ğŸ“Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--31-green)![](https://img.shields.io/badge/cite-3-red)

[**Emergent Analogical Reasoning in Large Language Models**](https://doi.org/10.48550/arXiv.2212.09196) ğŸ‘¨â€ğŸ“Taylor W. Webb,K. Holyoak,Hongjing Lu 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-5-red)

[**Discovering Language Model Behaviors with Model-Written Evaluations**](https://doi.org/10.48550/arXiv.2212.09251) ğŸ‘¨â€ğŸ“Ethan Perez,Sam Ringer,KamilÄ— LukoÅ¡iÅ«tÄ—,Karina Nguyen,Edwin Chen,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-6-red)

[**Constitutional AI: Harmlessness from AI Feedback**](https://doi.org/10.48550/arXiv.2212.08073) ğŸ‘¨â€ğŸ“Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell,John Kernion,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--15-green)![](https://img.shields.io/badge/cite-15-red)

[**On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08061) ğŸ‘¨â€ğŸ“Omar Shaikh,Hongxin Zhang,William B. Held,Michael Bernstein,Diyi Yang 2022 ![](https://img.shields.io/badge/pub-2022--12--15-green)![](https://img.shields.io/badge/cite-1-red)

[**Refining Semantic Similarity of Paraphasias Using a Contextual Language Model.**](https://doi.org/10.1044/2022_jslhr-22-00277) ğŸ‘¨â€ğŸ“Alexandra C. Salem,Robert C. Gale,Marianne Casilio,Mikala Fleegle,Gerasimos Fergadiotis,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--09-green)

[**Solving math word problems with process- and outcome-based feedback**](https://doi.org/10.48550/arXiv.2211.14275) ğŸ‘¨â€ğŸ“J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,etc 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-3-red)

[**Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**](https://doi.org/10.48550/arXiv.2211.12588) ğŸ‘¨â€ğŸ“Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen 2022 ![](https://img.shields.io/badge/pub-2022--11--22-green)![](https://img.shields.io/badge/cite-21-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)ğŸ‘ˆ

## In-context Learning

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) ğŸ‘¨â€ğŸ“Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-21-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,etc 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-26-red)

[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2302.135402205.11916) ğŸ‘¨â€ğŸ“Takeshi Kojima,S. Gu,Machel Reid,Yutaka Matsuo,Yusuke Iwasawa 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-185-red)

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2302.135402204.02311) ğŸ‘¨â€ğŸ“Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-612-red)

[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) ğŸ‘¨â€ğŸ“Andrew Kyle Lampinen,I. Dasgupta,Stephanie C. Y. Chan,Kory Matthewson,Michael Henry Tessler,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-61-red)



ğŸ‘‰[Complete paper list ğŸ”— for "In-context Learning"](./PaperList/InContextLearningList.md)ğŸ‘ˆ

## Multimodal Prompt

### ğŸ“Œ Hard Prompt/ Discrete Prompt

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--07-green)![](https://img.shields.io/badge/cite-2-red)

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) ğŸ‘¨â€ğŸ“Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,etc 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-25-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Hard Prompt"](./PaperList/HardPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Soft Prompt/ Continuous Prompt

[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) ğŸ‘¨â€ğŸ“G. Zuccon,B. Koopman 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://doi.org/10.48550/arXiv.2302.09236) ğŸ‘¨â€ğŸ“Yuhang Zhou,Suraj Maharjan,Bei Liu 2023 ![](https://img.shields.io/badge/pub-2023--02--18-green)

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) ğŸ‘¨â€ğŸ“Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) ğŸ‘¨â€ğŸ“Koustava Goswami,Lukas Lange,J. Araki,Heike Adel 2023 ![](https://img.shields.io/badge/pub-2023--02--14-green)

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) ğŸ‘¨â€ğŸ“Mingyu Derek Ma,Jiun-Yu Kao,Shuyang Gao,Arpit Gupta,Di Jin,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--26-green)

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) ğŸ‘¨â€ğŸ“M Saiful Bari,Aston Zhang,Shuai Zheng,Xingjian Shi,Yi Zhu,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) ğŸ‘¨â€ğŸ“Weijia Shi,Xiaochuang Han,Hila Gonen,Ari Holtzman,Yulia Tsvetkov,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-1-red)

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) ğŸ‘¨â€ğŸ“Damith Chamalke Senadeera,Julia Ive 2022 ![](https://img.shields.io/badge/pub-2022--12--06-green)

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2211.10681) ğŸ‘¨â€ğŸ“Xiaocheng Lu,Ziming Liu,Song Guo,Jingcai Guo 2022 ![](https://img.shields.io/badge/pub-2022--11--19-green)



ğŸ‘‰[Complete paper list ğŸ”— for "Soft Prompt"](./PaperList/SoftPromptList.md)ğŸ‘ˆ

<!-- ## Knowledge Augmented Prompts

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected'}

ğŸ‘‰[Complete paper list ğŸ”— for "Knowledge Augmented Prompts"](./PaperList/KnowledgeAugmentedPromptList.md)ğŸ‘ˆ

## Prompt for Knowledge Graph

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected'}

ğŸ‘‰[Complete paper list ğŸ”— for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)ğŸ‘ˆ -->

<!-- <img width="200%" src="./figures/hr.gif" /> -->

<!-- # ğŸ“ Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
ç»¼è¿°è®ºæ–‡å¯ä»¥æ”¾åœ¨è¿™ä¸ªä½ç½®
``` -->
<img width="200%" src="./figures/hr.gif" />

# âœ‰ï¸ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `helloegoalpha@gmail.com`.

We are willing to engage in discussions with friends from the academic and industrial communities, and explore the latest developments in prompt engineering and in-context learning together.

<img width="200%" src="./figures/hr.gif" />

# ğŸ™ Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. We also would like to express our sincere gratitude to the authors of the relevant resources. Your efforts have broadened our horizons and enabled us to perceive a more wonderful world.


<!-- <img width="200%" src="./figures/hr.gif" /> -->

<!-- # ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]() -->


<!-- ## Guest Contributors
* [No] -->

<!-- <img width="200%" src="./figures/hr.gif" />

# ğŸ“” License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div> -->
