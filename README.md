<div align="center">

<img src="./figures/Prompt-EgoAlpha_white" width="600px">

 <div align="center">
    <a href="https://blog.sunguoqi.com/">
      <img src="https://readme-typing-svg.demolab.com/?font=Tinos&size=30&duration=1500&pause=2000&color=00FF2B&background=000000&center=true&vCenter=true&width=435&lines=Hello+human%2C+are+you+ready%3F++;Welcome+to+my+world!" alt="Typing SVG" />
    </a>
  </div>

**An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./chatprompt_zh.md">ç®€ä½“ä¸­æ–‡</a>
    <p>
</h4>
<p align="center">
  <a href="#papersğŸ“œ">ğŸ“ Papers</a> |
  <a href="./Playground.md">âš¡ï¸  Playground</a> |
  <a href="./PromptEngineering.md">ğŸ›  Prompt Engineering</a> |
  <a href="./chatgptprompt.md">ğŸŒ ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **â­ï¸ Shining â­ï¸:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, letâ€™s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*ğŸ‰[Papers](#papersğŸ“œ)ğŸ‰*:  The latest papers about in-context learning or prompt engineering. 

*ğŸ‰[Playground](./Playground.md)ğŸ‰*:  Large language models that enable prompt experimentation. 

*ğŸ‰[Prompt Engineering](./PromptEngineering.md)ğŸ‰*: Prompt techniques for leveraging large language models. 

*ğŸ‰[ChatGPT Prompt](./chatgptprompt.md)ğŸ‰*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
ğŸ’EgoAlpha: Hello! humanğŸ‘¤, are you ready?
```  

# ğŸ“¢ News

- **[2023.3.4]** We establish this project that is organised by professor Yu Liu from EgoAlpha Lab.

<img width="200%" src="./figures/hr.gif" />

# PapersğŸ“œ

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
<!-- - [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph) -->

---

## Prompt Engineering

### ğŸ“Œ Prompt Design

[**Calibrate Before Use: Improving Few-Shot Performance of Language Models**](https://arxiv.org/abs/2302.135402102.09690) ğŸ‘¨â€ğŸ“Tony Zhao,Eric Wallace,Shi Feng,D. Klein,Sameer Singh 2021 ![](https://img.shields.io/badge/pub-2021--02--19-green)![](https://img.shields.io/badge/cite-281-red)



ğŸ‘‰[Complete paper list ğŸ”— for prompt design](./PaperList/PromptDesignList.md)ğŸ‘ˆ

### ğŸ“Œ Automatic Prompt 

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,etc 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-26-red)

[**On the Advance of Making Language Models Better Reasoners**](https://doi.org/10.48550/arXiv.2206.02336) ğŸ‘¨â€ğŸ“Yifei Li,Zeqi Lin,Shizhuo Zhang,Qiang Fu,Bei Chen,etc 2022 ![](https://img.shields.io/badge/pub-2022--06--06-green)![](https://img.shields.io/badge/cite-40-red)

[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2302.135402205.11916) ğŸ‘¨â€ğŸ“Takeshi Kojima,S. Gu,Machel Reid,Yutaka Matsuo,Yusuke Iwasawa 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-185-red)

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2302.135402204.02311) ğŸ‘¨â€ğŸ“Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-612-red)

[**Can language models learn from explanations in context?**](https://doi.org/10.48550/arXiv.2204.02329) ğŸ‘¨â€ğŸ“Andrew Kyle Lampinen,I. Dasgupta,Stephanie C. Y. Chan,Kory Matthewson,Michael Henry Tessler,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-61-red)

[**STaR: Bootstrapping Reasoning With Reasoning**](https://doi.org/10.48550/arXiv.2203.14465) ğŸ‘¨â€ğŸ“E. Zelikman,Yuhuai Wu,Noah D. Goodman 2022 ![](https://img.shields.io/badge/pub-2022--03--28-green)![](https://img.shields.io/badge/cite-56-red)

[**Self-Consistency Improves Chain of Thought Reasoning in Language Models**](https://doi.org/10.48550/arXiv.2203.11171) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,etc 2022 ![](https://img.shields.io/badge/pub-2022--03--21-green)![](https://img.shields.io/badge/cite-133-red)

[**Training language models to follow instructions with human feedback**](https://doi.org/10.48550/arXiv.2203.02155) ğŸ‘¨â€ğŸ“Long Ouyang,Jeff Wu,Xu Jiang,Diogo Almeida,Carroll L. Wainwright,etc 2022 ![](https://img.shields.io/badge/pub-2022--03--04-green)![](https://img.shields.io/badge/cite-426-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Chain of Thought

[**Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models**](https://doi.org/10.48550/arXiv.2302.00618) ğŸ‘¨â€ğŸ“Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--01-green)![](https://img.shields.io/badge/cite-2-red)

[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) ğŸ‘¨â€ğŸ“Namgyu Ho,Laura Schmid,Se-Young Yun 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-5-red)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) ğŸ‘¨â€ğŸ“Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-2-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Measuring and Narrowing the Compositionality Gap in Language Models**](https://doi.org/10.48550/arXiv.2210.03350) ğŸ‘¨â€ğŸ“Ofir Press,Muru Zhang,Sewon Min,Ludwig Schmidt,Noah A. Smith,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-27-red)

[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) ğŸ‘¨â€ğŸ“Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-21-red)

[**Decomposed Prompting: A Modular Approach for Solving Complex Tasks**](https://doi.org/10.48550/arXiv.2210.02406) ğŸ‘¨â€ğŸ“Tushar Khot,H. Trivedi,Matthew Finlayson,Yao Fu,Kyle Richardson,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--05-green)![](https://img.shields.io/badge/cite-25-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Compositional Semantic Parsing with Large Language Models**](https://doi.org/10.48550/arXiv.2209.15003) ğŸ‘¨â€ğŸ“Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,etc 2022 ![](https://img.shields.io/badge/pub-2022--09--29-green)![](https://img.shields.io/badge/cite-16-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Chain of Thought"](./PaperList/ChainofThoughtList.md)ğŸ‘ˆ

### ğŸ“Œ Evaluation & Reliability



ğŸ‘‰[Complete paper list ğŸ”— for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)ğŸ‘ˆ

## In-context Learning

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-23-red)

[**Language Models are Multilingual Chain-of-Thought Reasoners**](https://doi.org/10.48550/arXiv.2210.03057) ğŸ‘¨â€ğŸ“Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,etc 2022 ![](https://img.shields.io/badge/pub-2022--10--06-green)![](https://img.shields.io/badge/cite-21-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-18-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,etc 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-26-red)

[**Large Language Models are Zero-Shot Reasoners**](https://arxiv.org/abs/2302.135402205.11916) ğŸ‘¨â€ğŸ“Takeshi Kojima,S. Gu,Machel Reid,Yutaka Matsuo,Yusuke Iwasawa 2022 ![](https://img.shields.io/badge/pub-2022--05--24-green)![](https://img.shields.io/badge/cite-185-red)

[**PaLM: Scaling Language Modeling with Pathways**](https://arxiv.org/abs/2302.135402204.02311) ğŸ‘¨â€ğŸ“Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,etc 2022 ![](https://img.shields.io/badge/pub-2022--04--05-green)![](https://img.shields.io/badge/cite-612-red)



ğŸ‘‰[Complete paper list ğŸ”— for "In-context Learning"](./PaperList/InContextLearningList.md)ğŸ‘ˆ

## Multimodal Prompt

### ğŸ“Œ Hard Prompt/ Discrete Prompt

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,etc 2023 ![](https://img.shields.io/badge/pub-2023--02--07-green)![](https://img.shields.io/badge/cite-2-red)

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) ğŸ‘¨â€ğŸ“Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,etc 2022 ![](https://img.shields.io/badge/pub-2022--05--25-green)![](https://img.shields.io/badge/cite-25-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Hard Prompt"](./PaperList/HardPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Soft Prompt/ Continuous Prompt

[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) ğŸ‘¨â€ğŸ“G. Zuccon,B. Koopman 2023 ![](https://img.shields.io/badge/pub-2023--02--23-green)![](https://img.shields.io/badge/cite-1-red)

[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) ğŸ‘¨â€ğŸ“Simeng Sun,Yang Liu,Dan Iter,Chenguang Zhu,Mohit Iyyer 2023 ![](https://img.shields.io/badge/pub-2023--02--22-green)

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://doi.org/10.48550/arXiv.2302.09236) ğŸ‘¨â€ğŸ“Yuhang Zhou,Suraj Maharjan,Bei Liu 2023 ![](https://img.shields.io/badge/pub-2023--02--18-green)

[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) ğŸ‘¨â€ğŸ“Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan 2023 ![](https://img.shields.io/badge/pub-2023--02--17-green)

[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) ğŸ‘¨â€ğŸ“Koustava Goswami,Lukas Lange,J. Araki,Heike Adel 2023 ![](https://img.shields.io/badge/pub-2023--02--14-green)

[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) ğŸ‘¨â€ğŸ“Mingyu Derek Ma,Jiun-Yu Kao,Shuyang Gao,Arpit Gupta,Di Jin,etc 2023 ![](https://img.shields.io/badge/pub-2023--01--26-green)

[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) ğŸ‘¨â€ğŸ“M Saiful Bari,Aston Zhang,Shuai Zheng,Xingjian Shi,Yi Zhu,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-1-red)

[**Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) ğŸ‘¨â€ğŸ“Weijia Shi,Xiaochuang Han,Hila Gonen,Ari Holtzman,Yulia Tsvetkov,etc 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-1-red)

[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) ğŸ‘¨â€ğŸ“Damith Chamalke Senadeera,Julia Ive 2022 ![](https://img.shields.io/badge/pub-2022--12--06-green)

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://doi.org/10.48550/arXiv.2211.10681) ğŸ‘¨â€ğŸ“Xiaocheng Lu,Ziming Liu,Song Guo,Jingcai Guo 2022 ![](https://img.shields.io/badge/pub-2022--11--19-green)



ğŸ‘‰[Complete paper list ğŸ”— for "Soft Prompt"](./PaperList/SoftPromptList.md)ğŸ‘ˆ

<!-- ## Knowledge Augmented Prompts

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected'}

ğŸ‘‰[Complete paper list ğŸ”— for "Knowledge Augmented Prompts"](./PaperList/KnowledgeAugmentedPromptList.md)ğŸ‘ˆ

## Prompt for Knowledge Graph

// __PAPER_LIST__:{field:'Prompt Design',size:10,state:'corrected'}

ğŸ‘‰[Complete paper list ğŸ”— for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)ğŸ‘ˆ -->

<img width="200%" src="./figures/hr.gif" />

# ğŸ“ Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
ç»¼è¿°è®ºæ–‡å¯ä»¥æ”¾åœ¨è¿™ä¸ªä½ç½®
```
<img width="200%" src="./figures/hr.gif" />

# âœ‰ï¸ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `cyfedu1024@gmail.com` or `cyfedu1024@163.com`.

We are willing to communicate with your research team or confirm in variety of fields.

<img width="200%" src="./figures/hr.gif" />

# ğŸ™ Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. More researchers are welcome to join us and make more contributions to the community.

<img width="200%" src="./figures/hr.gif" />

# ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]()


<!-- ## Guest Contributors
* [No] -->

<img width="200%" src="./figures/hr.gif" />

# ğŸ“” License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div>
