<div align="center">

<img src="./figures/egoalpha_logo_compress.jpeg" width="600px">

 <div align="center">
    <a href="https://blog.sunguoqi.com/">
      <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&width=435&lines=%22Hello%2C%20World%22;Welcome to our project!&center=true&size=27" alt="Typing SVG" />
    </a>
  </div>

**An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.**

<img width="200%" src="./figures/hr.gif" />

<!-- <h3 align="center">
    <p>Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.</p>
</h3> -->
<h4 align="center">
    <p>
        <a href="./README.md">English</a> |
        <a href="./README_zh.md">ç®€ä½“ä¸­æ–‡</a>
    <p>
</h4>
<p align="center">
  <a href="#papersğŸ“œ">ğŸ“ Papers</a> |
  <a href="./Playground.md">âš¡ï¸  Playground</a> |
  <a href="./Promptzoo.md">ğŸ›  Prompt Zoo</a> |
  <a href="./chatgptprompt.md">ğŸŒ ChatGPT Prompt</a> 
</p>
</div>

<div align="center">

<!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) -->
![version](https://img.shields.io/badge/version-v1.0.0-blue)
<!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) -->
</div>


> **â­ï¸ Shining â­ï¸:** This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, letâ€™s take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.

The resources include:

*ğŸ‰[Papers](#papersğŸ“œ)ğŸ‰*:  The latest papers about in-context learning or prompt engineering. 

*ğŸ‰[Playground](./Playground.md)ğŸ‰*:  Large language models that enable prompt experimentation. 

*ğŸ‰[Prompt Zoo](./Promptzoo.md)ğŸ‰*: Prompt techniques for leveraging large language models. 

*ğŸ‰[ChatGPT Prompt](./chatgptprompt.md)ğŸ‰*: Prompt examples that can be applied in our work and daily lives. 

In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that's a question for Musk): Those who enhance their abilities through the use of AI; 
Those whose jobs are replaced by AI automation.

```
ğŸ’EgoAlpha: Hello! humanğŸ‘¤, are you ready?
```  

# ğŸ“¢ News

- **[2023.3.4]** We establish this project that is organised by professor Yu Liu from EgoAlpha Lab.

<img width="200%" src="./figures/hr.gif" />

# PapersğŸ“œ

- [Prompt Engineering](#prompt-engineering)
- [In-context learning](#in-context-learning)
- [Multimodal Prompt](#multimodal-prompt)
- [Knowledge Augmented Prompts](#knowledge-augmented-prompts)
- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph)

---

## Prompt Engineering

### ğŸ“Œ Prompt Design

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



-----------------------------------------ğŸ‘‰[Complete paper list ğŸ”— for "prompt design"](./PaperList/PromptDesignList.md)ğŸ‘ˆ--------------------------------------------------

### ğŸ“Œ Automatic Prompt 

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,Hongkun Yu,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-28-red)

[**Automatic Chain of Thought Prompting in Large Language Models**](https://doi.org/10.48550/arXiv.2210.03493) ğŸ‘¨â€ğŸ“Zhuosheng Zhang,Aston Zhang,Mu Li,Alexander J. Smola 2022 ![](https://img.shields.io/badge/pub-2022--10--07-green)![](https://img.shields.io/badge/cite-21-red)

[**Complexity-Based Prompting for Multi-Step Reasoning**](https://doi.org/10.48550/arXiv.2210.00720) ğŸ‘¨â€ğŸ“Yao Fu,Hao-Chun Peng,Ashish Sabharwal,Peter Clark,Tushar Khot 2022 ![](https://img.shields.io/badge/pub-2022--10--03-green)![](https://img.shields.io/badge/cite-17-red)

[**Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning**](https://doi.org/10.48550/arXiv.2209.14610) ğŸ‘¨â€ğŸ“Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan 2022 ![](https://img.shields.io/badge/pub-2022--09--29-green)![](https://img.shields.io/badge/cite-11-red)

[**Selective Annotation Makes Language Models Better Few-Shot Learners**](https://doi.org/10.48550/arXiv.2209.01975) ğŸ‘¨â€ğŸ“Hongjin Su,Jungo Kasai,Chen Henry Wu,Weijia Shi,Tianlu Wang,Jiayi Xin,Rui Zhang,Mari Ostendorf,Luke Zettlemoyer,Noah A. Smith,Tao Yu 2022 ![](https://img.shields.io/badge/pub-2022--09--05-green)![](https://img.shields.io/badge/cite-20-red)

[**Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models**](https://doi.org/10.1109/TVCG.2022.3209479) ğŸ‘¨â€ğŸ“Hendrik Strobelt,Albert Webson,Victor Sanh,Benjamin Hoover,J. Beyer,H. Pfister,Alexander M. Rush 2022 ![](https://img.shields.io/badge/pub-2022--08--16-green)![](https://img.shields.io/badge/cite-9-red)

[**Exploring CLIP for Assessing the Look and Feel of Images**](https://doi.org/10.48550/arXiv.2207.12396) ğŸ‘¨â€ğŸ“Jianyi Wang,Kelvin C. K. Chan,Chen Change Loy 2022 ![](https://img.shields.io/badge/pub-2022--07--25-green)![](https://img.shields.io/badge/cite-1-red)

[**Rationale-Augmented Ensembles in Language Models**](https://doi.org/10.48550/arXiv.2207.00747) ğŸ‘¨â€ğŸ“Xuezhi Wang,Jason Wei,D. Schuurmans,Quoc Le,E. Chi,Denny Zhou 2022 ![](https://img.shields.io/badge/pub-2022--07--02-green)![](https://img.shields.io/badge/cite-25-red)

[**Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal AI Generated Art**](https://doi.org/10.1145/3527927.3532792) ğŸ‘¨â€ğŸ“Han Qiao,Vivian Liu,Lydia B. Chilton 2022 ![](https://img.shields.io/badge/pub-2022--06--20-green)![](https://img.shields.io/badge/cite-5-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Automatic Prompt"](./PaperList/AutomaticPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Chain of Thought

[**Large Language Models Are Reasoning Teachers**](https://doi.org/10.48550/arXiv.2212.10071) ğŸ‘¨â€ğŸ“Namgyu Ho,Laura Schmid,Se-Young Yun 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-4-red)

[**Teaching Small Language Models to Reason**](https://doi.org/10.48550/arXiv.2212.08410) ğŸ‘¨â€ğŸ“Lucie Charlotte Magister,Jonathan Mallinson,Jakub Adamek,Eric Malmi,Aliaksei Severyn 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-7-red)

[**The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning**](https://doi.org/10.48550/arXiv.2212.08686) ğŸ‘¨â€ğŸ“Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric Xing 2022 ![](https://img.shields.io/badge/pub-2022--12--16-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**PAL: Program-aided Language Models**](https://doi.org/10.48550/arXiv.2211.10435) ğŸ‘¨â€ğŸ“Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig 2022 ![](https://img.shields.io/badge/pub-2022--11--18-green)![](https://img.shields.io/badge/cite-18-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Large Language Models Can Self-Improve**](https://doi.org/10.48550/arXiv.2210.11610) ğŸ‘¨â€ğŸ“Jiaxin Huang,S. Gu,Le Hou,Yuexin Wu,Xuezhi Wang,Hongkun Yu,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-28-red)

[**Scaling Instruction-Finetuned Language Models**](https://doi.org/10.48550/arXiv.2210.11416) ğŸ‘¨â€ğŸ“Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc V. Le,Jason Wei 2022 ![](https://img.shields.io/badge/pub-2022--10--20-green)![](https://img.shields.io/badge/cite-53-red)

[**Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them**](https://doi.org/10.48550/arXiv.2210.09261) ğŸ‘¨â€ğŸ“Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung,Aakanksha Chowdhery,Quoc V. Le,E. Chi,Denny Zhou,Jason Wei 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-27-red)

[**Prompting GPT-3 To Be Reliable**](https://doi.org/10.48550/arXiv.2210.09150) ğŸ‘¨â€ğŸ“Chenglei Si,Zhe Gan,Zhengyuan Yang,Shuohang Wang,Jianfeng Wang,Jordan L. Boyd-Graber,Lijuan Wang 2022 ![](https://img.shields.io/badge/pub-2022--10--17-green)![](https://img.shields.io/badge/cite-9-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Chain of Thought"](./PaperList/ChainofThoughtList.md)ğŸ‘ˆ

### ğŸ“Œ Evaluation & Reliability

[**Relay Node Placement in Wireless Sensor Networks With Respect to Delay and Reliability Requirements**](https://doi.org/10.1109/JSYST.2018.2838072) ğŸ‘¨â€ğŸ“Chaofan Ma,W. Liang,M. Zheng,Bofu Yang 2019 ![](https://img.shields.io/badge/pub-2019--09--01-green)![](https://img.shields.io/badge/cite-24-red)

[**Comparative Analysis of Transmission Power Level and Packet Size Optimization Strategies for WSNs**](https://doi.org/10.1109/JSYST.2018.2864941) ğŸ‘¨â€ğŸ“Huseyin Ugur Yildiz,Sinan Kurt,B. Tavli 2019 ![](https://img.shields.io/badge/pub-2019--09--01-green)![](https://img.shields.io/badge/cite-9-red)

[**RBD Model-Based Approach for Reliability Assessment in Complex Systems**](https://doi.org/10.1109/JSYST.2018.2840220) ğŸ‘¨â€ğŸ“M. Catelani,L. Ciani,M. Venzi 2019 ![](https://img.shields.io/badge/pub-2019--09--01-green)![](https://img.shields.io/badge/cite-13-red)

[**Joint Transmission Power Optimization and Connectivity Control in Asymmetric Networks**](https://doi.org/10.23919/ACC.2018.8431490) ğŸ‘¨â€ğŸ“Milad Esmacilpour,A. Aghdam,S. Blouin 2018 ![](https://img.shields.io/badge/pub-2018--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**Reliability Allocation Procedures in Complex Redundant Systems**](https://doi.org/10.1109/JSYST.2017.2651161) ğŸ‘¨â€ğŸ“M. Catelani,L. Ciani,G. Patrizi,M. Venzi 2018 ![](https://img.shields.io/badge/pub-2018--06--01-green)![](https://img.shields.io/badge/cite-26-red)

[**Device-to-Device Communications: A Performance Analysis in the Context of Social Comparison-Based Relaying**](https://doi.org/10.1109/TWC.2017.2751470) ğŸ‘¨â€ğŸ“Young Jin Chun,Gualtiero Colombo,S. Cotton,W. Scanlon,R. Whitaker,S. Allen 2017 ![](https://img.shields.io/badge/pub-2017--09--15-green)![](https://img.shields.io/badge/cite-17-red)

[**MIMO Wireless Communications over Generalized Fading Channels**](https://doi.org/10.1201/9781315116778) ğŸ‘¨â€ğŸ“B. Kumbhani,R. Kshetrimayum 2017 ![](https://img.shields.io/badge/pub-2017--06--01-green)![](https://img.shields.io/badge/cite-42-red)

[**Energy Saving With Network Coding Design Over Rayleigh Fading Channel**](https://doi.org/10.1109/TWC.2017.2699188) ğŸ‘¨â€ğŸ“Shijun Lin,Liqun Fu,Yong Li 2017 ![](https://img.shields.io/badge/pub-2017--05--03-green)![](https://img.shields.io/badge/cite-6-red)

[**Optimal WSN Deployment Models for Air Pollution Monitoring**](https://doi.org/10.1109/TWC.2017.2658601) ğŸ‘¨â€ğŸ“Ahmed Boubrima,Walid Bechkit,H. Rivano 2017 ![](https://img.shields.io/badge/pub-2017--05--01-green)![](https://img.shields.io/badge/cite-132-red)

[**Distance distribution between nodes in a 3D wireless network**](https://doi.org/10.1016/j.jpdc.2016.09.006) ğŸ‘¨â€ğŸ“J. Nichols,J. Michalowicz 2017 ![](https://img.shields.io/badge/pub-2017--04--01-green)![](https://img.shields.io/badge/cite-8-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Evaluation & Reliability"](./PaperList/EvaluationReliabilityList.md)ğŸ‘ˆ

## In-context Learning

[**Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**](https://doi.org/10.48550/arXiv.2301.11916) ğŸ‘¨â€ğŸ“Xinyi Wang,Wanrong Zhu,William Yang Wang 2023 ![](https://img.shields.io/badge/pub-2023--01--27-green)![](https://img.shields.io/badge/cite-1-red)

[**OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization**](https://doi.org/10.48550/arXiv.2212.12017) ğŸ‘¨â€ğŸ“S. Iyer,Xiaojuan Lin,Ramakanth Pasunuru,Todor Mihaylov,Daniel Simig,Ping Yu,Kurt Shuster,Tianlu Wang,Qing Liu,Punit Singh Koura,Xian Li,Brian O'Horo,Gabriel Pereyra,Jeff Wang,Christopher Dewan,Asli Celikyilmaz,Luke Zettlemoyer,Veselin Stoyanov 2022 ![](https://img.shields.io/badge/pub-2022--12--22-green)![](https://img.shields.io/badge/cite-9-red)

[**Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners**](https://doi.org/10.48550/arXiv.2212.10873) ğŸ‘¨â€ğŸ“Hyunsoo Cho,Hyuhng Joon Kim,Junyeob Kim,Sang-Woo Lee,Sang-goo Lee,Kang Min Yoo,Taeuk Kim 2022 ![](https://img.shields.io/badge/pub-2022--12--21-green)![](https://img.shields.io/badge/cite-2-red)

[**Self-adaptive In-context Learning**](https://doi.org/10.48550/arXiv.2212.10375) ğŸ‘¨â€ğŸ“Zhiyong Wu,Yaoxiang Wang,Jiacheng Ye,Lingpeng Kong 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Is GPT-3 a Good Data Annotator?**](https://doi.org/10.48550/arXiv.2212.10450) ğŸ‘¨â€ğŸ“Bosheng Ding,Chengwei Qin,Linlin Liu,Lidong Bing,Shafiq R. Joty,Boyang Li 2022 ![](https://img.shields.io/badge/pub-2022--12--20-green)![](https://img.shields.io/badge/cite-2-red)

[**Reasoning with Language Model Prompting: A Survey**](https://doi.org/10.48550/arXiv.2212.09597) ğŸ‘¨â€ğŸ“Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen 2022 ![](https://img.shields.io/badge/pub-2022--12--19-green)![](https://img.shields.io/badge/cite-7-red)

[**Structured Prompting: Scaling In-Context Learning to 1, 000 Examples**](https://doi.org/10.48550/arXiv.2212.06713) ğŸ‘¨â€ğŸ“Y. Hao,Yutao Sun,Li Dong,Zhixiong Han,Yuxian Gu,Furu Wei 2022 ![](https://img.shields.io/badge/pub-2022--12--13-green)![](https://img.shields.io/badge/cite-2-red)

[**Complementary Explanations for Effective In-Context Learning**](https://doi.org/10.48550/arXiv.2211.13892) ğŸ‘¨â€ğŸ“Xi Ye,Srini Iyer,Asli Celikyilmaz,V. Stoyanov,Greg Durrett,Ramakanth Pasunuru 2022 ![](https://img.shields.io/badge/pub-2022--11--25-green)![](https://img.shields.io/badge/cite-5-red)

[**Active Example Selection for In-Context Learning**](https://doi.org/10.48550/arXiv.2211.04486) ğŸ‘¨â€ğŸ“Yiming Zhang,Shi Feng,Chenhao Tan 2022 ![](https://img.shields.io/badge/pub-2022--11--08-green)![](https://img.shields.io/badge/cite-6-red)

[**Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning**](https://doi.org/10.48550/arXiv.2211.03044) ğŸ‘¨â€ğŸ“Yu Meng,Martin Michalski,Jiaxin Huang,Yu Zhang,T. Abdelzaher,Jiawei Han 2022 ![](https://img.shields.io/badge/pub-2022--11--06-green)![](https://img.shields.io/badge/cite-2-red)



ğŸ‘‰[Complete paper list ğŸ”— for "In-context Learning"](./PaperList/InContextLearningList.md)ğŸ‘ˆ

## Multimodal Prompt

### ğŸ“Œ Hard Prompt/ Discrete Prompt

[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://api.semanticscholar.org/2146867236) ğŸ‘¨â€ğŸ“Mingkai Deng,Jianyu Wang,Cheng-Ping Hsieh,Yihan Wang,Han Guo,Tianmin Shu,Meng Song,E. Xing,Zhiting Hu 2022 

[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://api.semanticscholar.org/123191916) ğŸ‘¨â€ğŸ“Yuxin Wen,Neel Jain,John Kirchenbauer,Micah Goldblum,Jonas Geiping,T. Goldstein 2023 



ğŸ‘‰[Complete paper list ğŸ”— for "Hard Prompt"](./PaperList/HardPromptList.md)ğŸ‘ˆ

### ğŸ“Œ Soft Prompt/ Continuous Prompt

[**Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models**](https://arxiv.org/abs/2302.135402210.10841) ğŸ‘¨â€ğŸ“ 

[**P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks**](https://api.semanticscholar.org/ec936b808e0fab9281c050ad4010cddec92c8cbe) 2022 

[**FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning**](https://arxiv.org/abs/2302.135402208.12268) ğŸ‘¨â€ğŸ“ 

[**Instance-aware prompt learning for language understanding and generation**](https://arxiv.org/abs/2302.135402201.07126) ğŸ‘¨â€ğŸ“ 

[**Learning to Compose Soft Prompts for Compositional Zero-Shot Learning**](https://arxiv.org/abs/2302.135402204.03574) ğŸ‘¨â€ğŸ“ 

[**FPT: Improving Prompt Tuning Efficiency via Progressive Training**](https://arxiv.org/abs/2302.135402211.06840) ğŸ‘¨â€ğŸ“ 

[**Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning**](https://arxiv.org/abs/2302.135402211.10681) ğŸ‘¨â€ğŸ“ 

[**Prompt Distribution Learning**](https://arxiv.org/abs/2302.135402205.03340) ğŸ‘¨â€ğŸ“ 

[**Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts**](https://arxiv.org/abs/2302.135402210.11292) ğŸ‘¨â€ğŸ“ 

[**Scalable Prompt Generation for Semi-supervised Learning with Language Models**](https://arxiv.org/abs/2302.135402302.09236) ğŸ‘¨â€ğŸ“ 



ğŸ‘‰[Complete paper list ğŸ”— for "Soft Prompt"](./PaperList/SoftPromptList.md)ğŸ‘ˆ

## Knowledge Augmented Prompts

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Knowledge Augmented Prompts"](./PaperList/KnowledgeAugmentedPromptList.md)ğŸ‘ˆ

## Prompt for Knowledge Graph

[**UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning**](https://doi.org/10.18653/v1/2022.acl-long.433) ğŸ‘¨â€ğŸ“Yuning Mao,Lambert Mathias,Rui Hou,Amjad Almahairi,Hao Ma,Jiawei Han,Wen-tau Yih,Madian Khabsa 2021 ![](https://img.shields.io/badge/pub-2021--10--14-green)![](https://img.shields.io/badge/cite-31-red)

[**HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization**](https://doi.org/10.18653/v1/2021.emnlp-main.13) ğŸ‘¨â€ğŸ“Ye Liu,Jianguo Zhang,Yao Wan,Congying Xia,Lifang He,Philip S. Yu 2021 ![](https://img.shields.io/badge/pub-2021--10--12-green)![](https://img.shields.io/badge/cite-11-red)

[**Can Language Models be Biomedical Knowledge Bases?**](https://doi.org/10.18653/v1/2021.emnlp-main.388) ğŸ‘¨â€ğŸ“Mujeen Sung,Jinhyuk Lee,Sean S. Yi,Minji Jeon,Sungdong Kim,Jaewoo Kang 2021 ![](https://img.shields.io/badge/pub-2021--09--15-green)![](https://img.shields.io/badge/cite-26-red)

[**The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation**](https://arxiv.org/abs/2302.135402108.06614) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Alex Marin,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--08--14-green)![](https://img.shields.io/badge/cite-4-red)

[**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://doi.org/10.1145/3560815) ğŸ‘¨â€ğŸ“Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig 2021 ![](https://img.shields.io/badge/pub-2021--07--28-green)![](https://img.shields.io/badge/cite-429-red)

[**On Training Instance Selection for Few-Shot Neural Text Generation**](https://doi.org/10.18653/v1/2021.acl-short.2) ğŸ‘¨â€ğŸ“Ernie Chang,Xiaoyu Shen,Hui-Syuan Yeh,V. Demberg 2021 ![](https://img.shields.io/badge/pub-2021--07--07-green)![](https://img.shields.io/badge/cite-15-red)

[**Template-Based Named Entity Recognition Using BART**](https://doi.org/10.18653/v1/2021.findings-acl.161) ğŸ‘¨â€ğŸ“Leyang Cui,Yu Wu,Jian Liu,Sen Yang,Yue Zhang 2021 ![](https://img.shields.io/badge/pub-2021--06--03-green)![](https://img.shields.io/badge/cite-100-red)

[**Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization**](https://doi.org/10.18653/V1/2021.NAACL-MAIN.381) ğŸ‘¨â€ğŸ“Yichen Jiang,Asli Celikyilmaz,P. Smolensky,Paul Soulos,Sudha Rao,H. Palangi,Roland Fernandez,Caitlin Smith,Mohit Bansal,Jianfeng Gao 2021 ![](https://img.shields.io/badge/pub-2021--06--01-green)![](https://img.shields.io/badge/cite-6-red)

[**SciFive: a text-to-text transformer model for biomedical literature**](https://arxiv.org/abs/2302.135402106.03598) ğŸ‘¨â€ğŸ“Long Phan,J. Anibal,Hieu Tran,Shaurya Chanana,Erol Bahadroglu,Alec Peltekian,G. Altan-Bonnet 2021 ![](https://img.shields.io/badge/pub-2021--05--28-green)![](https://img.shields.io/badge/cite-45-red)

[**PTR: Prompt Tuning with Rules for Text Classification**](https://doi.org/10.1016/j.aiopen.2022.11.003) ğŸ‘¨â€ğŸ“Xu Han,Weilin Zhao,Ning Ding,Zhiyuan Liu,Maosong Sun 2021 ![](https://img.shields.io/badge/pub-2021--05--24-green)![](https://img.shields.io/badge/cite-169-red)



ğŸ‘‰[Complete paper list ğŸ”— for "Prompt for Knowledge Graph"](./PaperList/PromptKnowledgeGraphList.md)ğŸ‘ˆ

<img width="200%" src="./figures/hr.gif" />

# ğŸ“ Citation

If you find our work helps, please star our project and cite our paper. Thanks a lot!

```
ç»¼è¿°è®ºæ–‡å¯ä»¥æ”¾åœ¨è¿™ä¸ªä½ç½®
```
<img width="200%" src="./figures/hr.gif" />

# âœ‰ï¸ Contact

This repo is maintained by [EgoAlpha Lab](https://github.com/EgoAlpha). Questions and discussions are welcome via `cyfedu1024@gmail.com` or `cyfedu1024@163.com`.

We are willing to communicate with your research team or confirm in variety of fields.

<img width="200%" src="./figures/hr.gif" />

# ğŸ™ Acknowledgements

Thanks to the PhD students from [EgoAlpha Lab](https://github.com/EgoAlpha) and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. More researchers are welcome to join us and make more contributions to the community.

<img width="200%" src="./figures/hr.gif" />

# ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors

## Main Contributors
* [Yu Liu]()
* [Yifei Cao](https://github.com/cyfedu1024)
* [Jizhe Yu]()
* [Yuan Yao]()
* [He Qi]()


<!-- ## Guest Contributors
* [No] -->

<img width="200%" src="./figures/hr.gif" />

# ğŸ“” License

This project is open source and available under the MIT

<div align="center">
<img src="./figures/rocket.png"/>
</div>


[new]: "https://github.com/EgoAlpha/prompt-in-context-learning/PaperList/PromptDesignList.md"
